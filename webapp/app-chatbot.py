import streamlit as st
import os
from dotenv import load_dotenv

# --- Imports LangChain (Assurez-vous qu'ils sont corrects) ---
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.utilities import SQLDatabase
from langchain_community.agent_toolkits import create_sql_agent
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory
from langchain_core.messages import HumanMessage, AIMessage
from chatbot import initialize_agent
import streamlit as st
import requests
import io # Pour g√©rer les donn√©es binaires des fichiers

# --- PARTIE A : Uploader et Classifier ---
load_dotenv()

GEO_SERVICE_API_KEY = os.getenv("GEO_SERVICE_API_KEY")
INFERENCE_SERVICE_URL = os.getenv("INFERENCE_SERVICE_URL")
st.header("Give me the trash items you want to classifyüóëÔ∏è")

# Utilisation du widget d'upload Streamlit, acceptant plusieurs fichiers
uploaded_files = st.file_uploader(
    "Send the images you cant to classify:",
    type=["jpg", "jpeg", "png"],
    accept_multiple_files=True
)
# Instanciation of final trash list but in a way not to be cleaned each time session is updated during chatbot discussion
if "final_trash_list" not in st.session_state:
    st.session_state.final_trash_list = []

def classify_images(files):
    """Appelle le service d'inf√©rence fichier par fichier."""

    if not files:
        return

    st.info(f"Classification de {len(files)} image(s) en cours...")

    # üí° L'AgentExecutor doit recevoir le r√©sultat final

    with st.spinner("Analyse des images en cours..."):
        for file in files:
            # Pr√©paration du dictionnaire 'files' pour UN SEUL fichier
            # Cl√©: le nom attendu par votre serveur (souvent 'file' ou 'image')
            files_to_send = {
                "file": (file.name, file.getvalue(), file.type)
            }

            try:
                # 1. Envoyer UN SEUL fichier √† la fois
                response = requests.post(
                    f"{INFERENCE_SERVICE_URL}/predict",
                    files=files_to_send, # Envoi du dictionnaire de fichiers UNIQUE
                    timeout=30
                )
                print(f"Contenu brut de la r√©ponse : {response.text}")
                response.raise_for_status() # Cette ligne l√®vera l'erreur si le statut est 4xx/5xx
                result_list = response.json()

                # 2. On suppose que la r√©ponse contient une liste de r√©sultats
                # et que vous voulez la classe du premier (et seul) r√©sultat
                if result_list and isinstance(result_list, list) and 'class' in result_list[0]:
                    st.session_state.final_trash_list.append(result_list[0]['class'])
                else:
                    st.warning(f"R√©ponse API inattendue pour {file.name}.")

            except requests.exceptions.RequestException as e:
                st.error(f"Erreur HTTP ou de connexion pour {file.name}. D√©tail : {e}")
                break # Arr√™ter si une requ√™te √©choue
            except Exception as e:
                st.error(f"Erreur inattendue lors du traitement de {file.name}. D√©tail : {e}")
                break

        if st.session_state.final_trash_list:
            unique_classes = set(st.session_state.final_trash_list)

            # 2. Convertir l'ensemble de classes uniques en une cha√Æne de caract√®res
            classes_summary = ", ".join(unique_classes)

            # 3. Afficher le message de succ√®s avec le r√©sum√©
            st.success(f"‚úÖ Classification termin√©e ! Classes trouv√©es : {classes_summary}.")
            st.info("Vous pouvez maintenant demander √† l'agent d'optimiser le trajet, par exemple : 'Minimise le temps de trajet en voiture pour d√©poser ces d√©chets'.")


# --- Logique d'appel Streamlit (reste la m√™me) ---
if uploaded_files:
    # On v√©rifie si les fichiers ont chang√© pour ne pas reclasser inutilement
    current_file_names = {f.name for f in uploaded_files}
    if st.session_state.get('last_uploaded_files') != current_file_names:
        classify_images(uploaded_files)
        st.session_state.last_uploaded_files = current_file_names

# ... (Reste de l'affichage des r√©sultats et de la boucle de chat)
st.markdown("---")

# --- PARTIE B : Boucle de Chat (Reste la m√™me) ---
# ... (le code de st.title, agent initialization, chat_input, etc. suit ici)
st.title("ü§ñ Chatbot Analyse BigQuery (SQL Agent)")
# Initialisation de l'agent une seule fois
agent = initialize_agent()

# G√©rer l'ID de session unique pour chaque utilisateur Streamlit
session_id = st.session_state.get("session_id", "default_user_1")
config = {"configurable": {"session_id": session_id}}

# Afficher l'historique de chat existant
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Gestion de la nouvelle entr√©e utilisateur
if prompt := st.chat_input("Posez une question sur les points de collecte..."):

    # 1. Afficher la question de l'utilisateur (votre code existant)
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # --- NOUVEAU : Pr√©paration du Prompt Enrichi ---
    full_prompt = prompt

    # V√©rifier si nous avons des classes de d√©chets en m√©moire
    if st.session_state.get('final_trash_list'):

        # 1. Obtenir la liste des classes uniques
        unique_classes = set(st.session_state.final_trash_list)
        classes_summary = ", ".join(unique_classes)

        # 2. Enrichir le prompt de l'utilisateur avec le contexte des d√©chets
        # Ceci est la technique d'Injection de Contexte
        full_prompt = (
            f"CONTEXTE D√âCHETS: Les types de d√©chets que je dois d√©poser sont : {classes_summary}. "
            f"QUESTION UTILISATEUR: {prompt}"
        )

  # ... (Dans la boucle de chat_input, au niveau de l'√©tape 2. Ex√©cuter l'agent)

    # 2. Ex√©cuter l'agent
    with st.spinner("L'agent SQL analyse la base de donn√©es..."):
        try:
            response = agent.invoke(
                {"input": full_prompt},
                config=config
            )

            raw_output = response["output"]
            agent_response = "" # Initialisation de la r√©ponse finale

            # --- D√âBUT CORRECTION ROBUSTE ---

            if isinstance(raw_output, list):
                # Si c'est une liste, nous cherchons le texte dans CHAQUE √©l√©ment

                all_text_parts = []
                for item in raw_output:
                    if isinstance(item, dict) and 'text' in item:
                        # Cas 1: C'est le format {'type': 'text', 'text': '...'}. On prend le texte.
                        all_text_parts.append(item['text'])
                    elif isinstance(item, str):
                        # Cas 2: C'est la suite de la cha√Æne qui a √©t√© coup√©e (votre cas r√©cent).
                        all_text_parts.append(item)
                    # Si c'est un autre type (comme 'tool_call', on l'ignore ici)

                # Joindre toutes les parties de texte en une seule r√©ponse
                agent_response = "\n".join(all_text_parts)

            else:
                # Si c'est d√©j√† une cha√Æne de caract√®res simple (le cas id√©al)
                agent_response = str(raw_output)

            # --- FIN CORRECTION ROBUSTE ---

        except Exception as e:
            agent_response = f"D√©sol√©, une erreur est survenue lors de l'ex√©cution : {e}"
            st.error(agent_response)

    # 3. Afficher la r√©ponse de l'agent
    with st.chat_message("assistant"):
        # Utiliser st.markdown pour bien formater la liste (bullets, retours √† la ligne)
        st.markdown(agent_response)

    # 4. Enregistrer la r√©ponse
    st.session_state.messages.append({"role": "assistant", "content": agent_response})


    # # --- 5) Ex√©cuter des Requ√™tes Conversationnelles ---

    # # D√©finition de l'ID de session
    # session_id = "user_session_123"
    # config = {"configurable": {"session_id": session_id}}

    # # üí° CORRECTION 1 : Appeler l'agent conversationnel (le wrapper)
    # # üí° CORRECTION 2 : Utiliser la cl√© "input" avec une cha√Æne de caract√®res
    # response = conversational_agent.invoke(
    #     {"input": "How many places can take batteries?"},
    #     config=config
    # )

    # # L'output est maintenant directement le contenu textuel de la r√©ponse finale
    # pprint(response["output"])

    # print("\n--- Interaction 2 (Test de la m√©moire) ---")
    # # Une seconde requ√™te pour tester la m√©moire
    # response_2 = conversational_agent.invoke(
    #     {"input": "Which one is the closest to Nantes city center?"},
    #     config=config
    # )
    # pprint(response_2["output"])
