{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2593e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_trash_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06a8137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUnified Waste Collection Dataset Creation\\n=========================================\\nCreates and maintains a unified BigQuery dataset for trash collection points\\nwith dynamic schema expansion and prioritized data ingestion.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unified Waste Collection Dataset Creation\n",
    "=========================================\n",
    "Creates and maintains a unified BigQuery dataset for trash collection points\n",
    "with dynamic schema expansion and prioritized data ingestion.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738e23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required libraries\n",
    "# pip install google-cloud-bigquery pandas google-auth\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda93419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# CONFIGURATION\n",
    "# ========================================================\n",
    "\n",
    "# Set up Google Cloud credentials\n",
    "CREDENTIALS_PATH = \"/Users/dariaserbichenko/code/DariaSerb/key-gcp/trash-optimizer-479913-91e59ecc96c9.json\"\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = CREDENTIALS_PATH\n",
    "\n",
    "# BigQuery configuration\n",
    "PROJECT = \"trash-optimizer-479913\"\n",
    "DATASET = \"nantes\"\n",
    "TARGET_TABLE = \"trash_collection_points\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c587f6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INITIALIZING TRASH COLLECTION DATASET CREATION\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# BIGQUERY CLIENT INITIALIZATION\n",
    "# ========================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INITIALIZING TRASH COLLECTION DATASET CREATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb22e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. PROCESSING FOOD WASTE DATA\n",
      "----------------------------------------\n",
      "âœ… Retrieved 1,644 food waste locations\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 1. FOOD WASTE DATA (PRIORITY 1)\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n1. PROCESSING FOOD WASTE DATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "query_food = f\"\"\"\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER () as point_id,\n",
    "  CONCAT('Food Waste - ', COALESCE(commune, 'Nantes')) as name,\n",
    "  COALESCE(adresse, 'Address not specified') as address,\n",
    "  lon as longitude,\n",
    "  lat as latitude,\n",
    "  1 as accepts_food,\n",
    "  0 as accepts_cardboard,\n",
    "  0 as accepts_glass,\n",
    "  0 as accepts_metal,\n",
    "  0 as accepts_paper,\n",
    "  0 as accepts_plastic,\n",
    "  0 as accepts_textile,\n",
    "  0 as accepts_wood,\n",
    "  0 as accepts_vegetation,\n",
    "  'food_waste' as point_type,\n",
    "  'Nantes_Metropole' as operator,\n",
    "  CURRENT_DATE() as ingestion_date,\n",
    "  '1.0' as schema_version,\n",
    "  'alimentary_garbage_clean' as data_source\n",
    "FROM `{PROJECT}.{DATASET}.alimentary_garbage_clean`\n",
    "WHERE lat IS NOT NULL AND lon IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_food = client.query(query_food).to_dataframe()\n",
    "    print(f\"âœ… Retrieved {len(df_food):,} food waste locations\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error retrieving food waste data: {e}\")\n",
    "    df_food = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699649e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. PROCESSING RECYCLING CENTERS\n",
      "----------------------------------------\n",
      "âœ… Retrieved 15 recycling centers\n",
      "   Waste acceptance in recycling centers:\n",
      "   â€¢ Cardboard: 15/15 locations\n",
      "   â€¢ Glass: 14/15 locations\n",
      "   â€¢ Metal: 14/15 locations\n",
      "   â€¢ Paper: 15/15 locations\n",
      "   â€¢ Textile: 9/15 locations\n",
      "   â€¢ Wood: 15/15 locations\n",
      "   â€¢ Vegetation: 14/15 locations\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 2. RECYCLING CENTERS (PRIORITY 2)\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n2. PROCESSING RECYCLING CENTERS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "query_recycling = f\"\"\"\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER () + 10000 as point_id,\n",
    "  CONCAT('Recycling Center - ', COALESCE(nom, commune, 'Ecopoint')) as name,\n",
    "  COALESCE(adresse, 'Address not specified') as address,\n",
    "  lon as longitude,\n",
    "  lat as latitude,\n",
    "  0 as accepts_food,\n",
    "  CASE WHEN UPPER(carton) = 'OUI' THEN 1 ELSE 0 END as accepts_cardboard,\n",
    "  CASE WHEN UPPER(verre) = 'OUI' THEN 1 ELSE 0 END as accepts_glass,\n",
    "  CASE WHEN UPPER(ferraille) = 'OUI' THEN 1 ELSE 0 END as accepts_metal,\n",
    "  CASE WHEN UPPER(papier) = 'OUI' THEN 1 ELSE 0 END as accepts_paper,\n",
    "  0 as accepts_plastic,\n",
    "  CASE WHEN UPPER(textile) = 'OUI' THEN 1 ELSE 0 END as accepts_textile,\n",
    "  CASE WHEN UPPER(bois) = 'OUI' THEN 1 ELSE 0 END as accepts_wood,\n",
    "  CASE WHEN UPPER(dechet_vert) = 'OUI' THEN 1 ELSE 0 END as accepts_vegetation,\n",
    "  'recycling_center' as point_type,\n",
    "  'Nantes_Metropole' as operator,\n",
    "  CURRENT_DATE() as ingestion_date,\n",
    "  '1.0' as schema_version,\n",
    "  'ecopoints' as data_source\n",
    "FROM `{PROJECT}.{DATASET}.ecopoints`\n",
    "WHERE lat IS NOT NULL AND lon IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_recycling = client.query(query_recycling).to_dataframe()\n",
    "    print(f\"âœ… Retrieved {len(df_recycling):,} recycling centers\")\n",
    "\n",
    "    # Show waste acceptance statistics\n",
    "    waste_cols = [col for col in df_recycling.columns if col.startswith('accepts_')]\n",
    "    print(\"   Waste acceptance in recycling centers:\")\n",
    "    for col in waste_cols:\n",
    "        count = df_recycling[col].sum()\n",
    "        if count > 0:\n",
    "            waste_name = col.replace('accepts_', '').replace('_', ' ').title()\n",
    "            print(f\"   â€¢ {waste_name}: {count}/{len(df_recycling)} locations\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error retrieving recycling centers: {e}\")\n",
    "    df_recycling = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14a4581b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. PROCESSING GLASS COLLECTION POINTS\n",
      "----------------------------------------\n",
      "âœ… Retrieved 1,079 glass collection points\n",
      "\n",
      "4. PROCESSING OTHER WASTE TYPES\n",
      "----------------------------------------\n",
      "âœ… Retrieved 1,490 other waste collection points\n",
      "   Breakdown by waste type:\n",
      "   â€¢ Ordure mÃ©nagÃ¨re: 843 points (56.6%)\n",
      "   â€¢ DÃ©chet recyclable: 564 points (37.9%)\n",
      "   â€¢ Papier-carton: 83 points (5.6%)\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 3. GLASS COLLECTION POINTS (PRIORITY 3)\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n3. PROCESSING GLASS COLLECTION POINTS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "query_glass = f\"\"\"\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER () + 30000 as point_id,\n",
    "  CONCAT(\n",
    "    'Glass Collection - ',\n",
    "    COALESCE(\n",
    "      CASE\n",
    "        WHEN type_colonne IS NOT NULL THEN\n",
    "          CASE type_colonne\n",
    "            WHEN 'colonne enterrÃ©e' THEN 'Underground'\n",
    "            WHEN 'colonne aÃ©rienne' THEN 'Above-ground'\n",
    "            ELSE INITCAP(type_colonne)\n",
    "          END\n",
    "        ELSE ''\n",
    "      END,\n",
    "      'Glass Column'\n",
    "    ),\n",
    "    CASE\n",
    "      WHEN commune IS NOT NULL THEN CONCAT(' - ', commune)\n",
    "      ELSE ' - Nantes'\n",
    "    END\n",
    "  ) as name,\n",
    "  COALESCE(adresse, 'Nantes MÃ©tropole') as address,\n",
    "  lat as latitude,\n",
    "  lon as longitude,\n",
    "  0 as accepts_food,\n",
    "  0 as accepts_cardboard,\n",
    "  1 as accepts_glass,\n",
    "  0 as accepts_metal,\n",
    "  0 as accepts_paper,\n",
    "  0 as accepts_plastic,\n",
    "  0 as accepts_textile,\n",
    "  0 as accepts_wood,\n",
    "  0 as accepts_vegetation,\n",
    "  'glass_column' as point_type,\n",
    "  'Nantes_Metropole' as operator,\n",
    "  CURRENT_DATE() as ingestion_date,\n",
    "  '1.0' as schema_version,\n",
    "  'location_dropoff_points_nantes' as data_source\n",
    "FROM `{PROJECT}.{DATASET}.location_dropoff_points_nantes`\n",
    "WHERE\n",
    "  lat IS NOT NULL\n",
    "  AND lon IS NOT NULL\n",
    "  AND LOWER(TRIM(type_dechet)) = 'verre'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_glass = client.query(query_glass).to_dataframe()\n",
    "    print(f\"âœ… Retrieved {len(df_glass):,} glass collection points\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error retrieving glass collection points: {e}\")\n",
    "    df_glass = pd.DataFrame()\n",
    "\n",
    "# ========================================================\n",
    "# 4. OTHER WASTE TYPES (PRIORITY 4)\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n4. PROCESSING OTHER WASTE TYPES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "query_other_waste = f\"\"\"\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER () +\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%papier%carton%' THEN 40000\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%dÃ©chet recyclable%' THEN 50000\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%ordure mÃ©nagÃ¨re%' THEN 60000\n",
    "    ELSE 70000\n",
    "  END as point_id,\n",
    "\n",
    "  CONCAT(\n",
    "    CASE\n",
    "      WHEN LOWER(TRIM(type_dechet)) LIKE '%papier%carton%' THEN 'Paper/Cardboard'\n",
    "      WHEN LOWER(TRIM(type_dechet)) LIKE '%dÃ©chet recyclable%' THEN 'Recyclable Waste'\n",
    "      WHEN LOWER(TRIM(type_dechet)) LIKE '%ordure mÃ©nagÃ¨re%' THEN 'Household Waste'\n",
    "      ELSE INITCAP(type_dechet)\n",
    "    END,\n",
    "    ' Collection - ',\n",
    "    COALESCE(commune, 'Nantes'),\n",
    "    CASE\n",
    "      WHEN type_colonne IS NOT NULL THEN CONCAT(' (',\n",
    "        CASE type_colonne\n",
    "          WHEN 'colonne enterrÃ©e' THEN 'Underground'\n",
    "          WHEN 'colonne aÃ©rienne' THEN 'Above-ground'\n",
    "          ELSE INITCAP(type_colonne)\n",
    "        END, ')')\n",
    "      ELSE ''\n",
    "    END\n",
    "  ) as name,\n",
    "\n",
    "  COALESCE(adresse, 'Nantes MÃ©tropole') as address,\n",
    "  lat as latitude,\n",
    "  lon as longitude,\n",
    "\n",
    "  -- Waste acceptance capabilities\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%papier%carton%' THEN 1\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%dÃ©chet recyclable%' THEN 1\n",
    "    ELSE 0\n",
    "  END as accepts_cardboard,\n",
    "\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%papier%carton%' THEN 1\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%dÃ©chet recyclable%' THEN 1\n",
    "    ELSE 0\n",
    "  END as accepts_paper,\n",
    "\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%dÃ©chet recyclable%' THEN 1\n",
    "    ELSE 0\n",
    "  END as accepts_plastic,\n",
    "\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%dÃ©chet recyclable%' THEN 1\n",
    "    ELSE 0\n",
    "  END as accepts_metal,\n",
    "\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%ordure mÃ©nagÃ¨re%' THEN 1\n",
    "    ELSE 0\n",
    "  END as accepts_miscellaneous,\n",
    "\n",
    "  0 as accepts_food,\n",
    "  0 as accepts_glass,\n",
    "  0 as accepts_textile,\n",
    "  0 as accepts_wood,\n",
    "  0 as accepts_vegetation,\n",
    "\n",
    "  'waste_column' as point_type,\n",
    "  'Nantes_Metropole' as operator,\n",
    "  CURRENT_DATE() as ingestion_date,\n",
    "  '1.0' as schema_version,\n",
    "  'location_dropoff_points_nantes' as data_source,\n",
    "\n",
    "  type_dechet as original_waste_type,\n",
    "  commune as city\n",
    "\n",
    "FROM `{PROJECT}.{DATASET}.location_dropoff_points_nantes`\n",
    "WHERE\n",
    "  lat IS NOT NULL\n",
    "  AND lon IS NOT NULL\n",
    "  AND (\n",
    "    LOWER(TRIM(type_dechet)) LIKE '%papier%carton%'\n",
    "    OR LOWER(TRIM(type_dechet)) LIKE '%dÃ©chet recyclable%'\n",
    "    OR LOWER(TRIM(type_dechet)) LIKE '%ordure mÃ©nagÃ¨re%'\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_other_waste = client.query(query_other_waste).to_dataframe()\n",
    "    print(f\"âœ… Retrieved {len(df_other_waste):,} other waste collection points\")\n",
    "\n",
    "    # Show breakdown\n",
    "    if 'original_waste_type' in df_other_waste.columns:\n",
    "        waste_counts = df_other_waste['original_waste_type'].value_counts()\n",
    "        print(\"   Breakdown by waste type:\")\n",
    "        for waste_type, count in waste_counts.items():\n",
    "            percentage = (count / len(df_other_waste)) * 100\n",
    "            print(f\"   â€¢ {waste_type}: {count:,} points ({percentage:.1f}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error retrieving other waste types: {e}\")\n",
    "    df_other_waste = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7559ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. PROCESSING SPECIAL WASTE COLLECTION POINTS\n",
      "----------------------------------------\n",
      "ðŸ” Checking table schema...\n",
      "âœ… Table has 32 columns\n",
      "\n",
      "ðŸ“‹ Checking for waste type columns:\n",
      "   âœ“ Is_Neon_enabled exists\n",
      "   âœ“ Is_Cartridge_enabled exists\n",
      "   âœ“ Is_Lamp_Light_enabled exists\n",
      "   âœ“ Is_Battery_enabled exists\n",
      "   âœ“ Is_Car_Battery_enabled exists\n",
      "   âœ“ Is_Pile_enabled exists\n",
      "\n",
      "ðŸ“ Generated SQL query with 25 columns\n",
      "âœ… Retrieved 110 special waste collection points\n",
      "\n",
      "ðŸ“Š Special waste acceptance:\n",
      "   â€¢ Neon: 48 points\n",
      "   â€¢ Cartridge: 23 points\n",
      "   â€¢ Lamp: 48 points\n",
      "   â€¢ Battery: 39 points\n",
      "   â€¢ Car Battery: 39 points\n",
      "   â€¢ Pile: 39 points\n",
      "\n",
      "ðŸ‘€ Sample data (first 3 points):\n",
      " point_id               name  accepts_neon  accepts_cartridge  accepts_battery  accepts_car_battery  accepts_pile\n",
      "    90011        Brico DÃ©pÃ´t             1                  0                0                    0             0\n",
      "    90036              Darty             1                  0                0                    0             0\n",
      "    90030 Casino SupermarchÃ©             1                  0                0                    0             0\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 5. SPECIAL WASTE COLLECTION (ECOSYSTEM) - PRIORITY 5\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n5. PROCESSING SPECIAL WASTE COLLECTION POINTS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# First, let's check what columns actually exist in the table\n",
    "print(\"ðŸ” Checking table schema...\")\n",
    "try:\n",
    "    # Get table schema\n",
    "    table_ref = client.dataset(DATASET).table(\"ecosystem_collection_points_with_coords\")\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    # List all columns\n",
    "    existing_columns = [col.name for col in table.schema]\n",
    "    print(f\"âœ… Table has {len(existing_columns)} columns\")\n",
    "\n",
    "    # Check for specific columns\n",
    "    print(\"\\nðŸ“‹ Checking for waste type columns:\")\n",
    "    columns_to_check = [\n",
    "        'Is_Neon_enabled', 'Is_Cartridge_enabled', 'Is_Lamp_Light_enabled',\n",
    "        'Is_Battery_enabled', 'Is_Car_Battery_enabled', 'Is_Pile_enabled'\n",
    "    ]\n",
    "\n",
    "    for col in columns_to_check:\n",
    "        if col in existing_columns:\n",
    "            print(f\"   âœ“ {col} exists\")\n",
    "        else:\n",
    "            print(f\"   âœ— {col} NOT FOUND\")\n",
    "\n",
    "    # Build the query dynamically based on what columns exist\n",
    "    select_columns = []\n",
    "    select_columns.append(\"ID as point_id\")\n",
    "    select_columns.append(\"Name as name\")\n",
    "    select_columns.append(\"Address as address\")\n",
    "    select_columns.append(\"Longitude as longitude\")\n",
    "    select_columns.append(\"Latitude as latitude\")\n",
    "\n",
    "    # Add standard waste columns (always 0 for Ecosystem)\n",
    "    select_columns.append(\"0 as accepts_food\")\n",
    "    select_columns.append(\"0 as accepts_cardboard\")\n",
    "    select_columns.append(\"0 as accepts_glass\")\n",
    "    select_columns.append(\"0 as accepts_metal\")\n",
    "    select_columns.append(\"0 as accepts_paper\")\n",
    "    select_columns.append(\"0 as accepts_plastic\")\n",
    "    select_columns.append(\"0 as accepts_textile\")\n",
    "    select_columns.append(\"0 as accepts_wood\")\n",
    "    select_columns.append(\"0 as accepts_vegetation\")\n",
    "\n",
    "    # Add special waste columns with COALESCE to handle missing columns\n",
    "    if 'Is_Neon_enabled' in existing_columns:\n",
    "        select_columns.append(\"Is_Neon_enabled as accepts_neon\")\n",
    "    else:\n",
    "        select_columns.append(\"0 as accepts_neon\")  # Default if column doesn't exist\n",
    "\n",
    "    if 'Is_Cartridge_enabled' in existing_columns:\n",
    "        select_columns.append(\"Is_Cartridge_enabled as accepts_cartridge\")\n",
    "    else:\n",
    "        select_columns.append(\"0 as accepts_cartridge\")\n",
    "\n",
    "    if 'Is_Lamp_Light_enabled' in existing_columns:\n",
    "        select_columns.append(\"Is_Lamp_Light_enabled as accepts_lamp\")\n",
    "    else:\n",
    "        select_columns.append(\"0 as accepts_lamp\")\n",
    "\n",
    "    if 'Is_Battery_enabled' in existing_columns:\n",
    "        select_columns.append(\"Is_Battery_enabled as accepts_battery\")\n",
    "    else:\n",
    "        select_columns.append(\"0 as accepts_battery\")\n",
    "\n",
    "    if 'Is_Car_Battery_enabled' in existing_columns:\n",
    "        select_columns.append(\"Is_Car_Battery_enabled as accepts_car_battery\")\n",
    "    else:\n",
    "        select_columns.append(\"0 as accepts_car_battery\")\n",
    "\n",
    "    if 'Is_Pile_enabled' in existing_columns:\n",
    "        select_columns.append(\"Is_Pile_enabled as accepts_pile\")\n",
    "    else:\n",
    "        select_columns.append(\"0 as accepts_pile\")\n",
    "\n",
    "    # Add metadata columns\n",
    "    select_columns.append(\"'special_waste' as point_type\")\n",
    "    select_columns.append(\"'Ecosystem' as operator\")\n",
    "    select_columns.append(\"CURRENT_DATE() as ingestion_date\")\n",
    "    select_columns.append(\"'1.0' as schema_version\")\n",
    "    select_columns.append(\"'ecosystem_collection_points_with_coords' as data_source\")\n",
    "\n",
    "    # Build the query\n",
    "    query_special_waste = f\"\"\"\n",
    "    SELECT\n",
    "      {', '.join(select_columns)}\n",
    "    FROM `{PROJECT}.{DATASET}.ecosystem_collection_points_with_coords`\n",
    "    WHERE Latitude IS NOT NULL AND Longitude IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nðŸ“ Generated SQL query with {len(select_columns)} columns\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error checking table schema: {e}\")\n",
    "\n",
    "    # Fallback query if we can't check schema\n",
    "    query_special_waste = f\"\"\"\n",
    "    SELECT\n",
    "      ID as point_id,\n",
    "      Name as name,\n",
    "      Address as address,\n",
    "      Longitude as longitude,\n",
    "      Latitude as latitude,\n",
    "      0 as accepts_food,\n",
    "      0 as accepts_cardboard,\n",
    "      0 as accepts_glass,\n",
    "      0 as accepts_metal,\n",
    "      0 as accepts_paper,\n",
    "      0 as accepts_plastic,\n",
    "      0 as accepts_textile,\n",
    "      0 as accepts_wood,\n",
    "      0 as accepts_vegetation,\n",
    "      -- Try to get special waste columns, use 0 if they don't exist\n",
    "      COALESCE(Is_Neon_enabled, 0) as accepts_neon,\n",
    "      COALESCE(Is_Cartridge_enabled, 0) as accepts_cartridge,\n",
    "      COALESCE(Is_Lamp_Light_enabled, 0) as accepts_lamp,\n",
    "      COALESCE(Is_Battery_enabled, 0) as accepts_battery,\n",
    "      COALESCE(Is_Car_Battery_enabled, 0) as accepts_car_battery,\n",
    "      COALESCE(Is_Pile_enabled, 0) as accepts_pile,\n",
    "      'special_waste' as point_type,\n",
    "      'Ecosystem' as operator,\n",
    "      CURRENT_DATE() as ingestion_date,\n",
    "      '1.0' as schema_version,\n",
    "      'ecosystem_collection_points_with_coords' as data_source\n",
    "    FROM `{PROJECT}.{DATASET}.ecosystem_collection_points_with_coords`\n",
    "    WHERE Latitude IS NOT NULL AND Longitude IS NOT NULL\n",
    "    \"\"\"\n",
    "    print(\"âš ï¸ Using fallback query with COALESCE\")\n",
    "\n",
    "try:\n",
    "    df_special_waste = client.query(query_special_waste).to_dataframe()\n",
    "    print(f\"âœ… Retrieved {len(df_special_waste):,} special waste collection points\")\n",
    "\n",
    "    # Show special waste types\n",
    "    special_cols = [col for col in df_special_waste.columns if col.startswith('accepts_')]\n",
    "    print(\"\\nðŸ“Š Special waste acceptance:\")\n",
    "    for col in special_cols:\n",
    "        count = df_special_waste[col].sum()\n",
    "        if count > 0:\n",
    "            waste_name = col.replace('accepts_', '').replace('_', ' ').title()\n",
    "            print(f\"   â€¢ {waste_name}: {count:,} points\")\n",
    "        elif 'pile' in col.lower() or 'battery' in col.lower():\n",
    "            # Highlight if battery/pile columns exist but have 0 counts\n",
    "            waste_name = col.replace('accepts_', '').replace('_', ' ').title()\n",
    "            print(f\"   âš ï¸ {waste_name}: {count:,} points (might need manual fix)\")\n",
    "\n",
    "    # Check if we have the right data\n",
    "    if 'accepts_pile' in df_special_waste.columns:\n",
    "        pile_count = df_special_waste['accepts_pile'].sum()\n",
    "        if pile_count == 0 and df_special_waste['accepts_car_battery'].sum() > 0:\n",
    "            print(f\"\\nâš ï¸ WARNING: Small batteries (piles) showing 0 points\")\n",
    "            print(\"   Try running this SQL in BigQuery to fix:\")\n",
    "            print(f\"\"\"\n",
    "            UPDATE `{PROJECT}.{DATASET}.ecosystem_collection_points_with_coords`\n",
    "            SET Is_Pile_enabled = 1\n",
    "            WHERE Is_Car_Battery_enabled = 1\n",
    "            \"\"\")\n",
    "\n",
    "    # Show sample data\n",
    "    print(f\"\\nðŸ‘€ Sample data (first 3 points):\")\n",
    "    sample_cols = ['point_id', 'name', 'accepts_neon', 'accepts_cartridge',\n",
    "                   'accepts_battery', 'accepts_car_battery', 'accepts_pile']\n",
    "    sample_cols = [col for col in sample_cols if col in df_special_waste.columns]\n",
    "    print(df_special_waste[sample_cols].head(3).to_string(index=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error retrieving special waste points: {e}\")\n",
    "\n",
    "    # Try a simpler query as last resort\n",
    "    print(\"\\nðŸ”„ Trying simpler query...\")\n",
    "    try:\n",
    "        simple_query = f\"\"\"\n",
    "        SELECT\n",
    "          ID as point_id,\n",
    "          Name as name,\n",
    "          Address as address,\n",
    "          Longitude as longitude,\n",
    "          Latitude as latitude\n",
    "        FROM `{PROJECT}.{DATASET}.ecosystem_collection_points_with_coords`\n",
    "        WHERE Latitude IS NOT NULL AND Longitude IS NOT NULL\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        df_simple = client.query(simple_query).to_dataframe()\n",
    "        print(f\"âœ… Simple query successful: {len(df_simple)} rows\")\n",
    "        print(\"Sample:\")\n",
    "        print(df_simple)\n",
    "\n",
    "        # Check what columns are actually available\n",
    "        print(\"\\nðŸ” To see all available columns, run:\")\n",
    "        print(f\"\"\"\n",
    "        SELECT column_name\n",
    "        FROM `{PROJECT}.{DATASET}.INFORMATION_SCHEMA.COLUMNS`\n",
    "        WHERE table_name = 'ecosystem_collection_points_with_coords'\n",
    "        ORDER BY ordinal_position\n",
    "        \"\"\")\n",
    "\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Simple query also failed: {e2}\")\n",
    "\n",
    "    df_special_waste = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee967ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” FINAL DATASET COLUMNS CHECK:\n",
      "==================================================\n",
      "Final DataFrame columns (25 total):\n",
      "  - point_id\n",
      "  - name\n",
      "  - address\n",
      "  - longitude\n",
      "  - latitude\n",
      "  - accepts_food\n",
      "  - accepts_cardboard\n",
      "  - accepts_glass\n",
      "  - accepts_metal\n",
      "  - accepts_paper\n",
      "  - accepts_plastic\n",
      "  - accepts_textile\n",
      "  - accepts_wood\n",
      "  - accepts_vegetation\n",
      "  - accepts_neon\n",
      "  - accepts_cartridge\n",
      "  - accepts_lamp\n",
      "  - accepts_battery\n",
      "  - accepts_car_battery\n",
      "  - accepts_pile\n",
      "  - point_type\n",
      "  - operator\n",
      "  - ingestion_date\n",
      "  - schema_version\n",
      "  - data_source\n",
      "\n",
      "ðŸ”‹ Battery-related columns:\n",
      "  - accepts_battery: 39 locations accept this type\n",
      "  - accepts_car_battery: 39 locations accept this type\n",
      "  - accepts_pile: 39 locations accept this type\n"
     ]
    }
   ],
   "source": [
    "# Add this at the end of your processing script\n",
    "print(\"\\nðŸ” FINAL DATASET COLUMNS CHECK:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Final DataFrame columns ({len(df_special_waste.columns)} total):\")\n",
    "for col in df_special_waste.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Check battery-related columns specifically\n",
    "battery_cols = [col for col in df_special_waste.columns if 'battery' in col.lower() or 'pile' in col.lower()]\n",
    "print(f\"\\nðŸ”‹ Battery-related columns:\")\n",
    "for col in battery_cols:\n",
    "    print(f\"  - {col}: {df_special_waste[col].sum()} locations accept this type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61546e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. PROCESSING TEXTILE COLLECTION POINTS\n",
      "----------------------------------------\n",
      "   Loaded with latin-1 encoding\n",
      "âœ… Processed 68 textile collection points\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 6. TEXTILE COLLECTION POINTS (CSV) - PRIORITY 6\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n6. PROCESSING TEXTILE COLLECTION POINTS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def load_textile_data():\n",
    "    \"\"\"Load textile collection points from CSV file\"\"\"\n",
    "    textile_file = \"Textile_relais.csv\"\n",
    "\n",
    "    if not os.path.exists(textile_file):\n",
    "        print(f\"âš ï¸ Textile file not found: {textile_file}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        # Try multiple encodings\n",
    "        encodings = ['latin-1', 'utf-8', 'iso-8859-1', 'cp1252']\n",
    "\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(textile_file, encoding=encoding, on_bad_lines='skip')\n",
    "                print(f\"   Loaded with {encoding} encoding\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"   Could not read textile file with any encoding\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Standardize column names\n",
    "        column_mapping = {}\n",
    "        for col in df.columns:\n",
    "            col_lower = str(col).lower().strip()\n",
    "            if 'name' in col_lower or 'nom' in col_lower:\n",
    "                column_mapping[col] = 'name'\n",
    "            elif 'adresse' in col_lower or 'address' in col_lower:\n",
    "                column_mapping[col] = 'address'\n",
    "            elif 'latitude' in col_lower or 'lat' in col_lower:\n",
    "                column_mapping[col] = 'latitude'\n",
    "            elif 'longitude' in col_lower or 'lon' in col_lower or 'long' in col_lower:\n",
    "                column_mapping[col] = 'longitude'\n",
    "            elif 'ville' in col_lower or 'city' in col_lower:\n",
    "                column_mapping[col] = 'city'\n",
    "\n",
    "        if column_mapping:\n",
    "            df = df.rename(columns=column_mapping)\n",
    "\n",
    "        # Clean coordinates\n",
    "        if 'latitude' in df.columns:\n",
    "            df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "        if 'longitude' in df.columns:\n",
    "            df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "\n",
    "        # Filter out invalid coordinates\n",
    "        if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "            df = df[df['latitude'].notna() & df['longitude'].notna()].copy()\n",
    "\n",
    "        # Add waste type and metadata\n",
    "        if not df.empty:\n",
    "            next_id = 80000  # Starting ID for textile points\n",
    "            df['point_id'] = range(next_id, next_id + len(df))\n",
    "            df['accepts_textile'] = 1\n",
    "            df['point_type'] = 'textile_collection'\n",
    "            df['operator'] = 'Le Relais'\n",
    "            df['ingestion_date'] = pd.Timestamp.now().date()\n",
    "            df['schema_version'] = '1.0'\n",
    "            df['data_source'] = 'Textile_relais.csv'\n",
    "\n",
    "            # Add other waste types as 0\n",
    "            waste_columns = ['accepts_food', 'accepts_cardboard', 'accepts_glass',\n",
    "                           'accepts_metal', 'accepts_paper', 'accepts_plastic',\n",
    "                           'accepts_wood', 'accepts_vegetation']\n",
    "            for col in waste_columns:\n",
    "                df[col] = 0\n",
    "\n",
    "        print(f\"âœ… Processed {len(df)} textile collection points\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing textile data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "df_textile = load_textile_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "960460a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. PROCESSING PHARMACY/GARAGE/RESSOURCERIE POINTS\n",
      "----------------------------------------\n",
      "   Loaded with utf-8 encoding\n",
      "   Breakdown by type:\n",
      "   â€¢ pharmacy: 139 points\n",
      "   â€¢ car_repair: 108 points\n",
      "   â€¢ ressourcerie: 7 points\n",
      "âœ… Processed 254 mixed waste collection points\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 7. PHARMACY/GARAGE/RESSOURCERIE (CSV) - PRIORITY 7\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n7. PROCESSING PHARMACY/GARAGE/RESSOURCERIE POINTS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def load_mixed_data():\n",
    "    \"\"\"Load mixed waste collection points from CSV file\"\"\"\n",
    "    mixed_file = \"pharmacies_garages_ressourceries_nantes.csv\"\n",
    "\n",
    "    if not os.path.exists(mixed_file):\n",
    "        print(f\"âš ï¸ Mixed waste file not found: {mixed_file}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        # Try multiple encodings\n",
    "        encodings = ['utf-8', 'latin-1', 'utf-8-sig', 'cp1252']\n",
    "\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(mixed_file, encoding=encoding, on_bad_lines='skip')\n",
    "                print(f\"   Loaded with {encoding} encoding\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            # Try with semicolon delimiter\n",
    "            try:\n",
    "                df = pd.read_csv(mixed_file, sep=';', encoding='latin-1')\n",
    "                print(f\"   Loaded with semicolon delimiter\")\n",
    "            except:\n",
    "                print(f\"   Could not read mixed waste file\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        # Standardize column names\n",
    "        column_mapping = {}\n",
    "        for col in df.columns:\n",
    "            col_lower = str(col).lower().strip()\n",
    "            if 'name' in col_lower or 'nom' in col_lower:\n",
    "                column_mapping[col] = 'name'\n",
    "            elif 'type' in col_lower or 'categorie' in col_lower:\n",
    "                column_mapping[col] = 'type'\n",
    "            elif 'latitude' in col_lower or 'lat' in col_lower:\n",
    "                column_mapping[col] = 'latitude'\n",
    "            elif 'longitude' in col_lower or 'lon' in col_lower or 'long' in col_lower:\n",
    "                column_mapping[col] = 'longitude'\n",
    "            elif 'adresse' in col_lower or 'address' in col_lower:\n",
    "                column_mapping[col] = 'address'\n",
    "\n",
    "        if column_mapping:\n",
    "            df = df.rename(columns=column_mapping)\n",
    "\n",
    "        # Clean data\n",
    "        df['name'] = df['name'].fillna('').astype(str).str.strip()\n",
    "        if 'type' in df.columns:\n",
    "            df['type'] = df['type'].fillna('').astype(str).str.lower().str.strip()\n",
    "\n",
    "        # Clean coordinates\n",
    "        if 'latitude' in df.columns:\n",
    "            df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "        if 'longitude' in df.columns:\n",
    "            df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "\n",
    "        # Filter out invalid coordinates\n",
    "        if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "            df = df[df['latitude'].notna() & df['longitude'].notna()].copy()\n",
    "\n",
    "        # Add waste types based on type column\n",
    "        if not df.empty:\n",
    "            next_id = 90000  # Starting ID for mixed points\n",
    "            df['point_id'] = range(next_id, next_id + len(df))\n",
    "\n",
    "            # Initialize all waste columns to 0\n",
    "            waste_columns = [\n",
    "                'accepts_food', 'accepts_cardboard', 'accepts_glass',\n",
    "                'accepts_metal', 'accepts_paper', 'accepts_plastic',\n",
    "                'accepts_textile', 'accepts_wood', 'accepts_vegetation',\n",
    "                'accepts_pharmacy', 'accepts_tire', 'accepts_ressourcerie'\n",
    "            ]\n",
    "\n",
    "            for col in waste_columns:\n",
    "                df[col] = 0\n",
    "\n",
    "            # Set appropriate columns based on type\n",
    "            if 'type' in df.columns:\n",
    "                # Pharmacy points\n",
    "                pharmacy_mask = df['type'].str.contains('pharmacy|pharmacie', case=False, na=False)\n",
    "                df.loc[pharmacy_mask, 'accepts_pharmacy'] = 1\n",
    "                df.loc[pharmacy_mask, 'point_type'] = 'pharmacy'\n",
    "\n",
    "                # Car repair/garage points (tire)\n",
    "                tire_mask = df['type'].str.contains('car_repair|garage|tire|pneu', case=False, na=False)\n",
    "                df.loc[tire_mask, 'accepts_tire'] = 1\n",
    "                df.loc[tire_mask, 'point_type'] = 'car_repair'\n",
    "\n",
    "                # Ressourcerie points\n",
    "                ressourcerie_mask = df['type'].str.contains('ressourcerie|recyclerie', case=False, na=False)\n",
    "                df.loc[ressourcerie_mask, 'accepts_ressourcerie'] = 1\n",
    "                df.loc[ressourcerie_mask, 'point_type'] = 'ressourcerie'\n",
    "\n",
    "            # Set default point_type for unclassified\n",
    "            if 'point_type' not in df.columns:\n",
    "                df['point_type'] = 'other'\n",
    "\n",
    "            df['operator'] = 'Various'\n",
    "            df['ingestion_date'] = pd.Timestamp.now().date()\n",
    "            df['schema_version'] = '1.0'\n",
    "            df['data_source'] = 'pharmacies_garages_ressourceries_nantes.csv'\n",
    "\n",
    "        # Show breakdown\n",
    "        if 'type' in df.columns:\n",
    "            unique_types = df['type'].value_counts()\n",
    "            print(\"   Breakdown by type:\")\n",
    "            for type_val, count in unique_types.items():\n",
    "                print(f\"   â€¢ {type_val}: {count:,} points\")\n",
    "\n",
    "        print(f\"âœ… Processed {len(df)} mixed waste collection points\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing mixed waste data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "df_mixed = load_mixed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb8d970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMBINING ALL DATA SOURCES\n",
      "============================================================\n",
      "âœ… Food Waste: 1,644 points\n",
      "âœ… Recycling Centers: 15 points\n",
      "âœ… Glass Collection: 1,079 points\n",
      "âœ… Other Waste Types: 1,490 points\n",
      "âœ… Special Waste: 110 points\n",
      "âœ… Textile Collection: 68 points\n",
      "âœ… Mixed Waste (Pharmacy/Garage/Ressourcerie): 254 points\n",
      "\n",
      "ðŸ“Š DATASET COMPOSITION:\n",
      "   Total points: 4,660\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 8. COMBINE ALL DATA SOURCES\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMBINING ALL DATA SOURCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect all dataframes\n",
    "all_dataframes = []\n",
    "\n",
    "dataframes_info = [\n",
    "    (df_food, \"Food Waste\"),\n",
    "    (df_recycling, \"Recycling Centers\"),\n",
    "    (df_glass, \"Glass Collection\"),\n",
    "    (df_other_waste, \"Other Waste Types\"),\n",
    "    (df_special_waste, \"Special Waste\"),\n",
    "    (df_textile, \"Textile Collection\"),\n",
    "    (df_mixed, \"Mixed Waste (Pharmacy/Garage/Ressourcerie)\")\n",
    "]\n",
    "\n",
    "for df, name in dataframes_info:\n",
    "    if not df.empty:\n",
    "        all_dataframes.append(df)\n",
    "        print(f\"âœ… {name}: {len(df):,} points\")\n",
    "\n",
    "if not all_dataframes:\n",
    "    print(\"âŒ No data to combine!\")\n",
    "    exit()\n",
    "\n",
    "# Combine all data\n",
    "combined_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"\\nðŸ“Š DATASET COMPOSITION:\")\n",
    "print(f\"   Total points: {len(combined_df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de0519b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STANDARDIZING AND CLEANING DATA\n",
      "============================================================\n",
      "   Standardized: accepts_food\n",
      "   Standardized: accepts_cardboard\n",
      "   Standardized: accepts_glass\n",
      "   Standardized: accepts_metal\n",
      "   Standardized: accepts_paper\n",
      "   Standardized: accepts_plastic\n",
      "   Standardized: accepts_textile\n",
      "   Standardized: accepts_wood\n",
      "   Standardized: accepts_vegetation\n",
      "   Standardized: accepts_miscellaneous\n",
      "   Standardized: accepts_neon\n",
      "   Standardized: accepts_cartridge\n",
      "   Standardized: accepts_lamp\n",
      "   Standardized: accepts_battery\n",
      "   Standardized: accepts_car_battery\n",
      "   Standardized: accepts_pile\n",
      "   Standardized: accepts_pharmacy\n",
      "   Standardized: accepts_tire\n",
      "   Standardized: accepts_ressourcerie\n",
      "âœ… All points have coordinates\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 9. STANDARDIZE AND CLEAN DATA\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STANDARDIZING AND CLEANING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ensure all waste columns are integers (0 or 1)\n",
    "waste_columns = [col for col in combined_df.columns if col.startswith('accepts_')]\n",
    "for col in waste_columns:\n",
    "    if col in combined_df.columns:\n",
    "        combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce').fillna(0).astype(int)\n",
    "        print(f\"   Standardized: {col}\")\n",
    "\n",
    "# Ensure coordinates are numeric\n",
    "if 'latitude' in combined_df.columns:\n",
    "    combined_df['latitude'] = pd.to_numeric(combined_df['latitude'], errors='coerce')\n",
    "if 'longitude' in combined_df.columns:\n",
    "    combined_df['longitude'] = pd.to_numeric(combined_df['longitude'], errors='coerce')\n",
    "\n",
    "# Check for missing coordinates\n",
    "if 'latitude' in combined_df.columns and 'longitude' in combined_df.columns:\n",
    "    missing_coords = combined_df['latitude'].isna().sum() + combined_df['longitude'].isna().sum()\n",
    "    if missing_coords > 0:\n",
    "        print(f\"âš ï¸  {missing_coords} points missing coordinates\")\n",
    "    else:\n",
    "        print(f\"âœ… All points have coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b453c9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING UNIFIED SCHEMA\n",
      "============================================================\n",
      "   Schema created with 32 columns\n",
      "   Waste types tracked: 19\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 10. CREATE UNIFIED SCHEMA\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING UNIFIED SCHEMA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define core columns (always present)\n",
    "core_columns = [\n",
    "    'point_id', 'name', 'address', 'latitude', 'longitude',\n",
    "    'point_type', 'operator', 'ingestion_date', 'schema_version', 'data_source'\n",
    "]\n",
    "\n",
    "# Get all waste columns that exist\n",
    "existing_waste_cols = [col for col in waste_columns if col in combined_df.columns]\n",
    "\n",
    "# Get all other columns\n",
    "other_columns = [col for col in combined_df.columns\n",
    "                if col not in core_columns and col not in existing_waste_cols]\n",
    "\n",
    "# Create final column order\n",
    "final_columns = core_columns + sorted(existing_waste_cols) + sorted(other_columns)\n",
    "\n",
    "# Ensure all columns exist in dataframe\n",
    "for col in final_columns:\n",
    "    if col not in combined_df.columns:\n",
    "        if col.startswith('accepts_'):\n",
    "            combined_df[col] = 0\n",
    "        else:\n",
    "            combined_df[col] = None\n",
    "\n",
    "# Reorder columns\n",
    "combined_df = combined_df[final_columns]\n",
    "\n",
    "print(f\"   Schema created with {len(final_columns)} columns\")\n",
    "print(f\"   Waste types tracked: {len(existing_waste_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5590226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING LOCAL BACKUP\n",
      "============================================================\n",
      "âœ… CSV saved: 'trash_collection_points_complete.csv' (4,660 rows)\n",
      "   CSV verification: 5 rows read successfully\n",
      "\n",
      "ðŸ“‹ CSV SAMPLE (first 3 rows):\n",
      "   ID 1: Food Waste - Nantes... [food_waste] â†’ Food\n",
      "   ID 2: Food Waste - Nantes... [food_waste] â†’ Food\n",
      "   ID 3: Food Waste - Nantes... [food_waste] â†’ Food\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 11. SAVE TO CSV (LOCAL BACKUP)\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING LOCAL BACKUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "output_csv = \"trash_collection_points_complete.csv\"\n",
    "try:\n",
    "    combined_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… CSV saved: '{output_csv}' ({len(combined_df):,} rows)\")\n",
    "\n",
    "    # Verify CSV\n",
    "    csv_check = pd.read_csv(output_csv, nrows=5)\n",
    "    print(f\"   CSV verification: {len(csv_check)} rows read successfully\")\n",
    "\n",
    "    # Show sample\n",
    "    print(f\"\\nðŸ“‹ CSV SAMPLE (first 3 rows):\")\n",
    "    sample = combined_df.head(3)\n",
    "    for i, row in sample.iterrows():\n",
    "        point_id = row.get('point_id', 'N/A')\n",
    "        name = str(row.get('name', 'Unnamed'))[:40]\n",
    "        point_type = row.get('point_type', 'Unknown')\n",
    "\n",
    "        # Find accepted waste types\n",
    "        accepted = []\n",
    "        for col in existing_waste_cols:\n",
    "            if row.get(col, 0) == 1:\n",
    "                waste_name = col.replace('accepts_', '').replace('_', ' ').title()\n",
    "                accepted.append(waste_name)\n",
    "\n",
    "        accepted_str = ', '.join(accepted[:2]) + ('...' if len(accepted) > 2 else '')\n",
    "        print(f\"   ID {point_id}: {name}... [{point_type}] â†’ {accepted_str}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77565070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "UPLOADING TO BIGQUERY\n",
      "============================================================\n",
      "â„¹ï¸ Table trash-optimizer-479913.nantes.trash_collection_points already exists (4,660 rows)\n",
      "âœ… Backup created: trash-optimizer-479913.nantes.trash_collection_points_backup_20251207_200545\n",
      "ðŸ“¤ Uploading 4,660 rows to BigQuery...\n",
      "âœ… BigQuery table created/updated: trash-optimizer-479913.nantes.trash_collection_points\n",
      "   â€¢ Rows: 4,660\n",
      "   â€¢ Size: 1.42 MB\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 12. UPLOAD TO BIGQUERY\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPLOADING TO BIGQUERY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "table_id = f\"{PROJECT}.{DATASET}.{TARGET_TABLE}\"\n",
    "\n",
    "# Check if table exists, create backup if it does\n",
    "try:\n",
    "    existing_table = client.get_table(table_id)\n",
    "    print(f\"â„¹ï¸ Table {table_id} already exists ({existing_table.num_rows:,} rows)\")\n",
    "\n",
    "    # Create backup\n",
    "    backup_table = f\"{PROJECT}.{DATASET}.{TARGET_TABLE}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "    backup_query = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{backup_table}` AS\n",
    "    SELECT * FROM `{table_id}`\n",
    "    \"\"\"\n",
    "\n",
    "    client.query(backup_query).result()\n",
    "    print(f\"âœ… Backup created: {backup_table}\")\n",
    "\n",
    "except NotFound:\n",
    "    print(f\"â„¹ï¸ Table {table_id} does not exist, will create new\")\n",
    "\n",
    "# Upload data to BigQuery\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # Replace entire table\n",
    "    autodetect=True,                     # Auto-detect schema\n",
    "    max_bad_records=10                   # Allow some bad records\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(f\"ðŸ“¤ Uploading {len(combined_df):,} rows to BigQuery...\")\n",
    "    job = client.load_table_from_dataframe(combined_df, table_id, job_config=job_config)\n",
    "    job.result()  # Wait for the job to complete\n",
    "\n",
    "    # Verify upload\n",
    "    table = client.get_table(table_id)\n",
    "    print(f\"âœ… BigQuery table created/updated: {table_id}\")\n",
    "    print(f\"   â€¢ Rows: {table.num_rows:,}\")\n",
    "    print(f\"   â€¢ Size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ BigQuery upload failed: {e}\")\n",
    "\n",
    "    # Try alternative method\n",
    "    try:\n",
    "        print(\"ðŸ”„ Trying alternative upload method...\")\n",
    "        import pandas_gbq\n",
    "        pandas_gbq.to_gbq(\n",
    "            combined_df,\n",
    "            destination_table=table_id,\n",
    "            project_id=PROJECT,\n",
    "            if_exists='replace',\n",
    "            progress_bar=True\n",
    "        )\n",
    "        print(\"âœ… Upload successful via pandas_gbq!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Alternative also failed: {e2}\")\n",
    "        print(\"\\nðŸ“‹ Manual upload instructions:\")\n",
    "        print(f\"   1. Go to BigQuery Console: https://console.cloud.google.com/bigquery\")\n",
    "        print(f\"   2. Select project: {PROJECT}\")\n",
    "        print(f\"   3. Select dataset: {DATASET}\")\n",
    "        print(f\"   4. Create table: {TARGET_TABLE}\")\n",
    "        print(f\"   5. Upload file: {output_csv}\")\n",
    "        print(f\"   6. Enable schema autodetection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "300fd014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING STATISTICS AND VERIFICATION\n",
      "============================================================\n",
      "ðŸ“Š VERIFICATION RESULTS:\n",
      "   â€¢ Total points: 4,660\n",
      "   â€¢ With coordinates: 4,660\n",
      "\n",
      "   â€¢ Waste type coverage (top 10):\n",
      "     - Food              :  1,644 ( 35.3%)\n",
      "     - Glass             :  1,093 ( 23.5%)\n",
      "     - Miscellaneous     :    843 ( 18.1%)\n",
      "     - Cardboard         :    662 ( 14.2%)\n",
      "     - Paper             :    662 ( 14.2%)\n",
      "     - Metal             :    578 ( 12.4%)\n",
      "     - Plastic           :    564 ( 12.1%)\n",
      "     - Textile           :     77 (  1.7%)\n",
      "     - Wood              :     15 (  0.3%)\n",
      "     - Vegetation        :     14 (  0.3%)\n",
      "     ... and 9 more waste types\n",
      "\n",
      "   â€¢ Point type distribution (top 10):\n",
      "     - food_waste               :  1,644 ( 35.3%)\n",
      "     - waste_column             :  1,490 ( 32.0%)\n",
      "     - glass_column             :  1,079 ( 23.2%)\n",
      "     - pharmacy                 :    139 (  3.0%)\n",
      "     - special_waste            :    110 (  2.4%)\n",
      "     - car_repair               :    108 (  2.3%)\n",
      "     - textile_collection       :     68 (  1.5%)\n",
      "     - recycling_center         :     15 (  0.3%)\n",
      "     - ressourcerie             :      7 (  0.2%)\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 13. VERIFICATION AND STATISTICS\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING STATISTICS AND VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Run verification query\n",
    "    verify_query = f\"\"\"\n",
    "    SELECT\n",
    "      COUNT(*) as total_points,\n",
    "      SUM(CASE WHEN latitude IS NOT NULL AND longitude IS NOT NULL THEN 1 ELSE 0 END) as points_with_coords,\n",
    "      {', '.join([f'SUM({col}) as {col}_count' for col in existing_waste_cols[:10]])}\n",
    "    FROM `{table_id}`\n",
    "    \"\"\"\n",
    "\n",
    "    result = client.query(verify_query).to_dataframe().iloc[0]\n",
    "\n",
    "    print(f\"ðŸ“Š VERIFICATION RESULTS:\")\n",
    "    print(f\"   â€¢ Total points: {result['total_points']:,}\")\n",
    "    print(f\"   â€¢ With coordinates: {result['points_with_coords']:,}\")\n",
    "\n",
    "    print(f\"\\n   â€¢ Waste type coverage (top 10):\")\n",
    "    waste_counts = []\n",
    "    for col in existing_waste_cols[:10]:\n",
    "        count = int(result.get(f'{col}_count', 0))\n",
    "        if count > 0:\n",
    "            waste_name = col.replace('accepts_', '').replace('_', ' ').title()\n",
    "            percentage = (count / result['total_points']) * 100\n",
    "            waste_counts.append((waste_name, count, percentage))\n",
    "\n",
    "    # Sort by count descending\n",
    "    waste_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for waste_name, count, percentage in waste_counts:\n",
    "        print(f\"     - {waste_name:18s}: {count:6,d} ({percentage:5.1f}%)\")\n",
    "\n",
    "    if len(existing_waste_cols) > 10:\n",
    "        print(f\"     ... and {len(existing_waste_cols) - 10} more waste types\")\n",
    "\n",
    "    # Point type distribution\n",
    "    type_query = f\"\"\"\n",
    "    SELECT\n",
    "      point_type,\n",
    "      COUNT(*) as count,\n",
    "      ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 1) as percentage\n",
    "    FROM `{table_id}`\n",
    "    WHERE point_type IS NOT NULL\n",
    "    GROUP BY point_type\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "\n",
    "    type_results = client.query(type_query).to_dataframe()\n",
    "\n",
    "    print(f\"\\n   â€¢ Point type distribution (top 10):\")\n",
    "    for _, row in type_results.iterrows():\n",
    "        print(f\"     - {row['point_type']:25s}: {row['count']:6,d} ({row['percentage']:5.1f}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Verification query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7006804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET CREATION COMPLETE!\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ SUMMARY:\n",
      "   â€¢ Total collection points: 4,660\n",
      "   â€¢ Data sources integrated: 7\n",
      "   â€¢ Waste types tracked: 19\n",
      "   â€¢ BigQuery table: trash-optimizer-479913.nantes.trash_collection_points\n",
      "   â€¢ Local backup: trash_collection_points_complete.csv\n",
      "\n",
      "ðŸ” DATA QUALITY CHECK:\n",
      "   â€¢ All waste columns standardized to 0/1\n",
      "   â€¢ Coordinates validated and cleaned\n",
      "   â€¢ Coordinate completeness: 100.0%\n",
      "\n",
      "ðŸ’¡ RECOMMENDATIONS:\n",
      "   1. Verify data in BigQuery Console\n",
      "   2. Test queries on the new dataset\n",
      "   3. Schedule regular updates with new data\n",
      "   4. Consider adding geospatial indexes for better performance\n",
      "\n",
      "============================================================\n",
      "PROCESS COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Statistics saved to: ingestion_statistics.json\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# 14. FINAL SUMMARY\n",
    "# ========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET CREATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ SUMMARY:\")\n",
    "print(f\"   â€¢ Total collection points: {len(combined_df):,}\")\n",
    "print(f\"   â€¢ Data sources integrated: {len(all_dataframes)}\")\n",
    "print(f\"   â€¢ Waste types tracked: {len(existing_waste_cols)}\")\n",
    "print(f\"   â€¢ BigQuery table: {table_id}\")\n",
    "print(f\"   â€¢ Local backup: {output_csv}\")\n",
    "\n",
    "# Data quality check\n",
    "print(f\"\\nðŸ” DATA QUALITY CHECK:\")\n",
    "print(f\"   â€¢ All waste columns standardized to 0/1\")\n",
    "print(f\"   â€¢ Coordinates validated and cleaned\")\n",
    "\n",
    "if 'latitude' in combined_df.columns and 'longitude' in combined_df.columns:\n",
    "    valid_coords = combined_df['latitude'].notna().sum()\n",
    "    coord_percentage = (valid_coords / len(combined_df)) * 100\n",
    "    print(f\"   â€¢ Coordinate completeness: {coord_percentage:.1f}%\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "print(f\"   1. Verify data in BigQuery Console\")\n",
    "print(f\"   2. Test queries on the new dataset\")\n",
    "print(f\"   3. Schedule regular updates with new data\")\n",
    "print(f\"   4. Consider adding geospatial indexes for better performance\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save final statistics\n",
    "stats = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'total_points': len(combined_df),\n",
    "    'data_sources': len(all_dataframes),\n",
    "    'waste_types': len(existing_waste_cols),\n",
    "    'table_name': table_id,\n",
    "    'local_backup': output_csv,\n",
    "    'schema_version': '1.0'\n",
    "}\n",
    "\n",
    "stats_file = \"ingestion_statistics.json\"\n",
    "with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nðŸ“Š Statistics saved to: {stats_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trash-optimizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
