{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e45445c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f3c6b618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Creating optimized trash collection points table\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# SET CREDENTIALS\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/dariaserbichenko/code/DariaSerb/key-gcp/trash-optimizer-479913-91e59ecc96c9.json\"\n",
    "\n",
    "PROJECT = \"trash-optimizer-479913\"\n",
    "DATASET = \"nantes\"\n",
    "client = bigquery.Client(project=PROJECT)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Creating optimized trash collection points table\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b97fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "1. Querying alimentary garbage (food waste)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 1ST QUERY FOR ALIMENTARY GARBAGE (FOOD WASTE)\n",
    "\n",
    "# Table of BIGQUERY (definition all columns with 0/1 values for each waste type)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. Querying alimentary garbage (food waste)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query1 = f\"\"\"\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER () as ID,\n",
    "  CONCAT('Food Waste - ', COALESCE(commune, 'Nantes')) as Name,\n",
    "  COALESCE(adresse, 'Address not specified') as Address,\n",
    "  lon as Longitude,\n",
    "  lat as Latitude,\n",
    "  0 as Is_Cardboard_enabled,\n",
    "  1 as Is_Food_enabled,\n",
    "  0 as Is_Glass_enabled,\n",
    "  0 as Is_Metal_enabled,\n",
    "  0 as Is_Paper_enabled,\n",
    "  0 as Is_Wood_enabled,\n",
    "  0 as Is_Plastic_enabled,\n",
    "  0 as Is_Textile_enabled,\n",
    "  0 as Is_Vegetation_enabled,\n",
    "  0 as Is_Neon_enabled,\n",
    "  0 as Is_Cartridge_enabled,\n",
    "  0 as Is_Lamp_Light_enabled,\n",
    "  0 as Is_Pile_enabled,\n",
    "  0 as Is_Battery_enabled,\n",
    "  0 as Is_Car_Battery_enabled,\n",
    "  0 as Is_Miscellanous_Trash_enabled,\n",
    "  0 as Is_Pharmacy_enabled,\n",
    "  0 as Is_Tire_enabled,\n",
    "  0 as Is_Ressourcerie_enabled\n",
    "FROM `{PROJECT}.{DATASET}.alimentary_garbage_clean`\n",
    "WHERE lat IS NOT NULL AND lon IS NOT NULL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d72dcd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1,644 food waste locations\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df1 = client.query(query1).to_dataframe()\n",
    "    print(f\"Retrieved {len(df1):,} food waste locations\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    df1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e5a98cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "2. Querying ecopoints with actual columns\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 2ND QUERY FOR ECOPOINTS WITH ACTUAL COLUMNS\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\n2. Querying ecopoints with actual columns\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# From inspection: columns are ['bois', 'carton', 'ferraille', 'cartouche', 'neon', 'papier', 'textile', 'verre' etc.]\n",
    "\n",
    "query2 = f\"\"\"\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER () + 10000 as ID,\n",
    "  CONCAT('Recycling Center - ', COALESCE(nom, commune, 'Ecopoint')) as Name,\n",
    "  COALESCE(adresse, 'Address not specified') as Address,\n",
    "  lon as Longitude,\n",
    "  lat as Latitude,\n",
    "\n",
    "  -- Use actual columns found\n",
    "\n",
    "  CASE WHEN UPPER(carton) = 'OUI' THEN 1 ELSE 0 END as Is_Cardboard_enabled,\n",
    "  0 as Is_Food_enabled,     -- 0 in food waste column\n",
    "  CASE WHEN UPPER(verre) = 'OUI' THEN 1 ELSE 0 END as Is_Glass_enabled,\n",
    "  CASE WHEN UPPER(ferraille) = 'OUI' THEN 1 ELSE 0 END as Is_Metal_enabled,\n",
    "  CASE WHEN UPPER(papier) = 'OUI' THEN 1 ELSE 0 END as Is_Paper_enabled,\n",
    "  CASE WHEN UPPER(bois) = 'OUI' THEN 1 ELSE 0 END as Is_Wood_enabled,\n",
    "  0 as Is_Plastic_enabled,  -- 0 in plastique column\n",
    "  CASE WHEN UPPER(textile) = 'OUI' THEN 1 ELSE 0 END as Is_Textile_enabled,\n",
    "  CASE WHEN UPPER(dechet_vert) = 'OUI' THEN 1 ELSE 0 END as Is_Vegetation_enabled,\n",
    "  CASE WHEN UPPER(neon) = 'OUI' THEN 1 ELSE 0 END as Is_Neon_enabled,\n",
    "  CASE WHEN UPPER(cartouche) = 'OUI' THEN 1 ELSE 0 END as Is_Cartridge_enabled,\n",
    "  0 as Is_Lamp_Light_enabled,  -- 0 in ampoule column\n",
    "  CASE WHEN UPPER(pile) = 'OUI' THEN 1 ELSE 0 END as Is_Pile_enabled,\n",
    "  CASE WHEN UPPER(batterie) = 'OUI' THEN 1 ELSE 0 END as Is_Car_Battery_enabled,\n",
    "  0 as Is_Miscellanous_Trash_enabled,  -- 0 in divers column\n",
    "  0 as Is_Pharmacy_enabled,            -- 0 in pharmacie column\n",
    "  CASE WHEN UPPER(pneus) = 'OUI' THEN 1 ELSE 0 END as Is_Tire_enabled,\n",
    "  0 as Is_Ressourcerie_enabled         -- 0 in ressourcerie column\n",
    "\n",
    "FROM `{PROJECT}.{DATASET}.ecopoints`\n",
    "WHERE lat IS NOT NULL AND lon IS NOT NULL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d33dfd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 15 recycling centers with actual waste types\n",
      "- Waste acceptance in recycling centers:\n",
      "   Cardboard: 15/15 locations\n",
      "   Glass: 14/15 locations\n",
      "   Metal: 14/15 locations\n",
      "   Paper: 15/15 locations\n",
      "   Wood: 15/15 locations\n",
      "   Textile: 9/15 locations\n",
      "   Vegetation: 14/15 locations\n",
      "   Neon: 8/15 locations\n",
      "   Cartridge: 15/15 locations\n",
      "   Pile: 13/15 locations\n",
      "   Car Battery: 14/15 locations\n",
      "   Tire: 2/15 locations\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df2 = client.query(query2).to_dataframe()\n",
    "    print(f\"Retrieved {len(df2)} recycling centers with actual waste types\")\n",
    "\n",
    "    # Check acceptance rates\n",
    "\n",
    "    waste_cols = [col for col in df2.columns if col.startswith('Is_')]\n",
    "    print(f\"- Waste acceptance in recycling centers:\")\n",
    "    for col in waste_cols:\n",
    "        count = df2[col].sum()\n",
    "        if count > 0:\n",
    "            waste_name = col.replace('Is_', '').replace('_enabled', '').replace('_', ' ').title()\n",
    "            print(f\"   {waste_name}: {count}/{len(df2)} locations\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    df2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcfddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "3. Querying glass collection columns (Verre only)\n",
      "============================================================\n",
      "Retrieved 1,079 glass collection columns\n",
      "============================================================\n",
      "GLASS COLUMNS SUMMARY:\n",
      "============================================================\n",
      "  Total glass columns: 1,079\n",
      "  With valid coordinates: 1,079\n",
      "SAMPLE GLASS COLUMNS (first 3):\n",
      "  1. Drop-off points - Underground - Nantes\n",
      "     Address: Rue de la petite Sensive\n",
      "     Location: (47.260437, -1.561580)\n",
      "     Glass enabled: ‚úì\n",
      "  2. Drop-off points - Underground - Nantes\n",
      "     Address: Rue Blaise Pascal\n",
      "     Location: (47.256204, -1.566761)\n",
      "     Glass enabled: ‚úì\n",
      "  3. Drop-off points - Underground - Nantes\n",
      "     Address: 2 Rue de Concarneau\n",
      "     Location: (47.264360, -1.578521)\n",
      "     Glass enabled: ‚úì\n",
      "WASTE TYPE ENABLEMENT (should be Glass only):\n",
      "  Glass: 1,079/1,079 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# 3RD QUERY FOR GLASS COLLECTION COLUMNS (TAKE VERRE ONLY)\n",
    "# Using localisation des colonnes d‚Äôapports volontaires de Nantes M√©tropole\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\n3. Querying glass collection columns (Verre only) using Nantes M√©tropole data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query3 = f\"\"\"\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER () + 30000 as ID,  # Start from 30000 for glass columns\n",
    "  CONCAT(\n",
    "    'Drop-off points - ',\n",
    "    COALESCE(\n",
    "      CASE\n",
    "        WHEN type_colonne IS NOT NULL THEN\n",
    "          CASE type_colonne\n",
    "            WHEN 'colonne enterr√©e' THEN 'Underground'\n",
    "            WHEN 'colonne a√©rienne' THEN 'Above-ground'\n",
    "            ELSE INITCAP(type_colonne)\n",
    "          END\n",
    "        ELSE ''\n",
    "      END,\n",
    "      'Glass Collection'\n",
    "    ),\n",
    "    CASE\n",
    "      WHEN commune IS NOT NULL THEN CONCAT(' - ', commune)\n",
    "      ELSE ' - Nantes'\n",
    "    END\n",
    "  ) as Name,\n",
    "  COALESCE(adresse, 'Nantes M√©tropole') as Address,\n",
    "  lat as Latitude,\n",
    "  lon as Longitude,\n",
    "\n",
    "  -- Waste type capabilities: ONLY GLASS ENABLED\n",
    "  0 as Is_Cardboard_enabled,\n",
    "  0 as Is_Food_enabled,\n",
    "  1 as Is_Glass_enabled, -- Only glass collection points\n",
    "  0 as Is_Metal_enabled,\n",
    "  0 as Is_Paper_enabled,\n",
    "  0 as Is_Wood_enabled,\n",
    "  0 as Is_Plastic_enabled,\n",
    "  0 as Is_Textile_enabled,\n",
    "  0 as Is_Vegetation_enabled,\n",
    "  0 as Is_Neon_enabled,\n",
    "  0 as Is_Cartridge_enabled,\n",
    "  0 as Is_Lamp_Light_enabled,\n",
    "  0 as Is_Pile_enabled,\n",
    "  0 as Is_Battery_enabled,\n",
    "  0 as Is_Car_Battery_enabled,\n",
    "  0 as Is_Miscellanous_Trash_enabled,\n",
    "  0 as Is_Pharmacy_enabled,\n",
    "  0 as Is_Tire_enabled,\n",
    "  0 as Is_Ressourcerie_enabled\n",
    "\n",
    "FROM `{PROJECT}.{DATASET}.location_dropoff_points_nantes`\n",
    "WHERE\n",
    "  lat IS NOT NULL\n",
    "  AND lon IS NOT NULL\n",
    "  AND LOWER(TRIM(type_dechet)) = 'verre'  # Only glass collection points\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df3 = client.query(query3).to_dataframe()\n",
    "    print(f\"Retrieved {len(df3):,} glass collection columns\")\n",
    "\n",
    "    # Show summary\n",
    "    print(\"=\"*60)\n",
    "    print(f\"GLASS COLUMNS SUMMARY:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Total glass columns: {len(df3):,}\")\n",
    "\n",
    "    # Check coordinate validity\n",
    "    valid_coords = df3['Latitude'].notna().sum()\n",
    "    print(f\"  With valid coordinates: {valid_coords:,}\")\n",
    "\n",
    "    # Show sample\n",
    "    print(f\"SAMPLE GLASS COLUMNS (first 3):\")\n",
    "    for i in range(min(3, len(df3))):\n",
    "        row = df3.iloc[i]\n",
    "        print(f\"  {i+1}. {row['Name']}\")\n",
    "        print(f\"     Address: {row['Address'][:60]}\")\n",
    "        print(f\"     Location: ({row['Latitude']:.6f}, {row['Longitude']:.6f})\")\n",
    "        print(f\"     Glass enabled: {'‚úì' if row['Is_Glass_enabled'] == 1 else '‚úó'}\")\n",
    "\n",
    "    # Show waste type summary\n",
    "    print(f\"WASTE TYPE ENABLEMENT (should be Glass only):\")\n",
    "    waste_cols = [col for col in df3.columns if col.startswith('Is_')]\n",
    "    for col in waste_cols:\n",
    "        count = df3[col].sum()\n",
    "        if count > 0:\n",
    "            waste_name = col.replace('Is_', '').replace('_enabled', '').replace('_', ' ').title()\n",
    "            print(f\"  {waste_name}: {count:,}/{len(df3):,} ({count/len(df3)*100:.1f}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error querying glass columns: {e}\")\n",
    "\n",
    "    # Debug: Check what types of waste exist in the table\n",
    "\n",
    "    print(\"Debug: Checking available waste types in the table...\")\n",
    "    try:\n",
    "        debug_query = f\"\"\"\n",
    "        SELECT\n",
    "          type_dechet,\n",
    "          COUNT(*) as count\n",
    "        FROM `{PROJECT}.{DATASET}.location_dropoff_points_nantes`\n",
    "        WHERE type_dechet IS NOT NULL\n",
    "        GROUP BY type_dechet\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        waste_types = client.query(debug_query).to_dataframe()\n",
    "        print(f\"Available waste types in table:\")\n",
    "        print(waste_types.to_string(index=False))\n",
    "    except:\n",
    "        print(\"Could not check waste types\")\n",
    "\n",
    "    df3 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1aa7d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "4. Querying non-glass waste columns with waste type names\n",
      "============================================================\n",
      "Retrieved 1,490 non-glass waste columns\n",
      "BREAKDOWN BY WASTE TYPE:\n",
      "  Household Waste: 843 columns (56.6%)\n",
      "  Recyclable Waste: 564 columns (37.9%)\n",
      "  Paper/Cardboard: 83 columns (5.6%)\n",
      "CAPABILITIES BY WASTE TYPE:\n",
      "\n",
      "  Paper/Cardboard columns accept:\n",
      "    ‚úì Cardboard, Paper\n",
      "\n",
      "  Recyclable Waste columns accept:\n",
      "    ‚úì Cardboard, Metal, Paper, Plastic\n",
      "\n",
      "  Household Waste columns accept:\n",
      "    ‚úì Miscellanous Trash\n",
      "SAMPLES (one of each type):\n",
      "\n",
      "  Paper/Cardboard:\n",
      "    Name: Paper/Cardboard Drop-off Point - Nantes (Above-ground)\n",
      "    Original Type: Papier-carton\n",
      "    Location: Nantes\n",
      "    Coordinates: (47.229835, -1.519756)\n",
      "\n",
      "  Recyclable Waste:\n",
      "    Name: Recyclable Waste Drop-off Point - Basse-Goulaine (Underground)\n",
      "    Original Type: D√©chet recyclable\n",
      "    Location: Basse-Goulaine\n",
      "    Coordinates: (47.208462, -1.466821)\n",
      "\n",
      "  Household Waste:\n",
      "    Name: Household Waste Drop-off Point - Basse-Goulaine (Underground)\n",
      "    Original Type: Ordure m√©nag√®re\n",
      "    Location: Basse-Goulaine\n",
      "    Coordinates: (47.214656, -1.467335)\n",
      "DISTRIBUTION BY COMMUNE (top 5):\n",
      "  Nantes: 872 columns\n",
      "  Saint-Herblain: 226 columns\n",
      "  Rez√©: 122 columns\n",
      "  Carquefou: 55 columns\n",
      "  Vertou: 48 columns\n"
     ]
    }
   ],
   "source": [
    "# 4TH QUERY FOR NON-GLASS WASTE TYPES USING ECOPOINTS\n",
    "# Add detailed waste type names and capabilities based on type_dechet\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\n4. Querying non-glass waste columns with waste type names\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query4 = f\"\"\"\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER () +\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%papier%carton%' THEN 40000\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%d√©chet recyclable%' THEN 50000\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%ordure m√©nag√®re%' THEN 60000\n",
    "    ELSE 70000\n",
    "  END as ID,\n",
    "\n",
    "  CONCAT(\n",
    "    CASE\n",
    "      WHEN LOWER(TRIM(type_dechet)) LIKE '%papier%carton%' THEN 'Paper/Cardboard'\n",
    "      WHEN LOWER(TRIM(type_dechet)) LIKE '%d√©chet recyclable%' THEN 'Recyclable Waste'\n",
    "      WHEN LOWER(TRIM(type_dechet)) LIKE '%ordure m√©nag√®re%' THEN 'Household Waste'\n",
    "      ELSE INITCAP(type_dechet)\n",
    "    END,\n",
    "    ' Drop-off Point - ',\n",
    "    COALESCE(commune, 'Nantes'),\n",
    "    CASE\n",
    "      WHEN type_colonne IS NOT NULL THEN CONCAT(' (',\n",
    "        CASE type_colonne\n",
    "          WHEN 'colonne enterr√©e' THEN 'Underground'\n",
    "          WHEN 'colonne a√©rienne' THEN 'Above-ground'\n",
    "          ELSE INITCAP(type_colonne)\n",
    "        END, ')')\n",
    "      ELSE ''\n",
    "    END\n",
    "  ) as Name,\n",
    "\n",
    "  COALESCE(adresse, 'Nantes M√©tropole') as Address,\n",
    "  lat as Latitude,\n",
    "  lon as Longitude,\n",
    "\n",
    "  -- Paper/Cardboard columns\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%papier%carton%' THEN 1\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%d√©chet recyclable%' THEN 1\n",
    "    ELSE 0\n",
    "  END as Is_Cardboard_enabled,\n",
    "\n",
    "  -- Food (only for household waste)\n",
    "  0 as Is_Food_enabled,\n",
    "\n",
    "  -- Glass (for recyclable and household waste - but NOT paper/cardboard)\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%verre%' THEN 1\n",
    "    ELSE 0\n",
    "  END as Is_Glass_enabled,\n",
    "\n",
    "  -- Metal (for recyclable and household waste)\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%d√©chet recyclable%' THEN 1\n",
    "    ELSE 0\n",
    "  END as Is_Metal_enabled,\n",
    "\n",
    "  -- Paper (for paper/cardboard, recyclable, and household)\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%papier%carton%' THEN 1\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%d√©chet recyclable%' THEN 1\n",
    "    ELSE 0\n",
    "  END as Is_Paper_enabled,\n",
    "\n",
    "  -- Plastic (for recyclable and household)\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%d√©chet recyclable%' THEN 1\n",
    "    ELSE 0\n",
    "  END as Is_Plastic_enabled,\n",
    "\n",
    "  -- Others (for household waste)\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%ordure m√©nag√®re%' THEN 1\n",
    "    ELSE 0\n",
    "  END as Is_Miscellanous_Trash_enabled,\n",
    "\n",
    "  -- Textile (only for household)\n",
    "  0 as Is_Textile_enabled,\n",
    "\n",
    "  -- Vegetation (only for household)\n",
    "  0 as Is_Vegetation_enabled,\n",
    "\n",
    "  -- Special waste types (none for these columns)\n",
    "  0 as Is_Neon_enabled,\n",
    "  0 as Is_Cartridge_enabled,\n",
    "  0 as Is_Lamp_Light_enabled,\n",
    "\n",
    "  type_dechet as Original_Waste_Type,\n",
    "  type_colonne as Original_Column_Type,\n",
    "  commune as Commune\n",
    "\n",
    "FROM `{PROJECT}.{DATASET}.location_dropoff_points_nantes`\n",
    "WHERE\n",
    "  lat IS NOT NULL\n",
    "  AND lon IS NOT NULL\n",
    "  AND (\n",
    "    LOWER(TRIM(type_dechet)) LIKE '%papier%carton%'\n",
    "    OR LOWER(TRIM(type_dechet)) LIKE '%d√©chet recyclable%'\n",
    "    OR LOWER(TRIM(type_dechet)) LIKE '%ordure m√©nag√®re%'\n",
    "  )\n",
    "ORDER BY\n",
    "  CASE\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%papier%carton%' THEN 1\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%d√©chet recyclable%' THEN 2\n",
    "    WHEN LOWER(TRIM(type_dechet)) LIKE '%ordure m√©nag√®re%' THEN 3\n",
    "    ELSE 4\n",
    "  END,\n",
    "  commune\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df4 = client.query(query4).to_dataframe()\n",
    "    print(f\"Retrieved {len(df4):,} non-glass waste columns\")\n",
    "\n",
    "    # Show breakdown\n",
    "\n",
    "    print(f\"BREAKDOWN BY WASTE TYPE:\")\n",
    "    if 'Original_Waste_Type' in df4.columns:\n",
    "        # Group by cleaned waste type name\n",
    "        df4['Waste_Category'] = df4['Original_Waste_Type'].apply(\n",
    "            lambda x: 'Paper/Cardboard' if 'papier' in str(x).lower() and 'carton' in str(x).lower()\n",
    "            else 'Recyclable Waste' if 'd√©chet recyclable' in str(x).lower()\n",
    "            else 'Household Waste' if 'ordure m√©nag√®re' in str(x).lower()\n",
    "            else 'Other'\n",
    "        )\n",
    "\n",
    "        waste_counts = df4['Waste_Category'].value_counts()\n",
    "        for waste_type, count in waste_counts.items():\n",
    "            percentage = (count / len(df4)) * 100\n",
    "            print(f\"  {waste_type}: {count:,} columns ({percentage:.1f}%)\")\n",
    "\n",
    "    # Show what each type accepts\n",
    "\n",
    "    print(f\"CAPABILITIES BY WASTE TYPE:\")\n",
    "    waste_categories = df4['Waste_Category'].unique() if 'Waste_Category' in df4.columns else df4['Original_Waste_Type'].unique()\n",
    "\n",
    "    for category in waste_categories:\n",
    "        if 'Waste_Category' in df4.columns:\n",
    "            subset = df4[df4['Waste_Category'] == category]\n",
    "        else:\n",
    "            subset = df4[df4['Original_Waste_Type'] == category]\n",
    "\n",
    "        if len(subset) > 0:\n",
    "            print(f\"\\n  {category} columns accept:\")\n",
    "            enabled_types = []\n",
    "            for col in [c for c in subset.columns if c.startswith('Is_') and c not in ['Is_Neon_enabled', 'Is_Cartridge_enabled', 'Is_Lamp_Light_enabled']]:\n",
    "                if subset[col].iloc[0] == 1:\n",
    "                    waste_name = col.replace('Is_', '').replace('_enabled', '').replace('_', ' ').title()\n",
    "                    enabled_types.append(waste_name)\n",
    "\n",
    "            if enabled_types:\n",
    "                print(f\"    ‚úì {', '.join(enabled_types)}\")\n",
    "            else:\n",
    "                print(f\"    ‚úó No specific waste types enabled\")\n",
    "\n",
    "    # Show samples\n",
    "\n",
    "    print(f\"SAMPLES (one of each type):\")\n",
    "    sample_shown = set()\n",
    "\n",
    "    for _, row in df4.iterrows():\n",
    "        waste_type = row['Original_Waste_Type']\n",
    "        if waste_type not in sample_shown:\n",
    "            sample_shown.add(waste_type)\n",
    "\n",
    "            category = row.get('Waste_Category', waste_type)\n",
    "            print(f\"\\n  {category}:\")\n",
    "            print(f\"    Name: {row['Name']}\")\n",
    "            print(f\"    Original Type: {row['Original_Waste_Type']}\")\n",
    "            print(f\"    Location: {row['Commune']}\")\n",
    "            print(f\"    Coordinates: ({row['Latitude']:.6f}, {row['Longitude']:.6f})\")\n",
    "\n",
    "            # Show enabled types\n",
    "\n",
    "            enabled = []\n",
    "            for col in [c for c in row.index if c.startswith('Is_') and row[col] == 1]:\n",
    "                waste_name = col.replace('Is_', '').replace('_enabled', '').replace('_', ' ').title()\n",
    "                enabled.append(waste_name)\n",
    "            if enabled:\n",
    "                print(f\"    Accepts: {', '.join(enabled)}\")\n",
    "\n",
    "            # Limit to 3 samples\n",
    "\n",
    "            if len(sample_shown) >= 3:\n",
    "                break\n",
    "\n",
    "    # Show distribution by commune\n",
    "\n",
    "    print(f\"DISTRIBUTION BY COMMUNE (top 5):\")\n",
    "    if 'Commune' in df4.columns:\n",
    "        commune_counts = df4['Commune'].value_counts().head(5)\n",
    "        for commune, count in commune_counts.items():\n",
    "            print(f\"  {commune}: {count:,} columns\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "    # Try exact match if LIKE doesn't work\n",
    "\n",
    "    print(\"Trying with exact matches\")\n",
    "    try:\n",
    "        query4_exact = f\"\"\"\n",
    "        SELECT\n",
    "          ROW_NUMBER() OVER () +\n",
    "          CASE\n",
    "            WHEN type_dechet = 'Papier-carton' THEN 40000\n",
    "            WHEN type_dechet = 'D√©chet recyclable' THEN 50000\n",
    "            WHEN type_dechet = 'Ordure m√©nag√®re' THEN 60000\n",
    "            ELSE 70000\n",
    "          END as ID,\n",
    "\n",
    "          CONCAT(\n",
    "            CASE\n",
    "              WHEN type_dechet = 'Papier-carton' THEN 'Paper/Cardboard'\n",
    "              WHEN type_dechet = 'D√©chet recyclable' THEN 'Recyclable Waste'\n",
    "              WHEN type_dechet = 'Ordure m√©nag√®re' THEN 'Household Waste'\n",
    "              ELSE type_dechet\n",
    "            END,\n",
    "            ' Column - ',\n",
    "            COALESCE(commune, 'Nantes')\n",
    "          ) as Name,\n",
    "\n",
    "          COALESCE(adresse, 'Nantes M√©tropole') as Address,\n",
    "          lat as Latitude,\n",
    "          lon as Longitude,\n",
    "\n",
    "          -- Capabilities (simplified for testing)\n",
    "          1 as Is_Cardboard_enabled,\n",
    "          0 as Is_Food_enabled,\n",
    "          0 as Is_Glass_enabled,\n",
    "          0 as Is_Metal_enabled,\n",
    "          1 as Is_Paper_enabled,\n",
    "          0 as Is_Plastic_enabled,\n",
    "          0 as Is_Textile_enabled,\n",
    "          0 as Is_Vegetation_enabled,\n",
    "          0 as Is_Neon_enabled,\n",
    "          0 as Is_Cartridge_enabled,\n",
    "          0 as Is_Lamp_Light_enabled,\n",
    "\n",
    "          type_dechet as Original_Waste_Type,\n",
    "          commune as Commune\n",
    "\n",
    "        FROM `{PROJECT}.{DATASET}.location_dropoff_points_nantes`\n",
    "        WHERE lat IS NOT NULL AND lon IS NOT NULL\n",
    "          AND type_dechet IN ('Papier-carton', 'D√©chet recyclable', 'Ordure m√©nag√®re')\n",
    "        LIMIT 100\n",
    "        \"\"\"\n",
    "\n",
    "        df4 = client.query(query4_exact).to_dataframe()\n",
    "        print(f\"Retrieved {len(df4):,} columns with exact matching\")\n",
    "\n",
    "    except Exception as e2:\n",
    "        print(f\"Exact match also failed: {e2}\")\n",
    "        df4 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "756f1131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "5. Querying ecosystem collection points\n",
      "============================================================\n",
      "Retrieved 110 Ecosystem collection points\n"
     ]
    }
   ],
   "source": [
    "# 5TH QUERY FOR ECOSYSTEM POINTS\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\n5. Querying ecosystem collection points\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query5 = f\"\"\"\n",
    "SELECT\n",
    "  ID,\n",
    "  Name,\n",
    "  Address,\n",
    "  Longitude,\n",
    "  Latitude,\n",
    "  Is_Neon_enabled,\n",
    "  Is_Cartridge_enabled,\n",
    "  Is_Lamp_Light_enabled,\n",
    "  Is_Battery_enabled,\n",
    "  Is_Car_Battery_enabled,\n",
    "  Is_Pile_enabled\n",
    "FROM `{PROJECT}.{DATASET}.ecosystem_collection_points_with_coords`\n",
    "WHERE Latitude IS NOT NULL AND Longitude IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df5 = client.query(query5).to_dataframe()\n",
    "    print(f\"Retrieved {len(df5):,} Ecosystem collection points\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving ecosystem points: {e}\")\n",
    "    df5 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6804e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE AND CREATE CURRENT TABLE WITH ECOSYSTEM POINTS\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING CURRENT TRASH COLLECTION POINTS TABLE WITH ECOSYSTEM POINTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize variables\n",
    "all_dataframes = []\n",
    "available_dfs = []\n",
    "\n",
    "# Check which dataframes exist and are not empty\n",
    "dataframe_names = ['df1', 'df2', 'df3', 'df4', 'df5']\n",
    "\n",
    "for df_name in dataframe_names:\n",
    "    if df_name in locals() and isinstance(locals()[df_name], pd.DataFrame):\n",
    "        df = locals()[df_name]\n",
    "        if not df.empty:\n",
    "            all_dataframes.append(df)\n",
    "            available_dfs.append(df_name)\n",
    "            print(f\"‚úì {df_name}: {len(df):,} points\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {df_name}: Empty dataframe\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {df_name}: Not found or not a DataFrame\")\n",
    "\n",
    "if all_dataframes:\n",
    "    # Show what we're working with\n",
    "    print(f\"\\nüìä DATASET COMPOSITION:\")\n",
    "    print(f\"  Number of dataframes to combine: {len(all_dataframes)}\")\n",
    "\n",
    "    for i, (df_name, df) in enumerate(zip(available_dfs, all_dataframes), 1):\n",
    "        print(f\"  {df_name}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        if i == 1:  # Show first few column names for first dataframe\n",
    "            print(f\"    Columns: {df.columns.tolist()[:10]}...\")\n",
    "\n",
    "    # Combine all data\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "    print(f\"\\n‚úÖ Combined dataset: {len(combined_df)} rows, {len(combined_df.columns)} columns\")\n",
    "\n",
    "    # Reset ID to be sequential\n",
    "    combined_df['ID'] = range(1, len(combined_df) + 1)\n",
    "\n",
    "    # Define final structure\n",
    "    final_columns = [\n",
    "        'ID', 'Name', 'Address', 'Longitude', 'Latitude',\n",
    "        'Is_Cardboard_enabled', 'Is_Food_enabled', 'Is_Glass_enabled',\n",
    "        'Is_Metal_enabled', 'Is_Paper_enabled', 'Is_Plastic_enabled',\n",
    "        'Is_Textile_enabled', 'Is_Vegetation_enabled', 'Is_Neon_enabled',\n",
    "        'Is_Cartridge_enabled', 'Is_Lamp_Light_enabled',\n",
    "        'Is_Miscellanous_Trash_enabled', 'Is_Pharmacy_enabled',\n",
    "        'Is_Tire_enabled', 'Is_Ressourcerie_enabled'\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüîÑ STANDARDIZING COLUMNS...\")\n",
    "    print(f\"  Target columns: {len(final_columns)}\")\n",
    "\n",
    "    # Ensure all columns exist (add missing ones)\n",
    "    columns_added = []\n",
    "    for col in final_columns:\n",
    "        if col not in combined_df.columns:\n",
    "            if col.startswith('Is_'):\n",
    "                combined_df[col] = 0\n",
    "                columns_added.append(col)\n",
    "            elif col in ['Longitude', 'Latitude']:\n",
    "                combined_df[col] = None\n",
    "                columns_added.append(col)\n",
    "            elif col == 'ID':\n",
    "                # Already added above\n",
    "                pass\n",
    "            else:\n",
    "                combined_df[col] = ''\n",
    "                columns_added.append(col)\n",
    "\n",
    "    if columns_added:\n",
    "        print(f\"  Added columns: {columns_added}\")\n",
    "\n",
    "    # Standardize coordinate column names\n",
    "    print(f\"\\nüìç STANDARDIZING COORDINATES...\")\n",
    "\n",
    "    # Find all possible coordinate columns\n",
    "    coord_mapping = {}\n",
    "    for col in combined_df.columns:\n",
    "        col_lower = str(col).lower()\n",
    "        if 'lat' in col_lower and col != 'Latitude':\n",
    "            coord_mapping['Latitude'] = col\n",
    "        elif ('lon' in col_lower or 'long' in col_lower) and col != 'Longitude':\n",
    "            coord_mapping['Longitude'] = col\n",
    "\n",
    "    if coord_mapping:\n",
    "        print(f\"  Found alternate coordinate columns: {coord_mapping}\")\n",
    "\n",
    "        # Copy values from alternate columns to standard ones\n",
    "        if 'Latitude' in coord_mapping:\n",
    "            print(f\"  Copying {coord_mapping['Latitude']} ‚Üí Latitude\")\n",
    "            combined_df['Latitude'] = combined_df[coord_mapping['Latitude']]\n",
    "\n",
    "        if 'Longitude' in coord_mapping:\n",
    "            print(f\"  Copying {coord_mapping['Longitude']} ‚Üí Longitude\")\n",
    "            combined_df['Longitude'] = combined_df[coord_mapping['Longitude']]\n",
    "\n",
    "        # Drop the alternate columns\n",
    "        drop_cols = list(coord_mapping.values())\n",
    "        combined_df = combined_df.drop(columns=drop_cols, errors='ignore')\n",
    "        print(f\"  Dropped alternate columns: {drop_cols}\")\n",
    "\n",
    "    # Convert to proper types\n",
    "    print(f\"\\nüîÑ CONVERTING DATA TYPES...\")\n",
    "\n",
    "    # Convert waste type columns to integers\n",
    "    waste_cols = [col for col in combined_df.columns if col.startswith('Is_') and col.endswith('_enabled')]\n",
    "    for col in waste_cols:\n",
    "        if col in combined_df.columns:\n",
    "            initial_nulls = combined_df[col].isna().sum()\n",
    "            combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce').fillna(0).astype(int)\n",
    "            final_nulls = combined_df[col].isna().sum()\n",
    "            print(f\"  ‚úì {col}: {initial_nulls} nulls ‚Üí {final_nulls} nulls\")\n",
    "\n",
    "    # Convert coordinates\n",
    "    coord_conversion = {}\n",
    "    for col in ['Longitude', 'Latitude']:\n",
    "        if col in combined_df.columns:\n",
    "            combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "            valid_count = combined_df[col].notna().sum()\n",
    "            total_count = len(combined_df)\n",
    "            percentage = (valid_count / total_count * 100) if total_count > 0 else 0\n",
    "            coord_conversion[col] = (valid_count, percentage)\n",
    "\n",
    "    print(f\"  Coordinate validity:\")\n",
    "    for col, (valid, pct) in coord_conversion.items():\n",
    "        print(f\"    {col}: {valid:,}/{len(combined_df):,} valid ({pct:.1f}%)\")\n",
    "\n",
    "    # Reorder columns - only keep final columns\n",
    "    available_columns = [col for col in final_columns if col in combined_df.columns]\n",
    "    missing_columns = [col for col in final_columns if col not in combined_df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        print(f\"\\n‚ö†Ô∏è WARNING: Missing expected columns: {missing_columns}\")\n",
    "\n",
    "    combined_df = combined_df[available_columns]\n",
    "    print(f\"  Final columns: {len(available_columns)}\")\n",
    "\n",
    "    total_locations = len(combined_df)\n",
    "    print(f\"\\n‚úÖ CURRENT TABLE CREATED: {total_locations:,} total trash collection points\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    print(\"\\nüìà CURRENT DATASET STATISTICS:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Points with coordinates\n",
    "    has_coords = combined_df['Latitude'].notna() & combined_df['Longitude'].notna()\n",
    "    coord_count = has_coords.sum()\n",
    "    coord_pct = (coord_count / total_locations * 100) if total_locations > 0 else 0\n",
    "    print(f\"  ‚Ä¢ Points with coordinates: {coord_count:,}/{total_locations:,} ({coord_pct:.1f}%)\")\n",
    "\n",
    "    # Points without coordinates (if any)\n",
    "    no_coords = total_locations - coord_count\n",
    "    if no_coords > 0:\n",
    "        print(f\"  ‚Ä¢ Points WITHOUT coordinates: {no_coords:,} ({no_coords/total_locations*100:.1f}%)\")\n",
    "\n",
    "    # Waste type coverage\n",
    "    print(f\"\\n  ‚Ä¢ Waste type coverage:\")\n",
    "\n",
    "    # Define waste types based on actual columns\n",
    "    waste_types = []\n",
    "    for col in combined_df.columns:\n",
    "        if col.startswith('Is_') and col.endswith('_enabled'):\n",
    "            waste_name = col.replace('Is_', '').replace('_enabled', '').replace('_', ' ').title()\n",
    "            waste_types.append((waste_name, col))\n",
    "\n",
    "    # Sort by count descending\n",
    "    waste_counts = []\n",
    "    for waste_name, col_name in waste_types:\n",
    "        count = int(combined_df[col_name].sum())\n",
    "        if count > 0:\n",
    "            percentage = (count / total_locations) * 100 if total_locations > 0 else 0\n",
    "            waste_counts.append((waste_name, col_name, count, percentage))\n",
    "\n",
    "    # Sort by count descending\n",
    "    waste_counts.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    for waste_name, col_name, count, percentage in waste_counts:\n",
    "        print(f\"    {waste_name:22s}: {count:6,d} points ({percentage:5.1f}%)\")\n",
    "\n",
    "    # Save to CSV for backup\n",
    "    output_csv = \"current_trash_collection_points.csv\"\n",
    "    try:\n",
    "        combined_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nüíæ Data saved locally: '{output_csv}' ({len(combined_df):,} rows)\")\n",
    "\n",
    "        # Verify CSV was saved correctly\n",
    "        csv_check = pd.read_csv(output_csv, nrows=5)\n",
    "        print(f\"  CSV verification: {len(csv_check)} rows read successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error saving CSV: {e}\")\n",
    "\n",
    "    # UPLOAD TO BIGQUERY\n",
    "    print(f\"\\n‚òÅÔ∏è UPLOADING TO BIGQUERY...\")\n",
    "\n",
    "    # Use consistent table name\n",
    "    table_id = f\"{PROJECT}.{DATASET}.trash_collection_points\"\n",
    "    print(f\"  Table: {table_id}\")\n",
    "\n",
    "    # Check if BigQuery client is available\n",
    "    if 'client' not in locals():\n",
    "        print(\"‚ùå BigQuery client not found. Creating local backup only.\")\n",
    "        print(f\"   Data saved to: {output_csv}\")\n",
    "    else:\n",
    "        try:\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                write_disposition=\"WRITE_TRUNCATE\",\n",
    "                autodetect=True,\n",
    "                max_bad_records=100\n",
    "            )\n",
    "\n",
    "            # Upload the dataframe\n",
    "            print(f\"  Uploading {len(combined_df):,} rows...\")\n",
    "            job = client.load_table_from_dataframe(combined_df, table_id, job_config=job_config)\n",
    "            job.result()\n",
    "\n",
    "            # Verify upload\n",
    "            table = client.get_table(table_id)\n",
    "            print(f\"‚úÖ BigQuery table created: {table_id}\")\n",
    "            print(f\"   Rows: {table.num_rows:,}\")\n",
    "            print(f\"   Size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "            # Run verification query\n",
    "            print(f\"\\nüîç VERIFICATION QUERY:\")\n",
    "\n",
    "            # Build dynamic verification query based on available columns\n",
    "            select_parts = [\"COUNT(*) as total_points\"]\n",
    "\n",
    "            # Add coordinate check\n",
    "            select_parts.append(\"SUM(CASE WHEN Latitude IS NOT NULL AND Longitude IS NOT NULL THEN 1 ELSE 0 END) as points_with_coords\")\n",
    "\n",
    "            # Add checks for each waste type column\n",
    "            waste_cols = [col for col in combined_df.columns if col.startswith('Is_') and col.endswith('_enabled')]\n",
    "            for col in waste_cols:\n",
    "                clean_name = col.replace('Is_', '').replace('_enabled', '')\n",
    "                select_parts.append(f\"SUM({col}) as {clean_name}_points\")\n",
    "\n",
    "            verify_query = f\"\"\"\n",
    "            SELECT\n",
    "              {', '.join(select_parts)}\n",
    "            FROM `{table_id}`\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                result = client.query(verify_query).to_dataframe().iloc[0]\n",
    "                print(f\"  ‚Ä¢ Total points: {result['total_points']:,}\")\n",
    "                print(f\"  ‚Ä¢ With coordinates: {result['points_with_coords']:,}\")\n",
    "\n",
    "                # Show top waste types by count\n",
    "                print(f\"\\n  ‚Ä¢ Top waste type counts:\")\n",
    "                waste_counts = []\n",
    "                for key, value in result.items():\n",
    "                    if key.endswith('_points') and key != 'points_with_coords':\n",
    "                        waste_name = key.replace('_points', '').replace('_', ' ').title()\n",
    "                        waste_counts.append((waste_name, int(value)))\n",
    "\n",
    "                # Sort by count descending and show top 10\n",
    "                waste_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "                for waste_name, count in waste_counts[:10]:\n",
    "                    if count > 0:\n",
    "                        pct = (count / result['total_points'] * 100) if result['total_points'] > 0 else 0\n",
    "                        print(f\"     - {waste_name:18s}: {count:6,d} ({pct:5.1f}%)\")\n",
    "\n",
    "                if len(waste_counts) > 10:\n",
    "                    print(f\"     ... and {len(waste_counts) - 10} more waste types\")\n",
    "\n",
    "            except Exception as query_error:\n",
    "                print(f\"  ‚ùå Verification query failed: {query_error}\")\n",
    "\n",
    "        except Exception as upload_error:\n",
    "            print(f\"‚ùå BigQuery upload failed: {upload_error}\")\n",
    "            print(f\"   Data saved locally: '{output_csv}'\")\n",
    "\n",
    "            # Try alternative method using pandas_gbq\n",
    "            try:\n",
    "                print(\"\\nüîÑ Trying alternative CSV upload method...\")\n",
    "                import pandas_gbq\n",
    "\n",
    "                pandas_gbq.to_gbq(\n",
    "                    combined_df,\n",
    "                    destination_table=table_id,\n",
    "                    project_id=PROJECT,\n",
    "                    if_exists='replace',\n",
    "                    progress_bar=False\n",
    "                )\n",
    "                print(\"‚úÖ Upload successful via pandas_gbq!\")\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Alternative also failed: {e2}\")\n",
    "                print(\"\\nüìã Manual upload instructions:\")\n",
    "                print(f\"   1. Go to BigQuery Console\")\n",
    "                print(f\"   2. Select dataset: {DATASET}\")\n",
    "                print(f\"   3. Create table 'trash_collection_points'\")\n",
    "                print(f\"   4. Upload file: {output_csv}\")\n",
    "                print(f\"   5. Enable schema autodetection\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No data to combine!\")\n",
    "    print(\"   Check if df1, df2, df3, df4, df5 are defined and not empty\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ UNIFIED TRASH COLLECTION DATABASE CREATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'total_locations' in locals():\n",
    "    print(f\"\\nSUMMARY:\")\n",
    "    print(f\"  ‚Ä¢ Total collection points: {total_locations:,}\")\n",
    "    print(f\"  ‚Ä¢ Datasets combined: {len(available_dfs)}\")\n",
    "\n",
    "    if 'table_id' in locals():\n",
    "        print(f\"  ‚Ä¢ BigQuery table: {table_id}\")\n",
    "\n",
    "    if 'output_csv' in locals():\n",
    "        print(f\"  ‚Ä¢ Local backup: {output_csv}\")\n",
    "\n",
    "    # Final data quality check\n",
    "    print(f\"\\nüìä FINAL DATA QUALITY CHECK:\")\n",
    "    if 'combined_df' in locals():\n",
    "        print(f\"  ‚Ä¢ Rows: {len(combined_df):,}\")\n",
    "        print(f\"  ‚Ä¢ Columns: {len(combined_df.columns):,}\")\n",
    "\n",
    "        # Check for nulls in critical columns\n",
    "        critical_cols = ['Name', 'Latitude', 'Longitude']\n",
    "        for col in critical_cols:\n",
    "            if col in combined_df.columns:\n",
    "                null_count = combined_df[col].isna().sum()\n",
    "                if null_count == 0:\n",
    "                    print(f\"  ‚Ä¢ ‚úÖ {col}: No null values\")\n",
    "                else:\n",
    "                    pct = (null_count / len(combined_df)) * 100\n",
    "                    print(f\"  ‚Ä¢ ‚ö†Ô∏è {col}: {null_count:,} nulls ({pct:.1f}%)\")\n",
    "\n",
    "        # Check waste type columns\n",
    "        waste_cols = [col for col in combined_df.columns if col.startswith('Is_')]\n",
    "        total_waste_points = 0\n",
    "        for col in waste_cols:\n",
    "            total_waste_points += combined_df[col].sum()\n",
    "\n",
    "        avg_waste_types = total_waste_points / len(combined_df) if len(combined_df) > 0 else 0\n",
    "        print(f\"  ‚Ä¢ Average waste types per point: {avg_waste_types:.2f}\")\n",
    "\n",
    "        # Check ID uniqueness\n",
    "        if 'ID' in combined_df.columns:\n",
    "            unique_ids = combined_df['ID'].nunique()\n",
    "            if unique_ids == len(combined_df):\n",
    "                print(f\"  ‚Ä¢ ‚úÖ All IDs are unique\")\n",
    "            else:\n",
    "                print(f\"  ‚Ä¢ ‚ùå ID duplicates: {len(combined_df) - unique_ids}\")\n",
    "\n",
    "print(\"\\n‚úÖ Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f7be55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING CURRENT TRASH COLLECTION POINTS TABLE WITH ECOSYSTEM POINTS\n",
      "============================================================\n",
      "‚úì Food waste points: 1,644\n",
      "‚úì Recycling centers: 15\n",
      "‚úì Underground containers: 1,079\n",
      "‚úì Underground containers (other waste types): 1,490\n",
      "‚úì Ecosystem special waste points: 110\n",
      "\n",
      "üìä DATASET COMPOSITION BEFORE PROCESSING:\n",
      "  Food Waste: 1,644 | Recycling Centers: 15 | Glass Containers: 1,079 | Other Waste Containers: 1,490 | Ecosystem Points: 110\n",
      "\n",
      "üìç CHECKING COORDINATE COLUMNS...\n",
      "Found coordinate columns: {'longitude': 'Longitude', 'latitude': 'Latitude'}\n",
      "\n",
      "üîÑ CONVERTING DATA TYPES...\n",
      "  ‚úì Converted Longitude: 4,338/4,338 valid values\n",
      "  ‚úì Converted Latitude: 4,338/4,338 valid values\n",
      "  ‚úì Converted Is_Cardboard_enabled to int\n",
      "  ‚úì Converted Is_Food_enabled to int\n",
      "  ‚úì Converted Is_Glass_enabled to int\n",
      "  ‚úì Converted Is_Metal_enabled to int\n",
      "  ‚úì Converted Is_Paper_enabled to int\n",
      "  ‚úì Converted Is_Wood_enabled to int\n",
      "  ‚úì Converted Is_Plastic_enabled to int\n",
      "  ‚úì Converted Is_Textile_enabled to int\n",
      "  ‚úì Converted Is_Vegetation_enabled to int\n",
      "  ‚úì Converted Is_Neon_enabled to int\n",
      "  ‚úì Converted Is_Cartridge_enabled to int\n",
      "  ‚úì Converted Is_Lamp_Light_enabled to int\n",
      "  ‚úì Converted Is_Pile_enabled to int\n",
      "  ‚úì Converted Is_Battery_enabled to int\n",
      "  ‚úì Converted Is_Car_Battery_enabled to int\n",
      "  ‚úì Converted Is_Miscellanous_Trash_enabled to int\n",
      "  ‚úì Converted Is_Pharmacy_enabled to int\n",
      "  ‚úì Converted Is_Tire_enabled to int\n",
      "  ‚úì Converted Is_Ressourcerie_enabled to int\n",
      "\n",
      "‚úÖ CURRENT TABLE CREATED: 4,338 total trash collection points\n",
      "\n",
      "üìà CURRENT DATASET STATISTICS:\n",
      "  ‚Ä¢ Points with coordinates: 4,338/4,338 (100.0%)\n",
      "  ‚Ä¢ Waste type coverage:\n",
      "    Cardboard         :    662 points ( 15.3%)\n",
      "    Food              :  1,644 points ( 37.9%)\n",
      "    Glass             :  1,093 points ( 25.2%)\n",
      "    Metal             :    578 points ( 13.3%)\n",
      "    Paper             :    662 points ( 15.3%)\n",
      "    Plastic           :    564 points ( 13.0%)\n",
      "    Textile           :      9 points (  0.2%)\n",
      "    Vegetation        :     14 points (  0.3%)\n",
      "    Neon              :     56 points (  1.3%)\n",
      "    Cartridge         :     38 points (  0.9%)\n",
      "    Lamp/Light        :     48 points (  1.1%)\n",
      "    Pile              :     13 points (  0.3%)\n",
      "    Battery           :     39 points (  0.9%)\n",
      "    Car Battery       :     53 points (  1.2%)\n",
      "    Miscellanous Trash:    843 points ( 19.4%)\n",
      "    Tire              :      2 points (  0.0%)\n",
      "\n",
      "üëÄ SAMPLE FROM EACH SOURCE:\n",
      "\n",
      "  Food Waste (df1):\n",
      "    ‚Ä¢ Food Waste - Nantes...\n",
      "    ‚Ä¢ Food Waste - Nantes...\n",
      "\n",
      "  Recycling Centers (df2):\n",
      "    ‚Ä¢ Recycling Center - Ecopoint Auvours...\n",
      "    ‚Ä¢ Recycling Center - D√©ch√®terie de Saint S...\n",
      "\n",
      "  Glass Containers (df3):\n",
      "    ‚Ä¢ Drop-off points - Underground - Nantes...\n",
      "    ‚Ä¢ Drop-off points - Underground - Nantes...\n",
      "\n",
      "  Other Waste Containers (df4):\n",
      "    ‚Ä¢ Paper/Cardboard Drop-off Point - Nantes ...\n",
      "    ‚Ä¢ Paper/Cardboard Drop-off Point - Nantes ...\n",
      "\n",
      "  Ecosystem Points (df5):\n",
      "    ‚Ä¢ Auchan... (Neon, Lamp/Light)\n",
      "    ‚Ä¢ Auchan... (Neon, Lamp/Light)\n",
      "\n",
      "üíæ Data saved locally: 'current_trash_collection_points.csv'\n",
      "\n",
      "‚òÅÔ∏è UPLOADING TO BIGQUERY...\n",
      "‚úÖ BigQuery table created: trash-optimizer-479913.nantes.trash_collection_points\n",
      "   Rows: 4,338\n",
      "   Size: 0.99 MB\n",
      "\n",
      "üîç VERIFICATION QUERY:\n",
      "  ‚Ä¢ Total points: 4,338\n",
      "  ‚Ä¢ With coordinates: 4,338\n",
      "\n",
      "  ‚Ä¢ Special waste points:\n",
      "     - Neon/Lamp: 56\n",
      "     - Cartridges: 38\n",
      "     - Batteries: 39\n",
      "     - Piles: 13\n",
      "     - Glass: 1,093\n",
      "     - Food: 1,644\n",
      "     - D√©chetteries: 843\n",
      "     - Pharmacy: 0\n",
      "     - Tire: 2\n",
      "     - Ressourcerie: 0\n",
      "\n",
      "============================================================\n",
      "üéØ UNIFIED TRASH COLLECTION DATABASE CREATED!\n",
      "============================================================\n",
      "Total collection points: 4,338\n",
      "Datasets combined: 5\n",
      "BigQuery table: trash-optimizer-479913.nantes.trash_collection_points\n",
      "Local backup: current_trash_collection_points.csv\n"
     ]
    }
   ],
   "source": [
    "# COMBINE AND CREATE CURRENT TABLE WITH ECOSYSTEM POINTS\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING CURRENT TRASH COLLECTION POINTS TABLE WITH ECOSYSTEM POINTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_dataframes = []\n",
    "\n",
    "if not df1.empty:\n",
    "    all_dataframes.append(df1)\n",
    "    print(f\"‚úì Food waste points: {len(df1):,}\")\n",
    "if not df2.empty:\n",
    "    all_dataframes.append(df2)\n",
    "    print(f\"‚úì Recycling centers: {len(df2):,}\")\n",
    "if not df3.empty:\n",
    "    all_dataframes.append(df3)\n",
    "    print(f\"‚úì Underground containers: {len(df3):,}\")\n",
    "if not df4.empty:\n",
    "    all_dataframes.append(df4)\n",
    "    print(f\"‚úì Underground containers (other waste types): {len(df4):,}\")\n",
    "if not df5.empty:\n",
    "    all_dataframes.append(df5)\n",
    "    print(f\"‚úì Ecosystem special waste points: {len(df5):,}\")\n",
    "\n",
    "if all_dataframes:\n",
    "    # Combine all data\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "    print(f\"\\nüìä DATASET COMPOSITION BEFORE PROCESSING:\")\n",
    "    dataset_sources = []\n",
    "    if not df1.empty:\n",
    "        dataset_sources.append(f\"Food Waste: {len(df1):,}\")\n",
    "    if not df2.empty:\n",
    "        dataset_sources.append(f\"Recycling Centers: {len(df2):,}\")\n",
    "    if not df3.empty:\n",
    "        dataset_sources.append(f\"Glass Containers: {len(df3):,}\")\n",
    "    if not df4.empty:\n",
    "        dataset_sources.append(f\"Other Waste Containers: {len(df4):,}\")\n",
    "    if not df5.empty:\n",
    "        dataset_sources.append(f\"Ecosystem Points: {len(df5):,}\")\n",
    "    print(\"  \" + \" | \".join(dataset_sources))\n",
    "\n",
    "    # Reset ID to be sequential\n",
    "    combined_df['ID'] = range(1, len(combined_df) + 1)\n",
    "\n",
    "    # Define final structure - CORRECTED TYPO: strash ‚Üí trash\n",
    "    final_columns = [\n",
    "        'ID', 'Name', 'Address', 'Longitude', 'Latitude',\n",
    "        'Is_Cardboard_enabled', 'Is_Food_enabled', 'Is_Glass_enabled',\n",
    "        'Is_Metal_enabled', 'Is_Paper_enabled', 'Is_Wood_enabled', 'Is_Plastic_enabled',\n",
    "        'Is_Textile_enabled', 'Is_Vegetation_enabled', 'Is_Neon_enabled',\n",
    "        'Is_Cartridge_enabled', 'Is_Lamp_Light_enabled', 'Is_Pile_enabled',\n",
    "        'Is_Battery_enabled', 'Is_Car_Battery_enabled',\n",
    "        'Is_Miscellanous_Trash_enabled', 'Is_Pharmacy_enabled',\n",
    "        'Is_Tire_enabled', 'Is_Ressourcerie_enabled'\n",
    "    ]\n",
    "\n",
    "    # Ensure all columns exist (add missing ones)\n",
    "    for col in final_columns:\n",
    "        if col not in combined_df.columns:\n",
    "            if col.startswith('Is_'):\n",
    "                combined_df[col] = 0\n",
    "            elif col in ['Longitude', 'Latitude']:\n",
    "                combined_df[col] = None\n",
    "\n",
    "    # Handle coordinate column names - ecosystem df5 uses 'Latitude'/'Longitude'\n",
    "    # while other datasets might use 'latitude'/'longitude' or 'lon'/'lat'\n",
    "    print(\"\\nüìç CHECKING COORDINATE COLUMNS...\")\n",
    "    coord_columns = {}\n",
    "    for col in combined_df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'lat' in col_lower:\n",
    "            coord_columns['latitude'] = col\n",
    "        elif 'lon' in col_lower or 'long' in col_lower:\n",
    "            coord_columns['longitude'] = col\n",
    "\n",
    "    print(f\"Found coordinate columns: {coord_columns}\")\n",
    "\n",
    "    # Standardize coordinate column names\n",
    "    if 'latitude' in coord_columns and coord_columns['latitude'] != 'Latitude':\n",
    "        print(f\"Copying {coord_columns['latitude']} to Latitude column\")\n",
    "        combined_df['Latitude'] = combined_df[coord_columns['latitude']]\n",
    "    if 'longitude' in coord_columns and coord_columns['longitude'] != 'Longitude':\n",
    "        print(f\"Copying {coord_columns['longitude']} to Longitude column\")\n",
    "        combined_df['Longitude'] = combined_df[coord_columns['longitude']]\n",
    "\n",
    "    # Convert to proper types\n",
    "    print(\"\\nüîÑ CONVERTING DATA TYPES...\")\n",
    "    for col in combined_df.columns:\n",
    "        if col.startswith('Is_'):\n",
    "            combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce').fillna(0).astype(int)\n",
    "            print(f\"  ‚úì Converted {col} to int\")\n",
    "        elif col in ['Longitude', 'Latitude']:\n",
    "            combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "            valid_coords = combined_df[col].notna().sum()\n",
    "            print(f\"  ‚úì Converted {col}: {valid_coords:,}/{len(combined_df):,} valid values\")\n",
    "\n",
    "    # Reorder columns - only keep columns that exist\n",
    "    available_columns = [col for col in final_columns if col in combined_df.columns]\n",
    "    combined_df = combined_df[available_columns]\n",
    "\n",
    "    total_locations = len(combined_df)\n",
    "    print(f\"\\n‚úÖ CURRENT TABLE CREATED: {total_locations:,} total trash collection points\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    print(\"\\nüìà CURRENT DATASET STATISTICS:\")\n",
    "\n",
    "    # Points with coordinates\n",
    "    has_coords = combined_df['Latitude'].notna() & combined_df['Longitude'].notna()\n",
    "    coord_count = has_coords.sum()\n",
    "    print(f\"  ‚Ä¢ Points with coordinates: {coord_count:,}/{total_locations:,} ({coord_count/total_locations*100:.1f}%)\")\n",
    "\n",
    "    # Waste type coverage\n",
    "    print(f\"  ‚Ä¢ Waste type coverage:\")\n",
    "    waste_types = [\n",
    "        ('Cardboard', 'Is_Cardboard_enabled'),\n",
    "        ('Food', 'Is_Food_enabled'),\n",
    "        ('Glass', 'Is_Glass_enabled'),\n",
    "        ('Metal', 'Is_Metal_enabled'),\n",
    "        ('Paper', 'Is_Paper_enabled'),\n",
    "        ('Plastic', 'Is_Plastic_enabled'),\n",
    "        ('Textile', 'Is_Textile_enabled'),\n",
    "        ('Vegetation', 'Is_Vegetation_enabled'),\n",
    "        ('Neon', 'Is_Neon_enabled'),\n",
    "        ('Cartridge', 'Is_Cartridge_enabled'),\n",
    "        ('Lamp/Light', 'Is_Lamp_Light_enabled'),\n",
    "        ('Pile', 'Is_Pile_enabled'),\n",
    "        ('Battery', 'Is_Battery_enabled'),\n",
    "        ('Car Battery', 'Is_Car_Battery_enabled'),\n",
    "        ('Miscellanous Trash', 'Is_Miscellanous_Trash_enabled'),\n",
    "        ('Pharmacy', 'Is_Pharmacy_enabled'),\n",
    "        ('Tire', 'Is_Tire_enabled'),\n",
    "        ('Ressourcerie', 'Is_Ressourcerie_enabled')\n",
    "    ]\n",
    "\n",
    "    for waste_name, col_name in waste_types:\n",
    "        if col_name in combined_df.columns:\n",
    "            count = combined_df[col_name].sum()\n",
    "            if count > 0:\n",
    "                percentage = (count / total_locations) * 100\n",
    "                print(f\"    {waste_name:18s}: {count:6,d} points ({percentage:5.1f}%)\")\n",
    "\n",
    "    # Show sample from each source\n",
    "    print(f\"\\nüëÄ SAMPLE FROM EACH SOURCE:\")\n",
    "    sample_size = 2\n",
    "\n",
    "    if not df1.empty:\n",
    "        print(f\"\\n  Food Waste (df1):\")\n",
    "        sample = df1.head(sample_size)\n",
    "        for i, row in sample.iterrows():\n",
    "            name = row.get('Name', 'Unknown')[:40] if 'Name' in row else 'Unknown'\n",
    "            print(f\"    ‚Ä¢ {name}...\")\n",
    "\n",
    "    if not df2.empty:\n",
    "        print(f\"\\n  Recycling Centers (df2):\")\n",
    "        sample = df2.head(sample_size)\n",
    "        for i, row in sample.iterrows():\n",
    "            name = row.get('Name', 'Unknown')[:40] if 'Name' in row else 'Unknown'\n",
    "            print(f\"    ‚Ä¢ {name}...\")\n",
    "\n",
    "    if not df3.empty:\n",
    "        print(f\"\\n  Glass Containers (df3):\")\n",
    "        sample = df3.head(sample_size)\n",
    "        for i, row in sample.iterrows():\n",
    "            name = row.get('Name', 'Unknown')[:40] if 'Name' in row else 'Unknown'\n",
    "            print(f\"    ‚Ä¢ {name}...\")\n",
    "\n",
    "    if not df4.empty:\n",
    "        print(f\"\\n  Other Waste Containers (df4):\")\n",
    "        sample = df4.head(sample_size)\n",
    "        for i, row in sample.iterrows():\n",
    "            name = row.get('Name', 'Unknown')[:40] if 'Name' in row else 'Unknown'\n",
    "            print(f\"    ‚Ä¢ {name}...\")\n",
    "\n",
    "    if not df5.empty:\n",
    "        print(f\"\\n  Ecosystem Points (df5):\")\n",
    "        sample = df5.head(sample_size)\n",
    "        for i, row in sample.iterrows():\n",
    "            name = row.get('Name', 'Unknown')[:40] if 'Name' in row else 'Unknown'\n",
    "\n",
    "            # Check what waste types are enabled\n",
    "            enabled = []\n",
    "            for waste_name, col_name in waste_types:\n",
    "                if col_name in row and row[col_name] == 1:\n",
    "                    enabled.append(waste_name)\n",
    "            waste_info = f\" ({', '.join(enabled[:2])})\" if enabled else \"\"\n",
    "            print(f\"    ‚Ä¢ {name}...{waste_info}\")\n",
    "\n",
    "    # Save to CSV for backup\n",
    "    output_csv = \"current_trash_collection_points.csv\"\n",
    "    combined_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nüíæ Data saved locally: '{output_csv}'\")\n",
    "\n",
    "    # UPLOAD TO BIGQUERY\n",
    "    print(f\"\\n‚òÅÔ∏è UPLOADING TO BIGQUERY...\")\n",
    "\n",
    "    # CORRECTED TABLE NAME: strash ‚Üí trash\n",
    "    table_id = f\"{PROJECT}.{DATASET}.trash_collection_points\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "        autodetect=True,\n",
    "        max_bad_records=100\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(combined_df, table_id, job_config=job_config)\n",
    "        job.result()\n",
    "\n",
    "        table = client.get_table(table_id)\n",
    "        print(f\"‚úÖ BigQuery table created: {table_id}\")\n",
    "        print(f\"   Rows: {table.num_rows:,}\")\n",
    "        print(f\"   Size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Verification query\n",
    "        print(f\"\\nüîç VERIFICATION QUERY:\")\n",
    "        verify_query = f\"\"\"\n",
    "        SELECT\n",
    "          COUNT(*) as total_points,\n",
    "          SUM(CASE WHEN Latitude IS NOT NULL AND Longitude IS NOT NULL THEN 1 ELSE 0 END) as points_with_coords,\n",
    "          SUM(Is_Neon_enabled) as neon_points,\n",
    "          SUM(Is_Cartridge_enabled) as cartridge_points,\n",
    "          SUM(Is_Battery_enabled) as battery_points,\n",
    "          SUM(Is_Pile_enabled) as pile_points,\n",
    "          SUM(Is_Glass_enabled) as glass_points,\n",
    "          SUM(Is_Food_enabled) as food_points,\n",
    "          SUM(Is_Miscellanous_Trash_enabled) as dechetteries,\n",
    "          SUM(Is_Pharmacy_enabled) as pharmacy_points,\n",
    "          SUM(Is_Tire_enabled) as tire_points,\n",
    "          SUM(Is_Ressourcerie_enabled) as ressourcerie_points\n",
    "        FROM `{table_id}`\n",
    "        \"\"\"\n",
    "\n",
    "        result = client.query(verify_query).to_dataframe().iloc[0]\n",
    "        print(f\"  ‚Ä¢ Total points: {result['total_points']:,}\")\n",
    "        print(f\"  ‚Ä¢ With coordinates: {result['points_with_coords']:,}\")\n",
    "        print(f\"\\n  ‚Ä¢ Special waste points:\")\n",
    "        print(f\"     - Neon/Lamp: {result['neon_points']:,}\")\n",
    "        print(f\"     - Cartridges: {result['cartridge_points']:,}\")\n",
    "        print(f\"     - Batteries: {result['battery_points']:,}\")\n",
    "        print(f\"     - Piles: {result['pile_points']:,}\")\n",
    "        print(f\"     - Glass: {result['glass_points']:,}\")\n",
    "        print(f\"     - Food: {result['food_points']:,}\")\n",
    "        print(f\"     - D√©chetteries: {result['dechetteries']:,}\")\n",
    "        print(f\"     - Pharmacy: {result['pharmacy_points']:,}\")\n",
    "        print(f\"     - Tire: {result['tire_points']:,}\")\n",
    "        print(f\"     - Ressourcerie: {result['ressourcerie_points']:,}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå BigQuery upload failed: {e}\")\n",
    "        print(f\"   Data saved locally: '{output_csv}'\")\n",
    "\n",
    "        # Try alternative method\n",
    "        try:\n",
    "            print(\"\\nüîÑ Trying alternative CSV upload method via pandas_gbq...\")\n",
    "            import pandas_gbq\n",
    "            pandas_gbq.to_gbq(\n",
    "                combined_df,\n",
    "                destination_table=table_id,\n",
    "                project_id=PROJECT,\n",
    "                if_exists='replace',\n",
    "                progress_bar=True\n",
    "            )\n",
    "            print(\"‚úÖ Upload successful via pandas_gbq!\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Alternative also failed: {e2}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No data to combine!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ UNIFIED TRASH COLLECTION DATABASE CREATED!\")\n",
    "print(\"=\"*60)\n",
    "if 'total_locations' in locals():\n",
    "    print(f\"Total collection points: {total_locations:,}\")\n",
    "    print(f\"Datasets combined: {len([df for df in [df1, df2, df3, df4, df5] if not df.empty])}\")\n",
    "    print(f\"BigQuery table: {table_id if 'table_id' in locals() else 'N/A'}\")\n",
    "    print(f\"Local backup: {output_csv if 'output_csv' in locals() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4a70fb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADDING MULTIPLE COLLECTION POINTS TO UNIFIED DATASET\n",
      "============================================================\n",
      "\n",
      "1. LOADING TEXTILE DATA\n",
      "   Successfully loaded with latin-1 encoding\n",
      "   Loaded 165 rows, 4 columns\n",
      "   Original columns: ['Name', 'Adresse', 'Latitude', 'Longitude']\n",
      "   Column mapping: {'Name': 'Name', 'Adresse': 'Address', 'Latitude': 'Latitude', 'Longitude': 'Longitude'}\n",
      "\n",
      "   SAMPLE TEXTILE DATA (3 rows):\n",
      "                                        Name                                    Address   Latitude  Longitude\n",
      "          Le relais VERTOU - Place du march√©             7 rue de Touraine 44120 VERTOU ¬†47.170327 ¬†-1.470135\n",
      "Le relais LA HAIE FOUASSI√àRE - Rue de Pibrac   2 rue de Pibrac 44690 LA HAIE-FOUASSI√àRE ¬†47.160871 ¬†-1.427889\n",
      "                 Le relais ERTOU - D√©cathlon 4 rue des Grands Ch√¢taigniers 44120 VERTOU ¬†47.179730 ¬†-1.502325\n",
      "\n",
      "2. LOADING PHARMACY/GARAGE/RESSOURCERIE DATA\n",
      "   Successfully loaded with utf-8 encoding\n",
      "   Loaded 254 rows, 4 columns\n",
      "   Columns: ['name', 'type', 'lat', 'lon']\n",
      "   Renaming columns: {'name': 'Name', 'type': 'Type', 'lat': 'Latitude', 'lon': 'Longitude'}\n",
      "\n",
      "   UNIQUE TYPES FOUND (3):\n",
      "     ‚Ä¢ 'car_repair': 108 points\n",
      "     ‚Ä¢ 'pharmacy': 139 points\n",
      "     ‚Ä¢ 'ressourcerie': 7 points\n",
      "\n",
      "   SAMPLE MIXED DATA (5 rows):\n",
      "                       Name     Type  Latitude  Longitude\n",
      "Pharmacie du Bout des Pav√©s pharmacy 47.253252  -1.576777\n",
      "      Pharmacie Jules Verne pharmacy 47.240728  -1.530007\n",
      "        Pharmacie du Pilori pharmacy 47.216303  -1.552776\n",
      "           Pharmacie Peneau pharmacy 47.215445  -1.681621\n",
      "            Pharmacie Verte pharmacy 47.216922  -1.547101\n",
      "\n",
      "3. PROCESSING TEXTILE DATA\n",
      "   Cleaned 165 rows, kept 0 with valid coordinates\n",
      "   ‚ö†Ô∏è 300 coordinate conversion errors\n",
      "   ‚úÖ Processed 0 textile collection points\n",
      "\n",
      "4. PROCESSING MIXED DATA\n",
      "   Cleaned 254 rows, kept 254 with valid coordinates\n",
      "\n",
      "   CLASSIFYING POINTS BY TYPE:\n",
      "     ‚Ä¢ Pharmacy: 139 points\n",
      "     ‚Ä¢ Car repair/tire: 108 points\n",
      "     ‚Ä¢ Ressourcerie: 7 points\n",
      "\n",
      "   ‚úÖ Processed 254 mixed collection points\n",
      "\n",
      "   FINAL BREAKDOWN:\n",
      "     ‚Ä¢ Pharmacy: 139\n",
      "     ‚Ä¢ Car repair/tire: 108\n",
      "     ‚Ä¢ Ressourcerie: 7\n",
      "\n",
      "5. LOADING EXISTING UNIFIED TABLE\n",
      "   Checking table: trash-optimizer-479913.nantes.trash_collection_points\n",
      "   Table exists with 4338 rows\n",
      "   Created backup: trash-optimizer-479913.nantes.trash_collection_points_backup_20251205_164003\n",
      "   ‚úÖ Loaded 4338 existing rows\n",
      "   Current ID range: 1 to 4338\n",
      "\n",
      "   CHECKING REQUIRED COLUMNS:\n",
      "\n",
      "   IDENTIFYING D√âCHETTERIES:\n",
      "     ‚Ä¢ Found 19 by name pattern\n",
      "     ‚Ä¢ Total marked as d√©chetteries: 862\n",
      "\n",
      "     SAMPLE D√âCHETTERIES:\n",
      "       ‚Ä¢ Recycling Center - Ecopoint Auvours\n",
      "       ‚Ä¢ Recycling Center - D√©ch√®terie de Saint S√©bastien\n",
      "       ‚Ä¢ Recycling Center - D√©ch√®terie de Carquefou\n",
      "     ‚Ä¢ Found 579 ressourceries in existing data\n",
      "   ‚úÖ Existing data prepared\n",
      "\n",
      "6. COMBINING ALL DATASETS\n",
      "   Mixed: 254 points\n",
      "\n",
      "   Total new points to add: 254\n",
      "   Next available ID: 4339\n",
      "   Assigned IDs 4339 to 4592\n",
      "\n",
      "   ALIGNING COLUMNS:\n",
      "\n",
      "   COMBINING DATASETS:\n",
      "     Existing data: 4338 rows\n",
      "     New data: 254 rows\n",
      "     Combined: Existing + New data\n",
      "\n",
      "   ‚úÖ Combined dataset: 4592 total points\n",
      "     ‚Ä¢ Previously existing: 4338\n",
      "     ‚Ä¢ Newly added: 254\n",
      "\n",
      "   NEW ADDITIONS SUMMARY:\n",
      "     ‚Ä¢ Pharmacy: 139 points\n",
      "     ‚Ä¢ Car Repair/Tire: 108 points\n",
      "     ‚Ä¢ Ressourcerie: 7 points\n",
      "\n",
      "7. FINAL STATISTICS\n",
      "----------------------------------------\n",
      "Total collection points: 4,592\n",
      "\n",
      "WASTE TYPE COVERAGE:\n",
      "  ‚Ä¢ Food                     : 1,644 points ( 35.8%)\n",
      "  ‚Ä¢ Glass                    : 1,093 points ( 23.8%)\n",
      "  ‚Ä¢ Miscellanous Trash       :   862 points ( 18.8%)\n",
      "  ‚Ä¢ Cardboard                :   662 points ( 14.4%)\n",
      "  ‚Ä¢ Paper                    :   662 points ( 14.4%)\n",
      "  ‚Ä¢ Ressourcerie             :   586 points ( 12.8%)\n",
      "  ‚Ä¢ Metal                    :   578 points ( 12.6%)\n",
      "  ‚Ä¢ Plastic                  :   564 points ( 12.3%)\n",
      "  ‚Ä¢ Pharmacy                 :   139 points (  3.0%)\n",
      "  ‚Ä¢ Tire                     :   110 points (  2.4%)\n",
      "  ‚Ä¢ Neon                     :    56 points (  1.2%)\n",
      "  ‚Ä¢ Lamp Light               :    48 points (  1.0%)\n",
      "  ‚Ä¢ Cartridge                :    38 points (  0.8%)\n",
      "  ‚Ä¢ Vegetation               :    14 points (  0.3%)\n",
      "  ‚Ä¢ Textile                  :     9 points (  0.2%)\n",
      "\n",
      "SPECIALTY COLLECTION POINTS:\n",
      "  ‚Ä¢ Pharmacy                 :   139 points (  3.0%)\n",
      "  ‚Ä¢ Tire                     :   110 points (  2.4%)\n",
      "  ‚Ä¢ Ressourcerie             :   586 points ( 12.8%)\n",
      "  ‚Ä¢ Textile                  :     9 points (  0.2%)\n",
      "\n",
      "D√âCHETTERIE STATISTICS:\n",
      "  ‚Ä¢ Flagged as d√©chetteries: 862 (18.8%)\n",
      "  ‚Ä¢ With 'D√©ch√®terie' in name: 15\n",
      "  ‚Ä¢ D√©chetteries with ressourcerie: 15\n",
      "\n",
      "DATA QUALITY CHECK:\n",
      "  ‚Ä¢ Total rows: 4,592\n",
      "  ‚Ä¢ ‚úÖ Name: No null values\n",
      "  ‚Ä¢ ‚úÖ Longitude: No null values\n",
      "  ‚Ä¢ ‚úÖ Latitude: No null values\n",
      "  ‚Ä¢ ‚úÖ All waste type columns: No null values\n",
      "  ‚Ä¢ ‚úÖ No duplicate IDs\n",
      "  ‚Ä¢ ID range: 1 to 4592\n",
      "\n",
      "8. SAVING RESULTS\n",
      "----------------------------------------\n",
      "‚úÖ CSV saved: 'trash_collection_points_complete.csv' (4592 rows)\n",
      "\n",
      "CSV SAMPLE (first 3 rows):\n",
      "  ID 1: Food Waste - Nantes... [Other]\n",
      "  ID 2: Food Waste - Nantes... [Other]\n",
      "  ID 3: Food Waste - Nantes... [Other]\n",
      "\n",
      "UPLOADING TO BIGQUERY...\n",
      "‚úÖ BigQuery table updated: trash-optimizer-479913.nantes.trash_collection_points\n",
      "   ‚Ä¢ Rows: 4,592\n",
      "   ‚Ä¢ Size: 0.90 MB\n",
      "\n",
      "VERIFICATION QUERY RESULTS:\n",
      "  ‚Ä¢ Total points: 4,592\n",
      "  ‚Ä¢ Textile: 9\n",
      "  ‚Ä¢ Pharmacy: 139\n",
      "  ‚Ä¢ Car repair/tire: 110\n",
      "  ‚Ä¢ Ressourcerie: 586\n",
      "  ‚Ä¢ D√©chetterie: 862\n",
      "  ‚Ä¢ D√©chetterie with ressourcerie: 15\n",
      "\n",
      "================================================================================\n",
      "DATA INTEGRATION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "SUMMARY:\n",
      "  ‚Ä¢ Total collection points: 4,592\n",
      "\n",
      "NEW TYPES ADDED:\n",
      "  ‚Ä¢ Pharmacy: 139 points\n",
      "  ‚Ä¢ Car Repair/Tire: 108 points\n",
      "  ‚Ä¢ Ressourcerie: 7 points\n",
      "\n",
      "FILES CREATED:\n",
      "  ‚Ä¢ CSV: trash_collection_points_complete.csv\n",
      "  ‚Ä¢ BigQuery Table: trash-optimizer-479913.nantes.trash_collection_points\n",
      "  ‚Ä¢ Backup: trash-optimizer-479913.nantes.trash_collection_points_backup_20251205_164003\n",
      "\n",
      "NEXT STEPS:\n",
      "  1. Verify data in BigQuery Console\n",
      "  2. Test queries on the new waste types\n",
      "  3. Update any dependent dashboards or applications\n",
      "\n",
      "================================================================================\n",
      "PROCESS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ADD TEXTILE, PHARMACY, GARAGE, AND RESSOURCERIE COLLECTION POINTS TO UNIFIED DATASET\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ADDING MULTIPLE COLLECTION POINTS TO UNIFIED DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize BigQuery client (add your credentials path)\n",
    "# credentials = service_account.Credentials.from_service_account_file('your-credentials.json')\n",
    "# client = bigquery.Client(credentials=credentials, project=PROJECT)\n",
    "\n",
    "# Or if using environment authentication:\n",
    "client = bigquery.Client(project=PROJECT)\n",
    "\n",
    "# 1. LOAD AND CLEAN TEXTILE DATA\n",
    "print(\"\\n1. LOADING TEXTILE DATA\")\n",
    "\n",
    "textile_file = \"Textile_relais.csv\"\n",
    "textile_df = pd.DataFrame()\n",
    "textile_final = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(textile_file):\n",
    "    try:\n",
    "        # Try multiple encodings\n",
    "        encodings = ['latin-1', 'utf-8', 'iso-8859-1', 'cp1252']\n",
    "        textile_df = None\n",
    "\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                textile_df = pd.read_csv(textile_file, encoding=encoding, on_bad_lines='skip')\n",
    "                print(f\"   Successfully loaded with {encoding} encoding\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if textile_df is None:\n",
    "            raise Exception(\"Could not read file with any encoding\")\n",
    "\n",
    "        print(f\"   Loaded {len(textile_df)} rows, {len(textile_df.columns)} columns\")\n",
    "        print(f\"   Original columns: {textile_df.columns.tolist()}\")\n",
    "\n",
    "        # Standardize column names\n",
    "        column_mapping = {}\n",
    "        for col in textile_df.columns:\n",
    "            col_lower = str(col).lower().strip()\n",
    "\n",
    "            if any(x in col_lower for x in ['name', 'nom', 'lieu']):\n",
    "                column_mapping[col] = 'Name'\n",
    "            elif any(x in col_lower for x in ['adresse', 'address', 'street', 'rue']):\n",
    "                column_mapping[col] = 'Address'\n",
    "            elif any(x in col_lower for x in ['latitude', 'lat']):\n",
    "                column_mapping[col] = 'Latitude'\n",
    "            elif any(x in col_lower for x in ['longitude', 'lon', 'long']):\n",
    "                column_mapping[col] = 'Longitude'\n",
    "            elif any(x in col_lower for x in ['ville', 'city']):\n",
    "                column_mapping[col] = 'City'\n",
    "            elif any(x in col_lower for x in ['code_postal', 'zip', 'postal']):\n",
    "                column_mapping[col] = 'Postal_Code'\n",
    "\n",
    "        print(f\"   Column mapping: {column_mapping}\")\n",
    "        textile_df = textile_df.rename(columns=column_mapping)\n",
    "\n",
    "        # Show sample\n",
    "        if len(textile_df) > 0:\n",
    "            print(f\"\\n   SAMPLE TEXTILE DATA (3 rows):\")\n",
    "            display_cols = [c for c in ['Name', 'Address', 'Latitude', 'Longitude'] if c in textile_df.columns]\n",
    "            if display_cols:\n",
    "                print(textile_df[display_cols].head(3).to_string(index=False))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error loading textile data: {e}\")\n",
    "        textile_df = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Textile file not found: {textile_file}\")\n",
    "\n",
    "# 2. LOAD AND CLEAN PHARMACY/GARAGE/RESSOURCERIE DATA\n",
    "print(\"\\n2. LOADING PHARMACY/GARAGE/RESSOURCERIE DATA\")\n",
    "\n",
    "mixed_file = \"pharmacies_garages_ressourceries_nantes.csv\"\n",
    "mixed_df = pd.DataFrame()\n",
    "mixed_final = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(mixed_file):\n",
    "    try:\n",
    "        # Try multiple encodings\n",
    "        encodings = ['utf-8', 'latin-1', 'utf-8-sig', 'cp1252']\n",
    "        mixed_df = None\n",
    "\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                mixed_df = pd.read_csv(mixed_file, encoding=encoding, on_bad_lines='skip')\n",
    "                print(f\"   Successfully loaded with {encoding} encoding\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        if mixed_df is None:\n",
    "            # Try with different delimiters\n",
    "            try:\n",
    "                mixed_df = pd.read_csv(mixed_file, sep=';', encoding='latin-1')\n",
    "                print(\"   Successfully loaded with ';' delimiter\")\n",
    "            except:\n",
    "                raise Exception(\"Could not read mixed data file\")\n",
    "\n",
    "        print(f\"   Loaded {len(mixed_df)} rows, {len(mixed_df.columns)} columns\")\n",
    "        print(f\"   Columns: {mixed_df.columns.tolist()}\")\n",
    "\n",
    "        # Standardize column names\n",
    "        column_mapping = {}\n",
    "        for col in mixed_df.columns:\n",
    "            col_lower = str(col).lower().strip()\n",
    "\n",
    "            if any(x in col_lower for x in ['name', 'nom', '√©tablissement', 'etablissement']):\n",
    "                column_mapping[col] = 'Name'\n",
    "            elif any(x in col_lower for x in ['type', 'categorie', 'category', 'type_dechet']):\n",
    "                column_mapping[col] = 'Type'\n",
    "            elif any(x in col_lower for x in ['latitude', 'lat']):\n",
    "                column_mapping[col] = 'Latitude'\n",
    "            elif any(x in col_lower for x in ['longitude', 'lon', 'long']):\n",
    "                column_mapping[col] = 'Longitude'\n",
    "            elif any(x in col_lower for x in ['adresse', 'address', 'adresse_complete']):\n",
    "                column_mapping[col] = 'Address'\n",
    "            elif any(x in col_lower for x in ['ville', 'city']):\n",
    "                column_mapping[col] = 'City'\n",
    "            elif any(x in col_lower for x in ['code_postal', 'zip', 'postal']):\n",
    "                column_mapping[col] = 'Postal_Code'\n",
    "\n",
    "        if column_mapping:\n",
    "            print(f\"   Renaming columns: {column_mapping}\")\n",
    "            mixed_df = mixed_df.rename(columns=column_mapping)\n",
    "        else:\n",
    "            print(\"   No column mapping needed\")\n",
    "\n",
    "        # Show unique types\n",
    "        if 'Type' in mixed_df.columns:\n",
    "            unique_types = mixed_df['Type'].dropna().unique()\n",
    "            print(f\"\\n   UNIQUE TYPES FOUND ({len(unique_types)}):\")\n",
    "            for type_val in sorted(unique_types)[:20]:  # Show first 20\n",
    "                count = (mixed_df['Type'] == type_val).sum()\n",
    "                print(f\"     ‚Ä¢ '{type_val}': {count} points\")\n",
    "\n",
    "            if len(unique_types) > 20:\n",
    "                print(f\"     ... and {len(unique_types) - 20} more types\")\n",
    "\n",
    "        # Show sample\n",
    "        if len(mixed_df) > 0:\n",
    "            print(f\"\\n   SAMPLE MIXED DATA (5 rows):\")\n",
    "            display_cols = [c for c in ['Name', 'Type', 'Latitude', 'Longitude', 'Address'] if c in mixed_df.columns]\n",
    "            if display_cols:\n",
    "                print(mixed_df[display_cols].head(5).to_string(index=False))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error loading mixed data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        mixed_df = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Mixed file not found: {mixed_file}\")\n",
    "\n",
    "# 3. PROCESS TEXTILE DATA\n",
    "if not textile_df.empty:\n",
    "    print(\"\\n3. PROCESSING TEXTILE DATA\")\n",
    "\n",
    "    textile_clean = textile_df.copy()\n",
    "    initial_count = len(textile_clean)\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if 'Name' not in textile_clean.columns:\n",
    "        textile_clean['Name'] = 'Textile Collection Point'\n",
    "\n",
    "    if 'Address' not in textile_clean.columns:\n",
    "        textile_clean['Address'] = ''\n",
    "\n",
    "    # Clean text fields\n",
    "    textile_clean['Name'] = textile_clean['Name'].fillna('Textile Collection Point').astype(str).str.strip()\n",
    "    textile_clean['Address'] = textile_clean['Address'].fillna('').astype(str).str.strip()\n",
    "\n",
    "    # Clean coordinates\n",
    "    coord_errors = 0\n",
    "    if 'Latitude' in textile_clean.columns:\n",
    "        textile_clean['Latitude'] = pd.to_numeric(textile_clean['Latitude'], errors='coerce')\n",
    "        coord_errors += textile_clean['Latitude'].isna().sum()\n",
    "\n",
    "    if 'Longitude' in textile_clean.columns:\n",
    "        textile_clean['Longitude'] = pd.to_numeric(textile_clean['Longitude'], errors='coerce')\n",
    "        coord_errors += textile_clean['Longitude'].isna().sum()\n",
    "\n",
    "    # Filter out points without valid coordinates\n",
    "    if 'Latitude' in textile_clean.columns and 'Longitude' in textile_clean.columns:\n",
    "        textile_clean = textile_clean[textile_clean['Latitude'].notna() & textile_clean['Longitude'].notna()].copy()\n",
    "\n",
    "    print(f\"   Cleaned {initial_count} rows, kept {len(textile_clean)} with valid coordinates\")\n",
    "    if coord_errors > 0:\n",
    "        print(f\"   ‚ö†Ô∏è {coord_errors} coordinate conversion errors\")\n",
    "\n",
    "    # Add waste type capabilities\n",
    "    waste_types = {\n",
    "        'Is_Textile_enabled': 1,\n",
    "        'Is_Cardboard_enabled': 0,\n",
    "        'Is_Food_enabled': 0,\n",
    "        'Is_Glass_enabled': 0,\n",
    "        'Is_Metal_enabled': 0,\n",
    "        'Is_Paper_enabled': 0,\n",
    "        'Is_Plastic_enabled': 0,\n",
    "        'Is_Vegetation_enabled': 0,\n",
    "        'Is_Neon_enabled': 0,\n",
    "        'Is_Cartridge_enabled': 0,\n",
    "        'Is_Lamp_Light_enabled': 0,\n",
    "        'Is_Miscellanous_Trash_enabled': 0,\n",
    "        'Is_Pharmacy_enabled': 0,\n",
    "        'Is_Tire_enabled': 0,\n",
    "        'Is_Ressourcerie_enabled': 0\n",
    "    }\n",
    "\n",
    "    for col, default_value in waste_types.items():\n",
    "        textile_clean[col] = default_value\n",
    "\n",
    "    # Add source information\n",
    "    textile_clean['Source'] = 'Textile_relais.csv'\n",
    "    textile_clean['Data_Type'] = 'Textile_Collection'\n",
    "\n",
    "    textile_final = textile_clean\n",
    "    print(f\"   ‚úÖ Processed {len(textile_final)} textile collection points\")\n",
    "else:\n",
    "    print(\"\\n3. TEXTILE DATA: No data to process\")\n",
    "    textile_final = pd.DataFrame()\n",
    "\n",
    "# 4. PROCESS MIXED DATA (PHARMACY/GARAGE/RESSOURCERIE)\n",
    "if not mixed_df.empty:\n",
    "    print(\"\\n4. PROCESSING MIXED DATA\")\n",
    "\n",
    "    mixed_clean = mixed_df.copy()\n",
    "    initial_count = len(mixed_clean)\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if 'Name' not in mixed_clean.columns:\n",
    "        mixed_clean['Name'] = ''\n",
    "\n",
    "    if 'Type' not in mixed_clean.columns:\n",
    "        mixed_clean['Type'] = 'unknown'\n",
    "\n",
    "    if 'Address' not in mixed_clean.columns:\n",
    "        mixed_clean['Address'] = ''\n",
    "\n",
    "    # Clean fields\n",
    "    mixed_clean['Name'] = mixed_clean['Name'].fillna('').astype(str).str.strip()\n",
    "    mixed_clean['Type'] = mixed_clean['Type'].fillna('unknown').astype(str).str.lower().str.strip()\n",
    "    mixed_clean['Address'] = mixed_clean['Address'].fillna('').astype(str).str.strip()\n",
    "\n",
    "    # Clean coordinates\n",
    "    coord_errors = 0\n",
    "    if 'Latitude' in mixed_clean.columns:\n",
    "        mixed_clean['Latitude'] = pd.to_numeric(mixed_clean['Latitude'], errors='coerce')\n",
    "        coord_errors += mixed_clean['Latitude'].isna().sum()\n",
    "\n",
    "    if 'Longitude' in mixed_clean.columns:\n",
    "        mixed_clean['Longitude'] = pd.to_numeric(mixed_clean['Longitude'], errors='coerce')\n",
    "        coord_errors += mixed_clean['Longitude'].isna().sum()\n",
    "\n",
    "    # Filter out points without valid coordinates\n",
    "    if 'Latitude' in mixed_clean.columns and 'Longitude' in mixed_clean.columns:\n",
    "        mixed_clean = mixed_clean[mixed_clean['Latitude'].notna() & mixed_clean['Longitude'].notna()].copy()\n",
    "\n",
    "    print(f\"   Cleaned {initial_count} rows, kept {len(mixed_clean)} with valid coordinates\")\n",
    "    if coord_errors > 0:\n",
    "        print(f\"   ‚ö†Ô∏è {coord_errors} coordinate conversion errors\")\n",
    "\n",
    "    # Initialize all waste type columns to 0\n",
    "    waste_columns = [\n",
    "        'Is_Cardboard_enabled', 'Is_Food_enabled', 'Is_Glass_enabled',\n",
    "        'Is_Metal_enabled', 'Is_Paper_enabled', 'Is_Plastic_enabled',\n",
    "        'Is_Textile_enabled', 'Is_Vegetation_enabled', 'Is_Neon_enabled',\n",
    "        'Is_Cartridge_enabled', 'Is_Lamp_Light_enabled',\n",
    "        'Is_Miscellanous_Trash_enabled', 'Is_Pharmacy_enabled',\n",
    "        'Is_Tire_enabled', 'Is_Ressourcerie_enabled'\n",
    "    ]\n",
    "\n",
    "    for col in waste_columns:\n",
    "        mixed_clean[col] = 0\n",
    "\n",
    "    # Classify points by type\n",
    "    print(\"\\n   CLASSIFYING POINTS BY TYPE:\")\n",
    "\n",
    "    # Pharmacy points\n",
    "    pharmacy_keywords = ['pharmacy', 'pharmacie', 'm√©dicament', 'medicament', 'drug']\n",
    "    pharmacy_pattern = '|'.join(pharmacy_keywords)\n",
    "    pharmacy_mask = mixed_clean['Type'].str.contains(pharmacy_pattern, case=False, na=False)\n",
    "    mixed_clean.loc[pharmacy_mask, 'Is_Pharmacy_enabled'] = 1\n",
    "    mixed_clean.loc[pharmacy_mask, 'Data_Type'] = 'Pharmacy'\n",
    "    pharmacy_count = pharmacy_mask.sum()\n",
    "    print(f\"     ‚Ä¢ Pharmacy: {pharmacy_count} points\")\n",
    "\n",
    "    # Car repair/garage points (tire)\n",
    "    tire_keywords = ['garage', 'car_repair', 'auto', 'voiture', 'tire', 'pneu', 'v√©hicule', 'vehicle']\n",
    "    tire_pattern = '|'.join(tire_keywords)\n",
    "    tire_mask = mixed_clean['Type'].str.contains(tire_pattern, case=False, na=False)\n",
    "    mixed_clean.loc[tire_mask, 'Is_Tire_enabled'] = 1\n",
    "    mixed_clean.loc[tire_mask, 'Data_Type'] = 'Car_Repair'\n",
    "    tire_count = tire_mask.sum()\n",
    "    print(f\"     ‚Ä¢ Car repair/tire: {tire_count} points\")\n",
    "\n",
    "    # Ressourcerie points\n",
    "    ressourcerie_keywords = ['ressourcerie', 'recyclerie', 'recup', 'recycl', 'reuse', 'r√©emploi']\n",
    "    ressourcerie_pattern = '|'.join(ressourcerie_keywords)\n",
    "    ressourcerie_mask = mixed_clean['Type'].str.contains(ressourcerie_pattern, case=False, na=False)\n",
    "    mixed_clean.loc[ressourcerie_mask, 'Is_Ressourcerie_enabled'] = 1\n",
    "    mixed_clean.loc[ressourcerie_mask, 'Data_Type'] = 'Ressourcerie'\n",
    "    ressourcerie_count = ressourcerie_mask.sum()\n",
    "    print(f\"     ‚Ä¢ Ressourcerie: {ressourcerie_count} points\")\n",
    "\n",
    "    # Handle overlaps (if a point matches multiple categories)\n",
    "    overlap_mask = (pharmacy_mask.astype(int) + tire_mask.astype(int) + ressourcerie_mask.astype(int)) > 1\n",
    "    overlap_count = overlap_mask.sum()\n",
    "    if overlap_count > 0:\n",
    "        print(f\"     ‚ö†Ô∏è {overlap_count} points match multiple categories\")\n",
    "        # For overlaps, prioritize in this order: Pharmacy > Ressourcerie > Car Repair\n",
    "        for idx in mixed_clean[overlap_mask].index:\n",
    "            if pharmacy_mask[idx]:\n",
    "                mixed_clean.at[idx, 'Data_Type'] = 'Pharmacy'\n",
    "            elif ressourcerie_mask[idx]:\n",
    "                mixed_clean.at[idx, 'Data_Type'] = 'Ressourcerie'\n",
    "            elif tire_mask[idx]:\n",
    "                mixed_clean.at[idx, 'Data_Type'] = 'Car_Repair'\n",
    "\n",
    "    # Check for unclassified points\n",
    "    classified_mask = pharmacy_mask | tire_mask | ressourcerie_mask\n",
    "    unclassified = mixed_clean[~classified_mask]\n",
    "    if len(unclassified) > 0:\n",
    "        print(f\"\\n   ‚ö†Ô∏è UNCLASSIFIED POINTS ({len(unclassified)}):\")\n",
    "        unique_unclassified_types = unclassified['Type'].unique()[:10]\n",
    "        for t in unique_unclassified_types:\n",
    "            count = (unclassified['Type'] == t).sum()\n",
    "            print(f\"     ‚Ä¢ '{t}': {count} points\")\n",
    "\n",
    "        # Default unclassified to 'Other'\n",
    "        mixed_clean.loc[~classified_mask, 'Data_Type'] = 'Other'\n",
    "\n",
    "    # Add source information\n",
    "    mixed_clean['Source'] = 'pharmacies_garages_ressourceries_nantes.csv'\n",
    "    if 'Data_Type' not in mixed_clean.columns:\n",
    "        mixed_clean['Data_Type'] = 'Unknown'\n",
    "\n",
    "    mixed_final = mixed_clean\n",
    "    print(f\"\\n   ‚úÖ Processed {len(mixed_final)} mixed collection points\")\n",
    "\n",
    "    # Final breakdown\n",
    "    print(f\"\\n   FINAL BREAKDOWN:\")\n",
    "    if 'Is_Pharmacy_enabled' in mixed_final.columns:\n",
    "        print(f\"     ‚Ä¢ Pharmacy: {mixed_final['Is_Pharmacy_enabled'].sum()}\")\n",
    "    if 'Is_Tire_enabled' in mixed_final.columns:\n",
    "        print(f\"     ‚Ä¢ Car repair/tire: {mixed_final['Is_Tire_enabled'].sum()}\")\n",
    "    if 'Is_Ressourcerie_enabled' in mixed_final.columns:\n",
    "        print(f\"     ‚Ä¢ Ressourcerie: {mixed_final['Is_Ressourcerie_enabled'].sum()}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n4. MIXED DATA: No data to process\")\n",
    "    mixed_final = pd.DataFrame()\n",
    "\n",
    "# 5. LOAD EXISTING UNIFIED TABLE\n",
    "print(\"\\n5. LOADING EXISTING UNIFIED TABLE\")\n",
    "\n",
    "existing_table_name = f\"{PROJECT}.{DATASET}.trash_collection_points\"\n",
    "backup_table_name = f\"{PROJECT}.{DATASET}.trash_collection_points_backup_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "try:\n",
    "    # First, check if table exists\n",
    "    print(f\"   Checking table: {existing_table_name}\")\n",
    "\n",
    "    # Try to get table metadata\n",
    "    try:\n",
    "        table = client.get_table(existing_table_name)\n",
    "        print(f\"   Table exists with {table.num_rows} rows\")\n",
    "\n",
    "        # Create backup\n",
    "        backup_query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{backup_table_name}` AS\n",
    "        SELECT * FROM `{existing_table_name}`\n",
    "        \"\"\"\n",
    "        client.query(backup_query).result()\n",
    "        print(f\"   Created backup: {backup_table_name}\")\n",
    "\n",
    "        # Load existing data\n",
    "        existing_query = f\"SELECT * FROM `{existing_table_name}`\"\n",
    "        existing_df = client.query(existing_query).to_dataframe()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Table doesn't exist or error: {e}\")\n",
    "        print(\"   Creating new table\")\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    if not existing_df.empty:\n",
    "        print(f\"   ‚úÖ Loaded {len(existing_df)} existing rows\")\n",
    "\n",
    "        # Show current ID range\n",
    "        if 'ID' in existing_df.columns:\n",
    "            min_id = existing_df['ID'].min()\n",
    "            max_id = existing_df['ID'].max()\n",
    "            print(f\"   Current ID range: {min_id} to {max_id}\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No ID column in existing table\")\n",
    "            # Create ID column if missing\n",
    "            existing_df['ID'] = range(1, len(existing_df) + 1)\n",
    "\n",
    "        # Check for missing waste type columns and add them\n",
    "        required_columns = [\n",
    "            'ID', 'Name', 'Address', 'Longitude', 'Latitude',\n",
    "            'Is_Cardboard_enabled', 'Is_Food_enabled', 'Is_Glass_enabled',\n",
    "            'Is_Metal_enabled', 'Is_Paper_enabled', 'Is_Plastic_enabled',\n",
    "            'Is_Textile_enabled', 'Is_Vegetation_enabled', 'Is_Neon_enabled',\n",
    "            'Is_Cartridge_enabled', 'Is_Lamp_Light_enabled',\n",
    "            'Is_Miscellanous_Trash_enabled', 'Is_Pharmacy_enabled',\n",
    "            'Is_Tire_enabled', 'Is_Ressourcerie_enabled'\n",
    "        ]\n",
    "\n",
    "        print(\"\\n   CHECKING REQUIRED COLUMNS:\")\n",
    "        for col in required_columns:\n",
    "            if col not in existing_df.columns:\n",
    "                if col.startswith('Is_'):\n",
    "                    existing_df[col] = 0\n",
    "                    print(f\"     ‚ö†Ô∏è Added missing: {col} (default 0)\")\n",
    "                else:\n",
    "                    existing_df[col] = None\n",
    "                    print(f\"     ‚ö†Ô∏è Added missing: {col}\")\n",
    "\n",
    "        # FIX: Identify and mark d√©chetteries\n",
    "        print(\"\\n   IDENTIFYING D√âCHETTERIES:\")\n",
    "\n",
    "        # Check for d√©chetteries in Name\n",
    "        d√©chetterie_patterns = [\n",
    "            'd√©ch√®terie', 'd√©chetterie', 'dechetterie', 'decheterie',\n",
    "            'recycling center', 'centre de recyclage', 'waste center'\n",
    "        ]\n",
    "        pattern = '|'.join([re.escape(p) for p in d√©chetterie_patterns])\n",
    "\n",
    "        name_mask = existing_df['Name'].astype(str).str.contains(pattern, case=False, na=False)\n",
    "        d√©chetterie_count = name_mask.sum()\n",
    "\n",
    "        # Mark as miscellaneous trash enabled\n",
    "        existing_df.loc[name_mask, 'Is_Miscellanous_Trash_enabled'] = 1\n",
    "\n",
    "        # Also check if already marked\n",
    "        already_marked = existing_df['Is_Miscellanous_Trash_enabled'].sum()\n",
    "\n",
    "        print(f\"     ‚Ä¢ Found {d√©chetterie_count} by name pattern\")\n",
    "        print(f\"     ‚Ä¢ Total marked as d√©chetteries: {already_marked}\")\n",
    "\n",
    "        if d√©chetterie_count > 0:\n",
    "            print(f\"\\n     SAMPLE D√âCHETTERIES:\")\n",
    "            sample = existing_df[name_mask].head(3)\n",
    "            for i, row in sample.iterrows():\n",
    "                name = str(row.get('Name', 'Unknown'))[:50]\n",
    "                print(f\"       ‚Ä¢ {name}\")\n",
    "\n",
    "        # Check for ressourceries in existing data\n",
    "        if 'Is_Ressourcerie_enabled' in existing_df.columns:\n",
    "            ressourcerie_names = ['ressourcerie', 'recyclerie', 'recup', 'recycl']\n",
    "            ressourcerie_pattern = '|'.join([re.escape(n) for n in ressourcerie_names])\n",
    "            ressourcerie_mask = existing_df['Name'].astype(str).str.contains(ressourcerie_pattern, case=False, na=False)\n",
    "\n",
    "            existing_df.loc[ressourcerie_mask, 'Is_Ressourcerie_enabled'] = 1\n",
    "            print(f\"     ‚Ä¢ Found {ressourcerie_mask.sum()} ressourceries in existing data\")\n",
    "\n",
    "        # Ensure all waste columns are integers\n",
    "        waste_cols = [col for col in existing_df.columns if col.startswith('Is_') and col.endswith('_enabled')]\n",
    "        for col in waste_cols:\n",
    "            existing_df[col] = pd.to_numeric(existing_df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        print(\"   ‚úÖ Existing data prepared\")\n",
    "\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è No existing data or empty table\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error loading existing table: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"   Creating new empty dataframe\")\n",
    "    existing_df = pd.DataFrame()\n",
    "\n",
    "# 6. COMBINE ALL DATASETS\n",
    "print(\"\\n6. COMBINING ALL DATASETS\")\n",
    "\n",
    "# Collect all new data\n",
    "new_datasets = []\n",
    "if not textile_final.empty:\n",
    "    new_datasets.append(textile_final)\n",
    "    print(f\"   Textile: {len(textile_final)} points\")\n",
    "if not mixed_final.empty:\n",
    "    new_datasets.append(mixed_final)\n",
    "    print(f\"   Mixed: {len(mixed_final)} points\")\n",
    "\n",
    "if new_datasets:\n",
    "    # Combine new datasets\n",
    "    if len(new_datasets) == 1:\n",
    "        new_data_combined = new_datasets[0]\n",
    "    else:\n",
    "        new_data_combined = pd.concat(new_datasets, ignore_index=True, sort=False)\n",
    "\n",
    "    print(f\"\\n   Total new points to add: {len(new_data_combined)}\")\n",
    "\n",
    "    # Determine next available ID\n",
    "    if not existing_df.empty and 'ID' in existing_df.columns:\n",
    "        next_id = int(existing_df['ID'].max()) + 1\n",
    "    else:\n",
    "        next_id = 1\n",
    "\n",
    "    print(f\"   Next available ID: {next_id}\")\n",
    "\n",
    "    # Assign IDs to new points\n",
    "    new_data_combined['ID'] = range(next_id, next_id + len(new_data_combined))\n",
    "    print(f\"   Assigned IDs {next_id} to {next_id + len(new_data_combined) - 1}\")\n",
    "\n",
    "    # Ensure both dataframes have compatible columns\n",
    "    print(\"\\n   ALIGNING COLUMNS:\")\n",
    "\n",
    "    # Define core columns (always needed)\n",
    "    core_columns = [\n",
    "        'ID', 'Name', 'Address', 'Longitude', 'Latitude',\n",
    "        'Is_Cardboard_enabled', 'Is_Food_enabled', 'Is_Glass_enabled',\n",
    "        'Is_Metal_enabled', 'Is_Paper_enabled', 'Is_Plastic_enabled',\n",
    "        'Is_Textile_enabled', 'Is_Vegetation_enabled', 'Is_Neon_enabled',\n",
    "        'Is_Cartridge_enabled', 'Is_Lamp_Light_enabled',\n",
    "        'Is_Miscellanous_Trash_enabled', 'Is_Pharmacy_enabled',\n",
    "        'Is_Tire_enabled', 'Is_Ressourcerie_enabled'\n",
    "    ]\n",
    "\n",
    "    # Remove metadata columns from new data that might conflict\n",
    "    metadata_cols_to_remove = ['City', 'Postal_Code', 'Source', 'Data_Type', 'Type']\n",
    "    for col in metadata_cols_to_remove:\n",
    "        if col in new_data_combined.columns:\n",
    "            new_data_combined = new_data_combined.drop(columns=[col], errors='ignore')\n",
    "\n",
    "    # Add missing columns to existing data\n",
    "    if not existing_df.empty:\n",
    "        for col in core_columns:\n",
    "            if col not in existing_df.columns:\n",
    "                if col.startswith('Is_'):\n",
    "                    existing_df[col] = 0\n",
    "                else:\n",
    "                    existing_df[col] = None\n",
    "\n",
    "    # Add missing columns to new data\n",
    "    for col in core_columns:\n",
    "        if col not in new_data_combined.columns:\n",
    "            if col.startswith('Is_'):\n",
    "                new_data_combined[col] = 0\n",
    "            else:\n",
    "                new_data_combined[col] = None\n",
    "\n",
    "    # Ensure all waste columns are integers\n",
    "    waste_cols = [col for col in core_columns if col.startswith('Is_')]\n",
    "    for col in waste_cols:\n",
    "        if col in new_data_combined.columns:\n",
    "            new_data_combined[col] = pd.to_numeric(new_data_combined[col], errors='coerce').fillna(0).astype(int)\n",
    "        if col in existing_df.columns and not existing_df.empty:\n",
    "            existing_df[col] = pd.to_numeric(existing_df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # Combine datasets\n",
    "    print(f\"\\n   COMBINING DATASETS:\")\n",
    "    print(f\"     Existing data: {len(existing_df)} rows\")\n",
    "    print(f\"     New data: {len(new_data_combined)} rows\")\n",
    "\n",
    "    if existing_df.empty:\n",
    "        combined_df = new_data_combined[core_columns].copy()\n",
    "        print(\"     Combined: New data only (no existing)\")\n",
    "    else:\n",
    "        combined_df = pd.concat([\n",
    "            existing_df[core_columns],\n",
    "            new_data_combined[core_columns]\n",
    "        ], ignore_index=True)\n",
    "        print(\"     Combined: Existing + New data\")\n",
    "\n",
    "    # Final cleanup\n",
    "    combined_df = combined_df.drop_duplicates(subset=['ID'], keep='first')\n",
    "    combined_df = combined_df.sort_values('ID').reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n   ‚úÖ Combined dataset: {len(combined_df)} total points\")\n",
    "    print(f\"     ‚Ä¢ Previously existing: {len(existing_df)}\")\n",
    "    print(f\"     ‚Ä¢ Newly added: {len(new_data_combined)}\")\n",
    "\n",
    "    # Show what was added\n",
    "    print(f\"\\n   NEW ADDITIONS SUMMARY:\")\n",
    "    if not textile_final.empty:\n",
    "        print(f\"     ‚Ä¢ Textile: {len(textile_final)} points\")\n",
    "\n",
    "    if not mixed_final.empty:\n",
    "        # Count by type from mixed data\n",
    "        type_counts = {}\n",
    "        if 'Is_Pharmacy_enabled' in mixed_final.columns:\n",
    "            type_counts['Pharmacy'] = mixed_final['Is_Pharmacy_enabled'].sum()\n",
    "        if 'Is_Tire_enabled' in mixed_final.columns:\n",
    "            type_counts['Car Repair/Tire'] = mixed_final['Is_Tire_enabled'].sum()\n",
    "        if 'Is_Ressourcerie_enabled' in mixed_final.columns:\n",
    "            type_counts['Ressourcerie'] = mixed_final['Is_Ressourcerie_enabled'].sum()\n",
    "\n",
    "        for type_name, count in type_counts.items():\n",
    "            print(f\"     ‚Ä¢ {type_name}: {count} points\")\n",
    "\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è No new data to add\")\n",
    "    if not existing_df.empty:\n",
    "        combined_df = existing_df\n",
    "        print(f\"   Keeping existing {len(combined_df)} points\")\n",
    "    else:\n",
    "        combined_df = pd.DataFrame()\n",
    "        print(\"   ‚ö†Ô∏è No data at all!\")\n",
    "\n",
    "# 7. FINAL STATISTICS\n",
    "if not combined_df.empty:\n",
    "    print(\"\\n7. FINAL STATISTICS\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(f\"Total collection points: {len(combined_df):,}\")\n",
    "\n",
    "    # Count by waste type\n",
    "    print(f\"\\nWASTE TYPE COVERAGE:\")\n",
    "    waste_stats = {}\n",
    "    for col in combined_df.columns:\n",
    "        if col.startswith('Is_') and col.endswith('_enabled'):\n",
    "            count = int(combined_df[col].sum())\n",
    "            if count > 0:\n",
    "                waste_name = col.replace('Is_', '').replace('_enabled', '').replace('_', ' ').title()\n",
    "                percentage = (count / len(combined_df)) * 100\n",
    "                waste_stats[waste_name] = (count, percentage)\n",
    "\n",
    "    # Sort by count descending\n",
    "    for waste, (count, percentage) in sorted(waste_stats.items(), key=lambda x: x[1][0], reverse=True):\n",
    "        print(f\"  ‚Ä¢ {waste:25s}: {count:5,d} points ({percentage:5.1f}%)\")\n",
    "\n",
    "    # Special focus on new types\n",
    "    print(f\"\\nSPECIALTY COLLECTION POINTS:\")\n",
    "    specialty_types = ['Pharmacy', 'Tire', 'Ressourcerie', 'Textile']\n",
    "    for type_name in specialty_types:\n",
    "        col_name = f'Is_{type_name}_enabled'\n",
    "        if col_name in combined_df.columns:\n",
    "            count = int(combined_df[col_name].sum())\n",
    "            if count > 0:\n",
    "                percentage = (count / len(combined_df)) * 100\n",
    "                print(f\"  ‚Ä¢ {type_name:25s}: {count:5,d} points ({percentage:5.1f}%)\")\n",
    "\n",
    "    # D√©chetterie statistics\n",
    "    print(f\"\\nD√âCHETTERIE STATISTICS:\")\n",
    "    if 'Is_Miscellanous_Trash_enabled' in combined_df.columns:\n",
    "        dechetterie_count = int(combined_df['Is_Miscellanous_Trash_enabled'].sum())\n",
    "        dechetterie_percentage = (dechetterie_count / len(combined_df)) * 100\n",
    "        print(f\"  ‚Ä¢ Flagged as d√©chetteries: {dechetterie_count:,} ({dechetterie_percentage:.1f}%)\")\n",
    "\n",
    "        # Also check by name\n",
    "        d√©chetterie_patterns = ['d√©ch√®terie', 'd√©chetterie', 'dechetterie']\n",
    "        pattern = '|'.join([re.escape(p) for p in d√©chetterie_patterns])\n",
    "        name_based_count = combined_df['Name'].astype(str).str.contains(pattern, case=False, na=False).sum()\n",
    "        print(f\"  ‚Ä¢ With 'D√©ch√®terie' in name: {name_based_count:,}\")\n",
    "\n",
    "        if 'Is_Ressourcerie_enabled' in combined_df.columns:\n",
    "            dechetterie_ressourcerie = combined_df[\n",
    "                (combined_df['Is_Miscellanous_Trash_enabled'] == 1) &\n",
    "                (combined_df['Is_Ressourcerie_enabled'] == 1)\n",
    "            ].shape[0]\n",
    "            print(f\"  ‚Ä¢ D√©chetteries with ressourcerie: {dechetterie_ressourcerie:,}\")\n",
    "\n",
    "    # Data quality check\n",
    "    print(f\"\\nDATA QUALITY CHECK:\")\n",
    "    print(f\"  ‚Ä¢ Total rows: {len(combined_df):,}\")\n",
    "\n",
    "    # Check for nulls in critical columns\n",
    "    critical_cols = ['Name', 'Longitude', 'Latitude']\n",
    "    for col in critical_cols:\n",
    "        if col in combined_df.columns:\n",
    "            null_count = combined_df[col].isna().sum()\n",
    "            if null_count == 0:\n",
    "                print(f\"  ‚Ä¢ ‚úÖ {col}: No null values\")\n",
    "            else:\n",
    "                print(f\"  ‚Ä¢ ‚ö†Ô∏è {col}: {null_count:,} null values ({null_count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "    # Check waste columns for nulls\n",
    "    waste_null_count = 0\n",
    "    waste_cols = [col for col in combined_df.columns if col.startswith('Is_') and col.endswith('_enabled')]\n",
    "    for col in waste_cols:\n",
    "        null_count = combined_df[col].isna().sum()\n",
    "        if null_count > 0:\n",
    "            waste_null_count += null_count\n",
    "\n",
    "    if waste_null_count == 0:\n",
    "        print(f\"  ‚Ä¢ ‚úÖ All waste type columns: No null values\")\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ ‚ö†Ô∏è Waste type columns: {waste_null_count:,} total null values\")\n",
    "\n",
    "    # ID check\n",
    "    if 'ID' in combined_df.columns:\n",
    "        duplicate_ids = combined_df['ID'].duplicated().sum()\n",
    "        if duplicate_ids == 0:\n",
    "            print(f\"  ‚Ä¢ ‚úÖ No duplicate IDs\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ ‚ùå {duplicate_ids:,} duplicate IDs!\")\n",
    "\n",
    "        id_range = f\"{combined_df['ID'].min()} to {combined_df['ID'].max()}\"\n",
    "        print(f\"  ‚Ä¢ ID range: {id_range}\")\n",
    "else:\n",
    "    print(\"\\n7. STATISTICS: No data available\")\n",
    "\n",
    "# 8. SAVE RESULTS\n",
    "print(\"\\n8. SAVING RESULTS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if not combined_df.empty:\n",
    "    # Save to CSV\n",
    "    output_csv = \"trash_collection_points_complete.csv\"\n",
    "\n",
    "    # Ensure proper column order\n",
    "    core_columns = [\n",
    "        'ID', 'Name', 'Address', 'Longitude', 'Latitude',\n",
    "        'Is_Cardboard_enabled', 'Is_Food_enabled', 'Is_Glass_enabled',\n",
    "        'Is_Metal_enabled', 'Is_Paper_enabled', 'Is_Plastic_enabled',\n",
    "        'Is_Textile_enabled', 'Is_Vegetation_enabled', 'Is_Neon_enabled',\n",
    "        'Is_Cartridge_enabled', 'Is_Lamp_Light_enabled',\n",
    "        'Is_Miscellanous_Trash_enabled', 'Is_Pharmacy_enabled',\n",
    "        'Is_Tire_enabled', 'Is_Ressourcerie_enabled'\n",
    "    ]\n",
    "\n",
    "    # Filter to only include columns that exist\n",
    "    final_columns = [col for col in core_columns if col in combined_df.columns]\n",
    "    combined_df = combined_df[final_columns]\n",
    "\n",
    "    # Save to CSV\n",
    "    combined_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úÖ CSV saved: '{output_csv}' ({len(combined_df)} rows)\")\n",
    "\n",
    "    # Show CSV sample\n",
    "    print(f\"\\nCSV SAMPLE (first 3 rows):\")\n",
    "    try:\n",
    "        csv_preview = pd.read_csv(output_csv, nrows=3)\n",
    "        for i, row in csv_preview.iterrows():\n",
    "            point_id = row.get('ID', 'N/A')\n",
    "            name = str(row.get('Name', 'Unnamed'))[:40]\n",
    "\n",
    "            # Determine point type\n",
    "            point_types = []\n",
    "            if row.get('Is_Pharmacy_enabled', 0) == 1:\n",
    "                point_types.append(\"Pharmacy\")\n",
    "            if row.get('Is_Tire_enabled', 0) == 1:\n",
    "                point_types.append(\"Car Repair\")\n",
    "            if row.get('Is_Ressourcerie_enabled', 0) == 1:\n",
    "                point_types.append(\"Ressourcerie\")\n",
    "            if row.get('Is_Textile_enabled', 0) == 1:\n",
    "                point_types.append(\"Textile\")\n",
    "            if row.get('Is_Miscellanous_Trash_enabled', 0) == 1:\n",
    "                point_types.append(\"D√©chetterie\")\n",
    "\n",
    "            point_type_str = \", \".join(point_types) if point_types else \"Other\"\n",
    "            print(f\"  ID {point_id}: {name}... [{point_type_str}]\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not read CSV sample: {e}\")\n",
    "\n",
    "    # Upload to BigQuery\n",
    "    print(f\"\\nUPLOADING TO BIGQUERY...\")\n",
    "\n",
    "    # Use the original table name for consistency\n",
    "    bq_table_name = f\"{PROJECT}.{DATASET}.trash_collection_points\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "        autodetect=True,\n",
    "        max_bad_records=10\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Upload dataframe\n",
    "        job = client.load_table_from_dataframe(\n",
    "            combined_df,\n",
    "            bq_table_name,\n",
    "            job_config=job_config\n",
    "        )\n",
    "        job.result()\n",
    "\n",
    "        # Verify upload\n",
    "        table = client.get_table(bq_table_name)\n",
    "        print(f\"‚úÖ BigQuery table updated: {bq_table_name}\")\n",
    "        print(f\"   ‚Ä¢ Rows: {table.num_rows:,}\")\n",
    "        print(f\"   ‚Ä¢ Size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Run verification query\n",
    "        print(f\"\\nVERIFICATION QUERY RESULTS:\")\n",
    "        verify_query = f\"\"\"\n",
    "        SELECT\n",
    "          COUNT(*) as total_points,\n",
    "          SUM(Is_Textile_enabled) as textile_points,\n",
    "          SUM(Is_Pharmacy_enabled) as pharmacy_points,\n",
    "          SUM(Is_Tire_enabled) as tire_points,\n",
    "          SUM(Is_Ressourcerie_enabled) as ressourcerie_points,\n",
    "          SUM(Is_Miscellanous_Trash_enabled) as dechetterie_points,\n",
    "          SUM(CASE WHEN Is_Miscellanous_Trash_enabled = 1 AND Is_Ressourcerie_enabled = 1 THEN 1 ELSE 0 END) as dechetterie_ressourceries\n",
    "        FROM `{bq_table_name}`\n",
    "        \"\"\"\n",
    "\n",
    "        result = client.query(verify_query).to_dataframe().iloc[0]\n",
    "        print(f\"  ‚Ä¢ Total points: {result['total_points']:,}\")\n",
    "        print(f\"  ‚Ä¢ Textile: {result['textile_points']:,}\")\n",
    "        print(f\"  ‚Ä¢ Pharmacy: {result['pharmacy_points']:,}\")\n",
    "        print(f\"  ‚Ä¢ Car repair/tire: {result['tire_points']:,}\")\n",
    "        print(f\"  ‚Ä¢ Ressourcerie: {result['ressourcerie_points']:,}\")\n",
    "        print(f\"  ‚Ä¢ D√©chetterie: {result['dechetterie_points']:,}\")\n",
    "        print(f\"  ‚Ä¢ D√©chetterie with ressourcerie: {result['dechetterie_ressourceries']:,}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå BigQuery upload failed: {e}\")\n",
    "        print(\"\\nTrying alternative method...\")\n",
    "\n",
    "        try:\n",
    "            # Try CSV upload\n",
    "            with open(output_csv, 'rb') as source_file:\n",
    "                job = client.load_table_from_file(\n",
    "                    source_file,\n",
    "                    bq_table_name,\n",
    "                    job_config=job_config\n",
    "                )\n",
    "                job.result()\n",
    "                print(\"‚úÖ Upload successful via CSV file\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå CSV upload also failed: {e2}\")\n",
    "            print(\"\\nYou can manually upload the CSV file to BigQuery:\")\n",
    "            print(f\"  1. Go to BigQuery Console\")\n",
    "            print(f\"  2. Create or select table: {bq_table_name}\")\n",
    "            print(f\"  3. Upload file: {output_csv}\")\n",
    "            print(f\"  4. Schema autodetection should work\")\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATA INTEGRATION COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\nSUMMARY:\")\n",
    "    print(f\"  ‚Ä¢ Total collection points: {len(combined_df):,}\")\n",
    "\n",
    "    if len(combined_df) < 1000:\n",
    "        print(f\"  ‚ö†Ô∏è WARNING: Only {len(combined_df)} points - expected more!\")\n",
    "\n",
    "    print(f\"\\nNEW TYPES ADDED:\")\n",
    "    new_type_counts = {\n",
    "        'Textile': textile_final['Is_Textile_enabled'].sum() if not textile_final.empty else 0,\n",
    "        'Pharmacy': mixed_final['Is_Pharmacy_enabled'].sum() if not mixed_final.empty else 0,\n",
    "        'Car Repair/Tire': mixed_final['Is_Tire_enabled'].sum() if not mixed_final.empty else 0,\n",
    "        'Ressourcerie': mixed_final['Is_Ressourcerie_enabled'].sum() if not mixed_final.empty else 0\n",
    "    }\n",
    "\n",
    "    for type_name, count in new_type_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"  ‚Ä¢ {type_name}: {count:,} points\")\n",
    "\n",
    "    print(f\"\\nFILES CREATED:\")\n",
    "    print(f\"  ‚Ä¢ CSV: {output_csv}\")\n",
    "    print(f\"  ‚Ä¢ BigQuery Table: {bq_table_name}\")\n",
    "\n",
    "    if 'backup_table_name' in locals():\n",
    "        print(f\"  ‚Ä¢ Backup: {backup_table_name}\")\n",
    "\n",
    "    print(f\"\\nNEXT STEPS:\")\n",
    "    print(f\"  1. Verify data in BigQuery Console\")\n",
    "    print(f\"  2. Test queries on the new waste types\")\n",
    "    print(f\"  3. Update any dependent dashboards or applications\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No data to save!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROCESS COMPLETE\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ed1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADDING MULTIPLE COLLECTION POINTS TO UNIFIED DATASET\n",
      "LOADING TEXTILE DATA\n",
      "Loaded textile data: 165 rows\n",
      "   Renaming columns: {'Name': 'Name', 'Adresse': 'Address', 'Latitude': 'Latitude', 'Longitude': 'Longitude'}\n",
      "SAMPLE TEXTILE DATA:\n",
      "                                        Name                                    Address   Latitude  Longitude\n",
      "          Le relais VERTOU - Place du march√©             7 rue de Touraine 44120 VERTOU ¬†47.170327 ¬†-1.470135\n",
      "Le relais LA HAIE FOUASSI√àRE - Rue de Pibrac   2 rue de Pibrac 44690 LA HAIE-FOUASSI√àRE ¬†47.160871 ¬†-1.427889\n",
      "                 Le relais ERTOU - D√©cathlon 4 rue des Grands Ch√¢taigniers 44120 VERTOU ¬†47.179730 ¬†-1.502325\n",
      "\n",
      "LOADING PHARMACY/GARAGE/RESSOURCERIE DATA\n",
      "Loaded mixed data: 254 rows\n",
      "   Columns: ['name', 'type', 'lat', 'lon']\n",
      "   Renaming columns: {'name': 'Name', 'type': 'Type', 'lat': 'Latitude', 'lon': 'Longitude'}\n",
      "   Unique types found: ['pharmacy', 'car_repair', 'ressourcerie']\n",
      "     ‚Ä¢ pharmacy: 139 points\n",
      "     ‚Ä¢ car_repair: 108 points\n",
      "     ‚Ä¢ ressourcerie: 7 points\n",
      "SAMPLE MIXED DATA:\n",
      "                       Name     Type  Latitude  Longitude\n",
      "Pharmacie du Bout des Pav√©s pharmacy 47.253252  -1.576777\n",
      "      Pharmacie Jules Verne pharmacy 47.240728  -1.530007\n",
      "        Pharmacie du Pilori pharmacy 47.216303  -1.552776\n",
      "           Pharmacie Peneau pharmacy 47.215445  -1.681621\n",
      "            Pharmacie Verte pharmacy 47.216922  -1.547101\n",
      "PROCESSING TEXTILE DATA\n",
      "Processed 69 textile collection points\n",
      "PROCESSING MIXED DATA\n",
      "   Pharmacy points: 139\n",
      "   Car repair/tire points: 108\n",
      "   Ressourcerie points: 7\n",
      "Processed 254 mixed collection points\n",
      "MIXED DATA BREAKDOWN:\n",
      "   ‚Ä¢ Pharmacy: 139\n",
      "   ‚Ä¢ Car repair/tire: 108\n",
      "   ‚Ä¢ Ressourcerie: 7\n",
      "SAMPLE OF PROCESSED MIXED DATA:\n",
      "   1. Pharmacie du Bout des Pav√©s (pharmacy)\n",
      "   2. Pharmacie Jules Verne (pharmacy)\n",
      "   3. Pharmacie du Pilori (pharmacy)\n",
      "   4. Pharmacie Peneau (pharmacy)\n",
      "   5. Pharmacie Verte (pharmacy)\n",
      "\n",
      "LOADING EXISTING UNIFIED TABLE\n",
      "‚úÖ Loaded existing table: 4338 rows\n",
      "   Current ID range: 1 to 4338\n",
      "   Added missing waste type column: Is_Pharmacy_enabled\n",
      "   Added missing waste type column: Is_Tire_enabled\n",
      "   Added missing waste type column: Is_Ressourcerie_enabled\n",
      "\n",
      "   IDENTIFYING D√âCH√àTERIES IN NAME COLUMN...\n",
      "   Created missing column: Is_Miscellanous_Trash_enabled\n",
      "   Found 19 entries with 'D√©ch√®terie' in Name\n",
      "   Sample d√©chetteries found:\n",
      "     ‚Ä¢ Recycling Center - Ecopoint Auvours\n",
      "     ‚Ä¢ Recycling Center - D√©ch√®terie de Saint S√©bastien\n",
      "     ‚Ä¢ Recycling Center - D√©ch√®terie de Carquefou\n",
      "     ‚Ä¢ Recycling Center - D√©ch√®terie de Saint Aignan Grand Lieu\n",
      "     ‚Ä¢ Recycling Center - Ecopoint Chantenay\n",
      "   Total d√©chetteries marked: 19\n",
      "   0 d√©chetteries are also ressourceries\n",
      "   Fixed null values in waste type columns\n",
      "\n",
      "COMBINING ALL DATASETS\n",
      "   Textile data ready: 69 points\n",
      "   Mixed data ready: 254 points\n",
      "\n",
      "   Total new data to add: 323 points\n",
      "   Next available ID: 4339\n",
      "   Assigned IDs 4339 to 4661 to new points\n",
      "   Removed metadata column from new data: Type\n",
      "\n",
      "üîó COMBINING DATA:\n",
      "   Existing data shape: (4338, 21)\n",
      "   New data shape: (323, 21)\n",
      "   Combined: Existing + New data\n",
      "‚úÖ Combined dataset: 4661 total points\n",
      "   - Existing points: 4338\n",
      "   - New points: 323\n",
      "\n",
      "üìä NEW DATA ADDED:\n",
      "   ‚Ä¢ Textile points: 69\n",
      "   ‚Ä¢ Pharmacy points: 139\n",
      "   ‚Ä¢ Car repair/tire points: 108\n",
      "   ‚Ä¢ Ressourcerie points: 7\n",
      "\n",
      "UPDATED STATISTICS:\n",
      "   Total collection points: 4,661\n",
      "\n",
      "   Waste/Specialty type coverage:\n",
      "     ‚Ä¢ Cardboard           :  1,505 points ( 32.3%)\n",
      "     ‚Ä¢ Cartridge           :     39 points (  0.8%)\n",
      "     ‚Ä¢ Food                :  2,487 points ( 53.4%)\n",
      "     ‚Ä¢ Glass               :  2,500 points ( 53.6%)\n",
      "     ‚Ä¢ Lamp Light          :     48 points (  1.0%)\n",
      "     ‚Ä¢ Metal               :  1,421 points ( 30.5%)\n",
      "     ‚Ä¢ Miscellanous Trash  :     19 points (  0.4%)\n",
      "     ‚Ä¢ Neon                :     56 points (  1.2%)\n",
      "     ‚Ä¢ Paper               :  1,505 points ( 32.3%)\n",
      "     ‚Ä¢ Pharmacy            :    139 points (  3.0%)\n",
      "     ‚Ä¢ Plastic             :  1,407 points ( 30.2%)\n",
      "     ‚Ä¢ Ressourcerie        :      7 points (  0.2%)\n",
      "     ‚Ä¢ Textile             :     78 points (  1.7%)\n",
      "     ‚Ä¢ Tire                :    108 points (  2.3%)\n",
      "     ‚Ä¢ Vegetation          :    857 points ( 18.4%)\n",
      "\n",
      "NEW SPECIALTY TYPES:\n",
      "     ‚Ä¢ Pharmacy            :    139 points (  3.0%)\n",
      "     ‚Ä¢ Tire                :    108 points (  2.3%)\n",
      "     ‚Ä¢ Ressourcerie        :      7 points (  0.2%)\n",
      "\n",
      "D√âCHETTERIE STATISTICS:\n",
      "   ‚Ä¢ Total d√©chetteries: 19\n",
      "   ‚Ä¢ Percentage of total: 0.4%\n",
      "   ‚Ä¢ By Name pattern: 18\n",
      "   ‚Ä¢ D√©chetteries with ressourcerie: 0\n",
      "\n",
      "SAVING UPDATED DATASET\n",
      "CSV saved: 'trash_collection_points_complete.csv' (4661 rows)\n",
      "FIRST 5 ROWS OF CSV FILE:\n",
      "   ID 1: Food Waste - Nantes... (Other)\n",
      "   ID 2: Food Waste - Nantes... (Other)\n",
      "   ID 3: Food Waste - Nantes... (Other)\n",
      "   ID 4: Food Waste - Nantes... (Other)\n",
      "   ID 5: Food Waste - Nantes... (Other)\n",
      "\n",
      "UPLOADING TO BIGQUERY: trash-optimizer-479913.nantes.trash_collection_points_complete\n",
      "BigQuery table created/updated: trash-optimizer-479913.nantes.trash_collection_points_complete\n",
      "   Rows: 4,661\n",
      "   Size: 0.91 MB\n",
      "\n",
      "VERIFICATION:\n",
      "   ‚Ä¢ Total points: 4,661\n",
      "   ‚Ä¢ Textile points: 78\n",
      "   ‚Ä¢ Pharmacy points: 139\n",
      "   ‚Ä¢ Car repair/tire points: 108\n",
      "   ‚Ä¢ Ressourcerie points: 7\n",
      "   ‚Ä¢ D√©chetteries (flagged): 19\n",
      "   ‚Ä¢ D√©chetteries (by name): 15\n",
      "   ‚Ä¢ D√©chetteries with ressourcerie: 0\n",
      "\n",
      "DATA QUALITY CHECK (NULL VALUES):\n",
      "   ‚Ä¢ Total rows: 4,661\n",
      "   ‚Ä¢ Null Is_Miscellanous_Trash_enabled: 0\n",
      "   ‚Ä¢ Null Is_Pharmacy_enabled: 0\n",
      "   ‚Ä¢ Null Is_Tire_enabled: 0\n",
      "   ‚Ä¢ Null Is_Ressourcerie_enabled: 0\n",
      "   ‚úÖ All waste type columns have 0 null values\n",
      "\n",
      "DATA INTEGRATION COMPLETE!\n",
      "\n",
      "Total collection points: 4,661\n",
      "\n",
      "New specialty types added:\n",
      "  ‚Ä¢ Textile: 78\n",
      "  FROM MIXED DATA FILE:\n",
      "    ‚Ä¢ Pharmacy: 139\n",
      "    ‚Ä¢ Car repair/tire: 108\n",
      "    ‚Ä¢ Ressourcerie: 7\n",
      "\n",
      "D√âCHETTERIES:\n",
      "  ‚Ä¢ Total d√©chetteries: 19\n",
      "  ‚Ä¢ With 'D√©ch√®terie' in Name: 18\n",
      "  ‚Ä¢ D√©chetteries with ressourcerie: 0\n",
      "\n",
      "BigQuery table: trash-optimizer-479913.nantes.trash_collection_points_complete\n",
      "Local backup: trash_collection_points_complete.csv\n",
      "\n",
      "FINAL VERIFICATION - CSV CHECK:\n",
      "\n",
      "   NULL VALUE CHECK IN CSV:\n",
      "     ‚úÖ Is_Cardboard_enabled: No null values\n",
      "     ‚úÖ Is_Food_enabled: No null values\n",
      "     ‚úÖ Is_Glass_enabled: No null values\n",
      "     ‚úÖ Is_Metal_enabled: No null values\n",
      "     ‚úÖ Is_Paper_enabled: No null values\n",
      "     ‚úÖ Is_Plastic_enabled: No null values\n",
      "     ‚úÖ Is_Textile_enabled: No null values\n",
      "     ‚úÖ Is_Vegetation_enabled: No null values\n",
      "     ‚úÖ Is_Neon_enabled: No null values\n",
      "     ‚úÖ Is_Cartridge_enabled: No null values\n",
      "     ‚úÖ Is_Lamp_Light_enabled: No null values\n",
      "     ‚úÖ Is_Miscellanous_Trash_enabled: No null values\n",
      "     ‚úÖ Is_Pharmacy_enabled: No null values\n",
      "     ‚úÖ Is_Tire_enabled: No null values\n",
      "     ‚úÖ Is_Ressourcerie_enabled: No null values\n",
      "\n",
      "   ‚úÖ All waste type columns in CSV have 0 null values\n",
      "\n",
      "   D√âCHETTERIES IN CSV:\n",
      "     ‚Ä¢ Total flagged: 19\n",
      "     ‚Ä¢ With 'D√©ch√®terie' in Name: 18\n",
      "     ‚Ä¢ With ressourcerie: 0\n",
      "\n",
      "   CSV COLUMN STRUCTURE:\n",
      "     Total columns: 20\n",
      "     Waste type columns: 15\n",
      "     First few columns: ['ID', 'Name', 'Address', 'Longitude', 'Latitude', 'Is_Cardboard_enabled']\n",
      "     Column names (waste types):\n",
      "       - Is_Cardboard_enabled\n",
      "       - Is_Food_enabled\n",
      "       - Is_Glass_enabled\n",
      "       - Is_Metal_enabled\n",
      "       - Is_Paper_enabled\n",
      "       - Is_Plastic_enabled\n",
      "       - Is_Textile_enabled\n",
      "       - Is_Vegetation_enabled\n",
      "       - Is_Neon_enabled\n",
      "       - Is_Cartridge_enabled\n",
      "       - Is_Lamp_Light_enabled\n",
      "       - Is_Miscellanous_Trash_enabled\n",
      "       - Is_Pharmacy_enabled\n",
      "       - Is_Tire_enabled\n",
      "       - Is_Ressourcerie_enabled\n"
     ]
    }
   ],
   "source": [
    "#TO DO\n",
    "\n",
    "import re\n",
    "\n",
    "# ADD TEXTILE, PHARMACY, GARAGE, AND RESSOURCERIE COLLECTION POINTS TO UNIFIED DATASET\n",
    "\n",
    "print(\"ADDING MULTIPLE COLLECTION POINTS TO UNIFIED DATASET\")\n",
    "\n",
    "# 1. LOAD AND CLEAN TEXTILE DATA\n",
    "print(\"LOADING TEXTILE DATA\")\n",
    "\n",
    "textile_file = \"Textile_relais.csv\"\n",
    "textile_df = pd.DataFrame()  # Initialize as empty\n",
    "textile_final = pd.DataFrame()  # Initialize as empty\n",
    "\n",
    "if os.path.exists(textile_file):\n",
    "    try:\n",
    "        # Read with latin-1 encoding\n",
    "        textile_df = pd.read_csv(textile_file, encoding='latin-1', on_bad_lines='skip')\n",
    "        print(f\"Loaded textile data: {len(textile_df)} rows\")\n",
    "\n",
    "        # Check column names and standardize\n",
    "        column_mapping = {}\n",
    "        for col in textile_df.columns:\n",
    "            col_lower = str(col).lower()\n",
    "            if 'name' in col_lower:\n",
    "                column_mapping[col] = 'Name'\n",
    "            elif 'adresse' in col_lower or 'address' in col_lower:\n",
    "                column_mapping[col] = 'Address'\n",
    "            elif 'latitude' in col_lower or 'lat' in col_lower:\n",
    "                column_mapping[col] = 'Latitude'\n",
    "            elif 'longitude' in col_lower or 'lon' in col_lower or 'long' in col_lower:\n",
    "                column_mapping[col] = 'Longitude'\n",
    "\n",
    "        if column_mapping:\n",
    "            print(f\"   Renaming columns: {column_mapping}\")\n",
    "            textile_df = textile_df.rename(columns=column_mapping)\n",
    "\n",
    "        # Show sample\n",
    "        if len(textile_df) > 0:\n",
    "            print(f\"SAMPLE TEXTILE DATA:\")\n",
    "            sample_cols = [col for col in ['Name', 'Address', 'Latitude', 'Longitude'] if col in textile_df.columns]\n",
    "            if sample_cols:\n",
    "                print(textile_df[sample_cols].head(3).to_string(index=False))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading textile data: {e}\")\n",
    "        textile_df = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"Textile file not found: {textile_file}\")\n",
    "\n",
    "# 2. LOAD AND CLEAN PHARMACY/GARAGE/RESSOURCERIE DATA\n",
    "print(\"\\nLOADING PHARMACY/GARAGE/RESSOURCERIE DATA\")\n",
    "\n",
    "mixed_file = \"pharmacies_garages_ressourceries_nantes.csv\"\n",
    "mixed_df = pd.DataFrame()\n",
    "mixed_final = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(mixed_file):\n",
    "    try:\n",
    "        mixed_df = pd.read_csv(mixed_file, encoding='utf-8')\n",
    "        print(f\"Loaded mixed data: {len(mixed_df)} rows\")\n",
    "        print(f\"   Columns: {mixed_df.columns.tolist()}\")\n",
    "\n",
    "        # Check column names\n",
    "        column_mapping = {}\n",
    "        for col in mixed_df.columns:\n",
    "            col_lower = str(col).lower()\n",
    "            if 'name' in col_lower or 'nom' in col_lower:\n",
    "                column_mapping[col] = 'Name'\n",
    "            elif 'type' in col_lower or 'categorie' in col_lower:\n",
    "                column_mapping[col] = 'Type'\n",
    "            elif 'latitude' in col_lower or 'lat' in col_lower:\n",
    "                column_mapping[col] = 'Latitude'\n",
    "            elif 'longitude' in col_lower or 'lon' in col_lower or 'long' in col_lower:\n",
    "                column_mapping[col] = 'Longitude'\n",
    "            elif 'adresse' in col_lower or 'address' in col_lower:\n",
    "                column_mapping[col] = 'Address'\n",
    "\n",
    "        if column_mapping:\n",
    "            print(f\"   Renaming columns: {column_mapping}\")\n",
    "            mixed_df = mixed_df.rename(columns=column_mapping)\n",
    "\n",
    "        # Show unique types\n",
    "        if 'Type' in mixed_df.columns:\n",
    "            unique_types = mixed_df['Type'].unique()\n",
    "            print(f\"   Unique types found: {list(unique_types)}\")\n",
    "            for type_val in unique_types:\n",
    "                count = (mixed_df['Type'] == type_val).sum()\n",
    "                print(f\"     ‚Ä¢ {type_val}: {count} points\")\n",
    "\n",
    "        # Show sample\n",
    "        if len(mixed_df) > 0:\n",
    "            print(f\"SAMPLE MIXED DATA:\")\n",
    "            sample_cols = [col for col in ['Name', 'Type', 'Latitude', 'Longitude', 'Address'] if col in mixed_df.columns]\n",
    "            if sample_cols:\n",
    "                print(mixed_df[sample_cols].head(5).to_string(index=False))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading mixed data: {e}\")\n",
    "        mixed_df = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"Mixed file not found: {mixed_file}\")\n",
    "\n",
    "# 3. PROCESS TEXTILE DATA\n",
    "if not textile_df.empty:\n",
    "    print(\"PROCESSING TEXTILE DATA\")\n",
    "\n",
    "    textile_clean = textile_df.copy()\n",
    "\n",
    "    # Clean text fields\n",
    "    if 'Name' in textile_clean.columns:\n",
    "        textile_clean['Name'] = textile_clean['Name'].fillna('Textile Collection Point').astype(str).str.strip()\n",
    "    else:\n",
    "        textile_clean['Name'] = 'Textile Collection Point'\n",
    "\n",
    "    if 'Address' in textile_clean.columns:\n",
    "        textile_clean['Address'] = textile_clean['Address'].fillna('').astype(str).str.strip()\n",
    "    else:\n",
    "        textile_clean['Address'] = ''\n",
    "\n",
    "    # Clean coordinates\n",
    "    if 'Latitude' in textile_clean.columns:\n",
    "        textile_clean['Latitude'] = textile_clean['Latitude'].astype(str).str.replace('[^0-9.-]', '', regex=True)\n",
    "        textile_clean['Latitude'] = pd.to_numeric(textile_clean['Latitude'], errors='coerce')\n",
    "\n",
    "    if 'Longitude' in textile_clean.columns:\n",
    "        textile_clean['Longitude'] = textile_clean['Longitude'].astype(str).str.replace('[^0-9.-]', '', regex=True)\n",
    "        textile_clean['Longitude'] = pd.to_numeric(textile_clean['Longitude'], errors='coerce')\n",
    "\n",
    "    # Filter out points without valid coordinates\n",
    "    if 'Latitude' in textile_clean.columns and 'Longitude' in textile_clean.columns:\n",
    "        textile_clean = textile_clean[textile_clean['Latitude'].notna() & textile_clean['Longitude'].notna()].copy()\n",
    "\n",
    "    # Add waste type capabilities (TEXTILE ONLY) - WITH CORRECT COLUMN NAMES\n",
    "    textile_clean['Is_Textile_enabled'] = 1\n",
    "    textile_clean['Is_Cardboard_enabled'] = 0\n",
    "    textile_clean['Is_Food_enabled'] = 0\n",
    "    textile_clean['Is_Glass_enabled'] = 0\n",
    "    textile_clean['Is_Metal_enabled'] = 0\n",
    "    textile_clean['Is_Paper_enabled'] = 0\n",
    "    textile_clean['Is_Plastic_enabled'] = 0\n",
    "    textile_clean['Is_Vegetation_enabled'] = 0\n",
    "    textile_clean['Is_Neon_enabled'] = 0\n",
    "    textile_clean['Is_Cartridge_enabled'] = 0\n",
    "    textile_clean['Is_Lamp_Light_enabled'] = 0\n",
    "    textile_clean['Is_Miscellanous_Trash_enabled'] = 0\n",
    "    textile_clean['Is_Pharmacy_enabled'] = 0\n",
    "    textile_clean['Is_Tire_enabled'] = 0\n",
    "    textile_clean['Is_Ressourcerie_enabled'] = 0\n",
    "\n",
    "    textile_final = textile_clean\n",
    "    print(f\"Processed {len(textile_final)} textile collection points\")\n",
    "else:\n",
    "    print(\"No textile data to process\")\n",
    "\n",
    "# 4. PROCESS MIXED DATA (PHARMACY/GARAGE/RESSOURCERIE)\n",
    "if not mixed_df.empty:\n",
    "    print(\"PROCESSING MIXED DATA\")\n",
    "\n",
    "    mixed_clean = mixed_df.copy()\n",
    "\n",
    "    # Clean Name\n",
    "    if 'Name' in mixed_clean.columns:\n",
    "        mixed_clean['Name'] = mixed_clean['Name'].fillna('').astype(str).str.strip()\n",
    "    else:\n",
    "        mixed_clean['Name'] = ''\n",
    "\n",
    "    # Clean Type (normalize to lowercase)\n",
    "    if 'Type' in mixed_clean.columns:\n",
    "        mixed_clean['Type'] = mixed_clean['Type'].fillna('').astype(str).str.lower().str.strip()\n",
    "\n",
    "    # Clean Address if exists\n",
    "    if 'Address' not in mixed_clean.columns:\n",
    "        mixed_clean['Address'] = ''\n",
    "\n",
    "    # Clean coordinates\n",
    "    if 'Latitude' in mixed_clean.columns:\n",
    "        mixed_clean['Latitude'] = pd.to_numeric(mixed_clean['Latitude'], errors='coerce')\n",
    "\n",
    "    if 'Longitude' in mixed_clean.columns:\n",
    "        mixed_clean['Longitude'] = pd.to_numeric(mixed_clean['Longitude'], errors='coerce')\n",
    "\n",
    "    # Filter out points without valid coordinates\n",
    "    if 'Latitude' in mixed_clean.columns and 'Longitude' in mixed_clean.columns:\n",
    "        mixed_clean = mixed_clean[mixed_clean['Latitude'].notna() & mixed_clean['Longitude'].notna()].copy()\n",
    "\n",
    "    # Initialize all waste type columns to 0 - WITH CORRECT NAMES\n",
    "    waste_columns = [\n",
    "        'Is_Cardboard_enabled', 'Is_Food_enabled', 'Is_Glass_enabled',\n",
    "        'Is_Metal_enabled', 'Is_Paper_enabled', 'Is_Plastic_enabled',\n",
    "        'Is_Textile_enabled', 'Is_Vegetation_enabled', 'Is_Neon_enabled',\n",
    "        'Is_Cartridge_enabled', 'Is_Lamp_Light_enabled',\n",
    "        'Is_Miscellanous_Trash_enabled', 'Is_Pharmacy_enabled',\n",
    "        'Is_Tire_enabled', 'Is_Ressourcerie_enabled'\n",
    "    ]\n",
    "\n",
    "    for col in waste_columns:\n",
    "        mixed_clean[col] = 0\n",
    "\n",
    "    # Set appropriate columns based on type\n",
    "    if 'Type' in mixed_clean.columns:\n",
    "        # Pharmacy points\n",
    "        pharmacy_mask = mixed_clean['Type'].str.contains('pharmacy|pharmacie', case=False, na=False)\n",
    "        mixed_clean.loc[pharmacy_mask, 'Is_Pharmacy_enabled'] = 1\n",
    "        print(f\"   Pharmacy points: {pharmacy_mask.sum()}\")\n",
    "\n",
    "        # Car repair/garage points (tire)\n",
    "        tire_mask = mixed_clean['Type'].str.contains('car_repair|garage|repair|tire|pneu', case=False, na=False)\n",
    "        mixed_clean.loc[tire_mask, 'Is_Tire_enabled'] = 1\n",
    "        print(f\"   Car repair/tire points: {tire_mask.sum()}\")\n",
    "\n",
    "        # Ressourcerie points\n",
    "        ressourcerie_mask = mixed_clean['Type'].str.contains('ressourcerie|recup|recycl', case=False, na=False)\n",
    "        mixed_clean.loc[ressourcerie_mask, 'Is_Ressourcerie_enabled'] = 1\n",
    "        print(f\"   Ressourcerie points: {ressourcerie_mask.sum()}\")\n",
    "\n",
    "        # Check for any unclassified types\n",
    "        unclassified = mixed_clean[~pharmacy_mask & ~tire_mask & ~ressourcerie_mask]\n",
    "        if len(unclassified) > 0:\n",
    "            print(f\"Unclassified types: {len(unclassified)} points\")\n",
    "            print(f\"Types: {unclassified['Type'].unique()}\")\n",
    "\n",
    "    # CRITICAL FIX: Assign to mixed_final\n",
    "    mixed_final = mixed_clean\n",
    "    print(f\"Processed {len(mixed_final)} mixed collection points\")\n",
    "\n",
    "    # Show breakdown\n",
    "    print(f\"MIXED DATA BREAKDOWN:\")\n",
    "    if 'Is_Pharmacy_enabled' in mixed_final.columns:\n",
    "        print(f\"   ‚Ä¢ Pharmacy: {mixed_final['Is_Pharmacy_enabled'].sum()}\")\n",
    "    if 'Is_Tire_enabled' in mixed_final.columns:\n",
    "        print(f\"   ‚Ä¢ Car repair/tire: {mixed_final['Is_Tire_enabled'].sum()}\")\n",
    "    if 'Is_Ressourcerie_enabled' in mixed_final.columns:\n",
    "        print(f\"   ‚Ä¢ Ressourcerie: {mixed_final['Is_Ressourcerie_enabled'].sum()}\")\n",
    "\n",
    "    # Show sample of mixed data\n",
    "    print(f\"SAMPLE OF PROCESSED MIXED DATA:\")\n",
    "    if len(mixed_final) > 0:\n",
    "        sample = mixed_final.head(5)\n",
    "        for i, row in sample.iterrows():\n",
    "            name = row.get('Name', 'Unnamed')\n",
    "            point_type = row.get('Type', 'unknown')\n",
    "            print(f\"   {i+1}. {name} ({point_type})\")\n",
    "else:\n",
    "    print(\"No mixed data to process\")\n",
    "    mixed_final = pd.DataFrame()\n",
    "\n",
    "# 5. LOAD EXISTING UNIFIED TABLE\n",
    "print(\"\\nLOADING EXISTING UNIFIED TABLE\")\n",
    "\n",
    "existing_table = f\"{PROJECT}.{DATASET}.trash_collection_points\"\n",
    "\n",
    "try:\n",
    "    # Query ALL existing data\n",
    "    existing_query = f\"\"\"\n",
    "    SELECT * FROM `{existing_table}`\n",
    "    \"\"\"\n",
    "    existing_df = client.query(existing_query).to_dataframe()\n",
    "    print(f\"‚úÖ Loaded existing table: {len(existing_df)} rows\")\n",
    "\n",
    "    if len(existing_df) == 0:\n",
    "        print(\"‚ö†Ô∏è WARNING: Existing table is empty! Check table name.\")\n",
    "\n",
    "    # Show current ID range\n",
    "    if 'ID' in existing_df.columns:\n",
    "        print(f\"   Current ID range: {existing_df['ID'].min()} to {existing_df['ID'].max()}\")\n",
    "\n",
    "    # FIX 1: Check and add new columns if they don't exist\n",
    "    new_columns = ['Is_Pharmacy_enabled', 'Is_Tire_enabled', 'Is_Ressourcerie_enabled']\n",
    "    for col in new_columns:\n",
    "        if col not in existing_df.columns:\n",
    "            existing_df[col] = 0\n",
    "            print(f\"   Added missing waste type column: {col}\")\n",
    "\n",
    "    # FIX 2: SET Is_Miscellanous_Trash_enabled = 1 FOR ALL D√âCH√àTERIES\n",
    "    print(f\"\\n   IDENTIFYING D√âCH√àTERIES IN NAME COLUMN...\")\n",
    "\n",
    "    # Check if Is_Miscellanous_Trash_enabled column exists\n",
    "    if 'Is_Miscellanous_Trash_enabled' not in existing_df.columns:\n",
    "        existing_df['Is_Miscellanous_Trash_enabled'] = 0\n",
    "        print(f\"   Created missing column: Is_Miscellanous_Trash_enabled\")\n",
    "\n",
    "    # Check for d√©chetteries in the Name column\n",
    "    d√©chetterie_patterns = [\n",
    "        'D√©ch√®terie', 'D√©chetterie', 'DECHETTERIE', 'dechetterie',\n",
    "        'D√©ch√®teries', 'D√©chetteries', 'DECHETTERIES', 'dechetteries',\n",
    "        'Recycling Center', 'RECYCLING CENTER', 'recycling center'\n",
    "    ]\n",
    "\n",
    "    # Create a pattern for case-insensitive matching\n",
    "    pattern = '|'.join([re.escape(pattern) for pattern in d√©chetterie_patterns])\n",
    "\n",
    "    # Find all entries with D√©ch√®terie in Name\n",
    "    d√©chetterie_mask = existing_df['Name'].str.contains(pattern, case=False, na=False)\n",
    "    d√©chetterie_count = d√©chetterie_mask.sum()\n",
    "\n",
    "    print(f\"   Found {d√©chetterie_count} entries with 'D√©ch√®terie' in Name\")\n",
    "\n",
    "    if d√©chetterie_count > 0:\n",
    "        # Show sample of found d√©chetteries\n",
    "        sample_d√©chetteries = existing_df[d√©chetterie_mask].head(5)\n",
    "        print(f\"   Sample d√©chetteries found:\")\n",
    "        for i, row in sample_d√©chetteries.iterrows():\n",
    "            name = row.get('Name', 'Unnamed')[:60]\n",
    "            print(f\"     ‚Ä¢ {name}\")\n",
    "\n",
    "    # Set Is_Miscellanous_Trash_enabled = 1 for all d√©chetteries\n",
    "    existing_df.loc[d√©chetterie_mask, 'Is_Miscellanous_Trash_enabled'] = 1\n",
    "\n",
    "    # Also check for entries already marked as d√©chetteries\n",
    "    already_marked = existing_df[existing_df['Is_Miscellanous_Trash_enabled'] == 1].shape[0]\n",
    "    print(f\"   Total d√©chetteries marked: {already_marked}\")\n",
    "\n",
    "    # Check for d√©chetteries that might also be ressourceries\n",
    "    if 'Is_Ressourcerie_enabled' in existing_df.columns:\n",
    "        ressourcerie_names = ['ressourcerie', 'recup', 'recyclerie', 'RESSOURCERIE', 'RECUP', 'RECYCLERIE']\n",
    "        ressourcerie_pattern = '|'.join([re.escape(name) for name in ressourcerie_names])\n",
    "        ressourcerie_mask = existing_df['Name'].str.contains(ressourcerie_pattern, case=False, na=False)\n",
    "\n",
    "        # Set Is_Ressourcerie_enabled = 1 for d√©chetteries that are also ressourceries\n",
    "        existing_df.loc[d√©chetterie_mask & ressourcerie_mask, 'Is_Ressourcerie_enabled'] = 1\n",
    "        dechetterie_ressourcerie_count = (d√©chetterie_mask & ressourcerie_mask).sum()\n",
    "        print(f\"   {dechetterie_ressourcerie_count} d√©chetteries are also ressourceries\")\n",
    "\n",
    "    # FIX 3: Ensure all waste columns are filled with 0 instead of null\n",
    "    waste_columns = [\n",
    "        'Is_Cardboard_enabled', 'Is_Food_enabled', 'Is_Glass_enabled',\n",
    "        'Is_Metal_enabled', 'Is_Paper_enabled', 'Is_Plastic_enabled',\n",
    "        'Is_Textile_enabled', 'Is_Vegetation_enabled', 'Is_Neon_enabled',\n",
    "        'Is_Cartridge_enabled', 'Is_Lamp_Light_enabled',\n",
    "        'Is_Miscellanous_Trash_enabled', 'Is_Pharmacy_enabled',\n",
    "        'Is_Tire_enabled', 'Is_Ressourcerie_enabled'\n",
    "    ]\n",
    "\n",
    "    for col in waste_columns:\n",
    "        if col in existing_df.columns:\n",
    "            existing_df[col] = existing_df[col].fillna(0).astype(int)\n",
    "\n",
    "    print(f\"   Fixed null values in waste type columns\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading existing table: {e}\")\n",
    "    print(\"Creating new empty dataframe\")\n",
    "    existing_df = pd.DataFrame()\n",
    "\n",
    "# 6. COMBINE ALL DATASETS\n",
    "print(\"\\nCOMBINING ALL DATASETS\")\n",
    "\n",
    "all_new_data = []\n",
    "if not textile_final.empty:\n",
    "    all_new_data.append(textile_final)\n",
    "    print(f\"   Textile data ready: {len(textile_final)} points\")\n",
    "if not mixed_final.empty:\n",
    "    all_new_data.append(mixed_final)\n",
    "    print(f\"   Mixed data ready: {len(mixed_final)} points\")\n",
    "\n",
    "if all_new_data:\n",
    "    # Combine all new data\n",
    "    if len(all_new_data) == 1:\n",
    "        new_data_combined = all_new_data[0]\n",
    "    else:\n",
    "        new_data_combined = pd.concat(all_new_data, ignore_index=True, sort=False)\n",
    "\n",
    "    print(f\"\\n   Total new data to add: {len(new_data_combined)} points\")\n",
    "\n",
    "    # Get the next available ID starting from existing max + 1\n",
    "    if not existing_df.empty and 'ID' in existing_df.columns:\n",
    "        next_id = int(existing_df['ID'].max()) + 1\n",
    "    else:\n",
    "        next_id = 1\n",
    "\n",
    "    print(f\"   Next available ID: {next_id}\")\n",
    "\n",
    "    # Assign IDs to new points\n",
    "    new_data_combined['ID'] = range(next_id, next_id + len(new_data_combined))\n",
    "    print(f\"   Assigned IDs {next_id} to {next_id + len(new_data_combined) - 1} to new points\")\n",
    "\n",
    "    # Ensure both dataframes have same columns\n",
    "    all_columns = set(existing_df.columns.tolist() if not existing_df.empty else [])\n",
    "    all_columns.update(new_data_combined.columns.tolist())\n",
    "\n",
    "    # Remove metadata columns if they exist in new data\n",
    "    metadata_columns_to_remove = ['City', 'Source_Files', 'Original_Waste_Types', 'Type']\n",
    "    for col in metadata_columns_to_remove:\n",
    "        if col in new_data_combined.columns:\n",
    "            new_data_combined = new_data_combined.drop(columns=[col])\n",
    "            print(f\"   Removed metadata column from new data: {col}\")\n",
    "\n",
    "    # Add missing columns to each dataframe\n",
    "    for col in all_columns:\n",
    "        if col not in existing_df.columns and not existing_df.empty:\n",
    "            existing_df[col] = None\n",
    "        if col not in new_data_combined.columns:\n",
    "            new_data_combined[col] = None\n",
    "\n",
    "    # Define column order (ONLY essential columns)\n",
    "    common_columns = [\n",
    "        'ID', 'Name', 'Address', 'Longitude', 'Latitude',\n",
    "        'Is_Cardboard_enabled', 'Is_Food_enabled', 'Is_Glass_enabled',\n",
    "        'Is_Metal_enabled', 'Is_Paper_enabled', 'Is_Plastic_enabled',\n",
    "        'Is_Textile_enabled', 'Is_Vegetation_enabled', 'Is_Neon_enabled',\n",
    "        'Is_Cartridge_enabled', 'Is_Lamp_Light_enabled',\n",
    "        'Is_Miscellanous_Trash_enabled', 'Is_Pharmacy_enabled',\n",
    "        'Is_Tire_enabled', 'Is_Ressourcerie_enabled'\n",
    "    ]\n",
    "\n",
    "    # Keep only columns that exist\n",
    "    existing_cols = [col for col in common_columns if col in existing_df.columns or col in new_data_combined.columns]\n",
    "\n",
    "    # Ensure all waste columns are integers (0 or 1)\n",
    "    for col in existing_cols:\n",
    "        if col.startswith('Is_') and col.endswith('_enabled'):\n",
    "            if col in existing_df.columns:\n",
    "                existing_df[col] = existing_df[col].fillna(0).astype(int)\n",
    "            if col in new_data_combined.columns:\n",
    "                new_data_combined[col] = new_data_combined[col].fillna(0).astype(int)\n",
    "\n",
    "    # Combine - Make sure we keep ALL existing data\n",
    "    print(f\"\\nüîó COMBINING DATA:\")\n",
    "    print(f\"   Existing data shape: {existing_df.shape}\")\n",
    "    print(f\"   New data shape: {new_data_combined.shape}\")\n",
    "\n",
    "    if existing_df.empty:\n",
    "        combined_df = new_data_combined[existing_cols].copy()\n",
    "        print(f\"   Combined: New data only (no existing data)\")\n",
    "    else:\n",
    "        combined_df = pd.concat([\n",
    "            existing_df[existing_cols],\n",
    "            new_data_combined[existing_cols]\n",
    "        ], ignore_index=True, sort=False)\n",
    "        print(f\"   Combined: Existing + New data\")\n",
    "\n",
    "    print(f\"‚úÖ Combined dataset: {len(combined_df)} total points\")\n",
    "    print(f\"   - Existing points: {len(existing_df) if not existing_df.empty else 0}\")\n",
    "    print(f\"   - New points: {len(new_data_combined)}\")\n",
    "\n",
    "    # Show what was added\n",
    "    print(f\"\\nüìä NEW DATA ADDED:\")\n",
    "    if not textile_final.empty:\n",
    "        print(f\"   ‚Ä¢ Textile points: {len(textile_final)}\")\n",
    "    if not mixed_final.empty:\n",
    "        print(f\"   ‚Ä¢ Pharmacy points: {mixed_final['Is_Pharmacy_enabled'].sum()}\")\n",
    "        print(f\"   ‚Ä¢ Car repair/tire points: {mixed_final['Is_Tire_enabled'].sum()}\")\n",
    "        print(f\"   ‚Ä¢ Ressourcerie points: {mixed_final['Is_Ressourcerie_enabled'].sum()}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No new data to add\")\n",
    "    combined_df = existing_df\n",
    "\n",
    "# 7. UPDATE STATISTICS\n",
    "if not combined_df.empty:\n",
    "    print(\"\\nUPDATED STATISTICS:\")\n",
    "    print(f\"   Total collection points: {len(combined_df):,}\")\n",
    "\n",
    "    if len(combined_df) < 1000:\n",
    "        print(f\"   ‚ö†Ô∏è WARNING: Only {len(combined_df)} points - data may be lost!\")\n",
    "\n",
    "    # Count by waste type\n",
    "    waste_stats = {}\n",
    "    for col in combined_df.columns:\n",
    "        if col.startswith('Is_') and col.endswith('_enabled'):\n",
    "            count = int(combined_df[col].sum())\n",
    "            if count > 0:\n",
    "                waste_name = col.replace('Is_', '').replace('_enabled', '').replace('_', ' ').title()\n",
    "                waste_stats[waste_name] = count\n",
    "\n",
    "    print(f\"\\n   Waste/Specialty type coverage:\")\n",
    "    for waste, count in sorted(waste_stats.items()):\n",
    "        percentage = (count / len(combined_df)) * 100\n",
    "        print(f\"     ‚Ä¢ {waste:20s}: {count:6,d} points ({percentage:5.1f}%)\")\n",
    "\n",
    "    # Special statistics for new types\n",
    "    print(f\"\\nNEW SPECIALTY TYPES:\")\n",
    "    new_types = ['Pharmacy', 'Tire', 'Ressourcerie']\n",
    "    for type_name in new_types:\n",
    "        col_name = f'Is_{type_name}_enabled'\n",
    "        if col_name in combined_df.columns:\n",
    "            count = int(combined_df[col_name].sum())\n",
    "            if count > 0:\n",
    "                percentage = (count / len(combined_df)) * 100\n",
    "                print(f\"     ‚Ä¢ {type_name:20s}: {count:6,d} points ({percentage:5.1f}%)\")\n",
    "\n",
    "    # Show d√©chetterie statistics\n",
    "    if 'Is_Miscellanous_Trash_enabled' in combined_df.columns:\n",
    "        dechetterie_count = int(combined_df['Is_Miscellanous_Trash_enabled'].sum())\n",
    "        print(f\"\\nD√âCHETTERIE STATISTICS:\")\n",
    "        print(f\"   ‚Ä¢ Total d√©chetteries: {dechetterie_count:,}\")\n",
    "        print(f\"   ‚Ä¢ Percentage of total: {(dechetterie_count/len(combined_df))*100:.1f}%\")\n",
    "\n",
    "        # Also check by Name pattern for verification\n",
    "        d√©chetterie_patterns = ['D√©ch√®terie', 'D√©chetterie', 'DECHETTERIE', 'dechetterie']\n",
    "        pattern = '|'.join([re.escape(pattern) for pattern in d√©chetterie_patterns])\n",
    "        name_based_count = combined_df['Name'].str.contains(pattern, case=False, na=False).sum()\n",
    "        print(f\"   ‚Ä¢ By Name pattern: {name_based_count:,}\")\n",
    "\n",
    "        if 'Is_Ressourcerie_enabled' in combined_df.columns:\n",
    "            dechetterie_ressourcerie = combined_df[\n",
    "                (combined_df['Is_Miscellanous_Trash_enabled'] == 1) &\n",
    "                (combined_df['Is_Ressourcerie_enabled'] == 1)\n",
    "            ].shape[0]\n",
    "            print(f\"   ‚Ä¢ D√©chetteries with ressourcerie: {dechetterie_ressourcerie:,}\")\n",
    "\n",
    "# 8. SAVE TO BIGQUERY\n",
    "print(\"\\nSAVING UPDATED DATASET\")\n",
    "\n",
    "# Use the same table name\n",
    "new_table = f\"{PROJECT}.{DATASET}.trash_collection_points_complete\"\n",
    "output_csv = \"trash_collection_points_complete.csv\"\n",
    "\n",
    "if not combined_df.empty:\n",
    "    # Sort by ID\n",
    "    combined_df = combined_df.sort_values('ID').reset_index(drop=True)\n",
    "\n",
    "    # Ensure all waste columns are integers\n",
    "    waste_columns = [col for col in combined_df.columns if col.startswith('Is_') and col.endswith('_enabled')]\n",
    "    for col in waste_columns:\n",
    "        combined_df[col] = combined_df[col].fillna(0).astype(int)\n",
    "\n",
    "    # Save to CSV\n",
    "    combined_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"CSV saved: '{output_csv}' ({len(combined_df)} rows)\")\n",
    "\n",
    "    # Show sample of the CSV file\n",
    "    print(f\"FIRST 5 ROWS OF CSV FILE:\")\n",
    "    try:\n",
    "        saved_csv = pd.read_csv(output_csv, nrows=5)\n",
    "\n",
    "        # Show all rows\n",
    "        for i, row in saved_csv.head(5).iterrows():\n",
    "            name = row.get('Name', 'Unnamed')[:50]\n",
    "            point_id = row.get('ID', 'N/A')\n",
    "\n",
    "            # Determine point type\n",
    "            point_type = \"Other\"\n",
    "            if row.get('Is_Pharmacy_enabled', 0) == 1:\n",
    "                point_type = \"Pharmacy\"\n",
    "            elif row.get('Is_Tire_enabled', 0) == 1:\n",
    "                point_type = \"Car Repair/Tire\"\n",
    "            elif row.get('Is_Ressourcerie_enabled', 0) == 1:\n",
    "                point_type = \"Ressourcerie\"\n",
    "            elif row.get('Is_Textile_enabled', 0) == 1:\n",
    "                point_type = \"Textile\"\n",
    "            elif row.get('Is_Miscellanous_Trash_enabled', 0) == 1:\n",
    "                point_type = \"D√©chetterie\"\n",
    "\n",
    "            print(f\"   ID {point_id}: {name}... ({point_type})\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Could not read CSV for sample: {e}\")\n",
    "\n",
    "    # Upload to BigQuery\n",
    "    print(f\"\\nUPLOADING TO BIGQUERY: {new_table}\")\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "        autodetect=True,\n",
    "        max_bad_records=100\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(combined_df, new_table, job_config=job_config)\n",
    "        job.result()\n",
    "\n",
    "        table = client.get_table(new_table)\n",
    "        print(f\"BigQuery table created/updated: {new_table}\")\n",
    "        print(f\"   Rows: {table.num_rows:,}\")\n",
    "        print(f\"   Size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Verification query\n",
    "        verify_query = f\"\"\"\n",
    "        SELECT\n",
    "          COUNT(*) as total_points,\n",
    "          SUM(Is_Textile_enabled) as textile_points,\n",
    "          SUM(Is_Pharmacy_enabled) as pharmacy_points,\n",
    "          SUM(Is_Tire_enabled) as tire_points,\n",
    "          SUM(Is_Ressourcerie_enabled) as ressourcerie_points,\n",
    "          SUM(Is_Miscellanous_Trash_enabled) as dechetteries,\n",
    "          SUM(CASE WHEN Is_Miscellanous_Trash_enabled = 1 AND Is_Ressourcerie_enabled = 1 THEN 1 ELSE 0 END) as dechetterie_ressourceries,\n",
    "          -- Also count by Name pattern for verification\n",
    "          SUM(CASE WHEN UPPER(Name) LIKE '%D√âCH√àTERIE%' OR UPPER(Name) LIKE '%DECHETTERIE%' THEN 1 ELSE 0 END) as name_based_dechetteries\n",
    "        FROM `{new_table}`\n",
    "        \"\"\"\n",
    "\n",
    "        result = client.query(verify_query).to_dataframe().iloc[0]\n",
    "        print(f\"\\nVERIFICATION:\")\n",
    "        print(f\"   ‚Ä¢ Total points: {result['total_points']:,}\")\n",
    "        print(f\"   ‚Ä¢ Textile points: {result['textile_points']:,}\")\n",
    "        print(f\"   ‚Ä¢ Pharmacy points: {result['pharmacy_points']:,}\")\n",
    "        print(f\"   ‚Ä¢ Car repair/tire points: {result['tire_points']:,}\")\n",
    "        print(f\"   ‚Ä¢ Ressourcerie points: {result['ressourcerie_points']:,}\")\n",
    "        print(f\"   ‚Ä¢ D√©chetteries (flagged): {result['dechetteries']:,}\")\n",
    "        print(f\"   ‚Ä¢ D√©chetteries (by name): {result['name_based_dechetteries']:,}\")\n",
    "        print(f\"   ‚Ä¢ D√©chetteries with ressourcerie: {result['dechetterie_ressourceries']:,}\")\n",
    "\n",
    "        # Check data quality\n",
    "        quality_query = f\"\"\"\n",
    "        SELECT\n",
    "          COUNT(*) as total_rows,\n",
    "          SUM(CASE WHEN Is_Miscellanous_Trash_enabled IS NULL THEN 1 ELSE 0 END) as null_misc_trash,\n",
    "          SUM(CASE WHEN Is_Pharmacy_enabled IS NULL THEN 1 ELSE 0 END) as null_pharmacy,\n",
    "          SUM(CASE WHEN Is_Tire_enabled IS NULL THEN 1 ELSE 0 END) as null_tire,\n",
    "          SUM(CASE WHEN Is_Ressourcerie_enabled IS NULL THEN 1 ELSE 0 END) as null_ressourcerie\n",
    "        FROM `{new_table}`\n",
    "        \"\"\"\n",
    "\n",
    "        quality_result = client.query(quality_query).to_dataframe().iloc[0]\n",
    "        print(f\"\\nDATA QUALITY CHECK (NULL VALUES):\")\n",
    "        print(f\"   ‚Ä¢ Total rows: {quality_result['total_rows']:,}\")\n",
    "        print(f\"   ‚Ä¢ Null Is_Miscellanous_Trash_enabled: {quality_result['null_misc_trash']:,}\")\n",
    "        print(f\"   ‚Ä¢ Null Is_Pharmacy_enabled: {quality_result['null_pharmacy']:,}\")\n",
    "        print(f\"   ‚Ä¢ Null Is_Tire_enabled: {quality_result['null_tire']:,}\")\n",
    "        print(f\"   ‚Ä¢ Null Is_Ressourcerie_enabled: {quality_result['null_ressourcerie']:,}\")\n",
    "\n",
    "        if (quality_result['null_misc_trash'] == 0 and\n",
    "            quality_result['null_pharmacy'] == 0 and\n",
    "            quality_result['null_tire'] == 0 and\n",
    "            quality_result['null_ressourcerie'] == 0):\n",
    "            print(f\"   ‚úÖ All waste type columns have 0 null values\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Some waste type columns have null values\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"BigQuery upload failed: {e}\")\n",
    "\n",
    "        # Try alternative\n",
    "        try:\n",
    "            print(\"\\nTrying alternative CSV upload...\")\n",
    "            import pandas_gbq\n",
    "            pandas_gbq.to_gbq(\n",
    "                combined_df,\n",
    "                destination_table=new_table,\n",
    "                project_id=PROJECT,\n",
    "                if_exists='replace',\n",
    "                progress_bar=True\n",
    "            )\n",
    "            print(\"Upload successful via pandas_gbq!\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Alternative failed: {e2}\")\n",
    "else:\n",
    "    print(\"No data to save!\")\n",
    "\n",
    "print(\"\\nDATA INTEGRATION COMPLETE!\")\n",
    "\n",
    "if not combined_df.empty:\n",
    "    print(f\"\\nTotal collection points: {len(combined_df):,}\")\n",
    "\n",
    "    if len(combined_df) < 4000:\n",
    "        print(f\"‚ö†Ô∏è WARNING: Expected ~4000+ points, but got only {len(combined_df)}\")\n",
    "        print(f\"   Check if existing data was loaded correctly from: {existing_table}\")\n",
    "\n",
    "    print(f\"\\nNew specialty types added:\")\n",
    "    if 'Is_Textile_enabled' in combined_df.columns:\n",
    "        textile_count = int(combined_df['Is_Textile_enabled'].sum())\n",
    "        print(f\"  ‚Ä¢ Textile: {textile_count:,}\")\n",
    "\n",
    "    if not mixed_final.empty:\n",
    "        print(f\"  FROM MIXED DATA FILE:\")\n",
    "        if 'Is_Pharmacy_enabled' in mixed_final.columns:\n",
    "            pharmacy_count = int(mixed_final['Is_Pharmacy_enabled'].sum())\n",
    "            print(f\"    ‚Ä¢ Pharmacy: {pharmacy_count:,}\")\n",
    "        if 'Is_Tire_enabled' in mixed_final.columns:\n",
    "            tire_count = int(mixed_final['Is_Tire_enabled'].sum())\n",
    "            print(f\"    ‚Ä¢ Car repair/tire: {tire_count:,}\")\n",
    "        if 'Is_Ressourcerie_enabled' in mixed_final.columns:\n",
    "            ressourcerie_count = int(mixed_final['Is_Ressourcerie_enabled'].sum())\n",
    "            print(f\"    ‚Ä¢ Ressourcerie: {ressourcerie_count:,}\")\n",
    "\n",
    "    # Show d√©chetterie information\n",
    "    if 'Is_Miscellanous_Trash_enabled' in combined_df.columns:\n",
    "        dechetterie_count = int(combined_df['Is_Miscellanous_Trash_enabled'].sum())\n",
    "        print(f\"\\nD√âCHETTERIES:\")\n",
    "        print(f\"  ‚Ä¢ Total d√©chetteries: {dechetterie_count:,}\")\n",
    "\n",
    "        # Also show count by name pattern\n",
    "        d√©chetterie_patterns = ['D√©ch√®terie', 'D√©chetterie', 'DECHETTERIE', 'dechetterie']\n",
    "        pattern = '|'.join([re.escape(pattern) for pattern in d√©chetterie_patterns])\n",
    "        name_based_count = combined_df['Name'].str.contains(pattern, case=False, na=False).sum()\n",
    "        print(f\"  ‚Ä¢ With 'D√©ch√®terie' in Name: {name_based_count:,}\")\n",
    "\n",
    "        if 'Is_Ressourcerie_enabled' in combined_df.columns:\n",
    "            dechetterie_ressourcerie = combined_df[\n",
    "                (combined_df['Is_Miscellanous_Trash_enabled'] == 1) &\n",
    "                (combined_df['Is_Ressourcerie_enabled'] == 1)\n",
    "            ].shape[0]\n",
    "            print(f\"  ‚Ä¢ D√©chetteries with ressourcerie: {dechetterie_ressourcerie:,}\")\n",
    "\n",
    "    print(f\"\\nBigQuery table: {new_table}\")\n",
    "    print(f\"Local backup: {output_csv}\")\n",
    "\n",
    "    # Final verification\n",
    "    print(f\"\\nFINAL VERIFICATION - CSV CHECK:\")\n",
    "    try:\n",
    "        final_check = pd.read_csv(output_csv)\n",
    "\n",
    "        # Check for null values in waste columns\n",
    "        print(f\"\\n   NULL VALUE CHECK IN CSV:\")\n",
    "        waste_cols = [col for col in final_check.columns if col.startswith('Is_') and col.endswith('_enabled')]\n",
    "        all_good = True\n",
    "\n",
    "        for col in waste_cols:\n",
    "            null_count = final_check[col].isna().sum()\n",
    "            if null_count > 0:\n",
    "                print(f\"     ‚ùå {col}: {null_count:,} null values\")\n",
    "                all_good = False\n",
    "            else:\n",
    "                print(f\"     ‚úÖ {col}: No null values\")\n",
    "\n",
    "        if all_good:\n",
    "            print(f\"\\n   ‚úÖ All waste type columns in CSV have 0 null values\")\n",
    "        else:\n",
    "            print(f\"\\n   ‚ö†Ô∏è Some waste type columns in CSV have null values\")\n",
    "\n",
    "        # Show d√©chetterie count in CSV\n",
    "        if 'Is_Miscellanous_Trash_enabled' in final_check.columns:\n",
    "            csv_dechetteries = final_check['Is_Miscellanous_Trash_enabled'].sum()\n",
    "            print(f\"\\n   D√âCHETTERIES IN CSV:\")\n",
    "            print(f\"     ‚Ä¢ Total flagged: {int(csv_dechetteries):,}\")\n",
    "\n",
    "            # Also check by name\n",
    "            d√©chetterie_patterns = ['D√©ch√®terie', 'D√©chetterie', 'DECHETTERIE', 'dechetterie']\n",
    "            pattern = '|'.join([re.escape(pattern) for pattern in d√©chetterie_patterns])\n",
    "            name_based_count = final_check['Name'].str.contains(pattern, case=False, na=False).sum()\n",
    "            print(f\"     ‚Ä¢ With 'D√©ch√®terie' in Name: {name_based_count:,}\")\n",
    "\n",
    "            if 'Is_Ressourcerie_enabled' in final_check.columns:\n",
    "                csv_dechetterie_ressourcerie = final_check[\n",
    "                    (final_check['Is_Miscellanous_Trash_enabled'] == 1) &\n",
    "                    (final_check['Is_Ressourcerie_enabled'] == 1)\n",
    "                ].shape[0]\n",
    "                print(f\"     ‚Ä¢ With ressourcerie: {csv_dechetterie_ressourcerie:,}\")\n",
    "\n",
    "        # Show column structure\n",
    "        print(f\"\\n   CSV COLUMN STRUCTURE:\")\n",
    "        print(f\"     Total columns: {len(final_check.columns)}\")\n",
    "        print(f\"     Waste type columns: {len(waste_cols)}\")\n",
    "        print(f\"     First few columns: {final_check.columns[:6].tolist()}\")\n",
    "        print(f\"     Column names (waste types):\")\n",
    "        for col in waste_cols:\n",
    "            print(f\"       - {col}\")\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"   Error checking CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91611af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DETAILED ANALYSIS FOR TRASH COLLECTION POINTS\n",
      "============================================================\n",
      "\n",
      "1. FACILITY TYPE BREAKDOWN:\n",
      "----------------------------------------\n",
      "   Type                              Count   Percentage\n",
      "   --------------------------------------------\n",
      "   Food Waste Collection           1,644      35.3%\n",
      "   Recycling Center                1,422      30.5%\n",
      "   Glass Collection                1,079      23.1%\n",
      "   Pharmacy                          139       3.0%\n",
      "   Dual Recycling Point              131       2.8%\n",
      "   Car Repair/Tire                   108       2.3%\n",
      "   Collection Point                   69       1.5%\n",
      "   Other Facility                     38       0.8%\n",
      "   Single-Type Collection             24       0.5%\n",
      "   Ressourcerie                        7       0.2%\n",
      "\n",
      "   SUMMARY:\n",
      "   ‚Ä¢ Total facilities: 4,661\n",
      "   ‚Ä¢ Top 3 types cover 88.9% of all facilities\n",
      "   ‚Ä¢ Most common: Food Waste Collection (1,644 locations)\n",
      "\n",
      "2. WASTE TYPE ACCEPTANCE ANALYSIS:\n",
      "----------------------------------------\n",
      "   Waste Type                    Count   Acceptance Rate\n",
      "   --------------------------------------------------\n",
      "   Glass                      2,500     53.6% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Food                       2,487     53.4% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Cardboard                  1,505     32.3% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Paper                      1,505     32.3% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Metal                      1,421     30.5% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Plastic                    1,407     30.2% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Vegetation                   857     18.4% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Pharmacy                     139      3.0% \n",
      "   Tire                         108      2.3% \n",
      "   Textile                       78      1.7% \n",
      "\n",
      "   KEY INSIGHTS:\n",
      "   ‚Ä¢ Glass collection is the most available service\n",
      "   ‚Ä¢ Food waste collection is nearly as widespread as glass\n",
      "   ‚Ä¢ 2 waste types are available at >50% of locations\n",
      "   ‚Ä¢ 7 specialized services are available at <5% of locations\n",
      "\n",
      "3. FACILITY SPECIALIZATION:\n",
      "----------------------------------------\n",
      "   Specialization Level        Count   Percentage\n",
      "   --------------------------------------------\n",
      "   Single-type                3,070      65.9%\n",
      "   Versatile (6+)               857      18.4%\n",
      "   Multi-type (3-5)             565      12.1%\n",
      "   Dual-type                    131       2.8%\n",
      "   No waste types                38       0.8%\n",
      "\n",
      "4. CORRELATION ANALYSIS:\n",
      "----------------------------------------\n",
      "   Top Facility Types and Their Waste Acceptance Patterns:\n",
      "   -------------------------------------------------------\n",
      "   ‚Ä¢ Food Waste Collection     1,644 facilities\n",
      "     Accepts: Food (100%)\n",
      "   ‚Ä¢ Recycling Center          1,422 facilities\n",
      "     Accepts: Cardboard (100%), Paper (100%), Glass (100%)\n",
      "   ‚Ä¢ Glass Collection          1,079 facilities\n",
      "     Accepts: Glass (100%)\n",
      "   ‚Ä¢ Pharmacy                   139 facilities\n",
      "     Accepts: Pharmacy (100%)\n",
      "   ‚Ä¢ Dual Recycling Point       131 facilities\n",
      "     Accepts: Cardboard (63%), Paper (63%), Neon (37%)\n",
      "\n",
      "5. GEOGRAPHIC DISTRIBUTION:\n",
      "----------------------------------------\n",
      "   Facility Density Analysis:\n",
      "   ‚Ä¢ Food Waste Collection     Moderate spread (1,644 with coordinates)\n",
      "   ‚Ä¢ Recycling Center          Moderate spread (1,422 with coordinates)\n",
      "   ‚Ä¢ Glass Collection          Widespread      (1,079 with coordinates)\n",
      "\n",
      "6. SERVICE COVERAGE GAP ANALYSIS:\n",
      "----------------------------------------\n",
      "   Rare Service Combinations:\n",
      "   ‚Ä¢ 7 waste types have <10% coverage:\n",
      "\n",
      "7. RECOMMENDATIONS:\n",
      "----------------------------------------\n",
      "   Based on the analysis, consider:\n",
      "   1. Food waste and glass collection have excellent coverage (>50%)\n",
      "   2. Food Waste Collection facilities are most common - could serve as model for expansion\n",
      "   3. Specialized services (pharmacy, car repair, ressourcerie) have limited coverage\n",
      "   4. 857 facilities offer 6+ waste types\n",
      "   5. 857/857 versatile facilities have coordinates\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE - KEY FINDINGS:\n",
      "============================================================\n",
      "\n",
      "üìä SUMMARY STATISTICS:\n",
      "   ‚Ä¢ Total collection points: 4,661\n",
      "   ‚Ä¢ Average waste types per facility: 2.6\n",
      "   ‚Ä¢ Most versatile facility accepts: 8 waste types\n",
      "   ‚Ä¢ Facility types: 10 distinct categories\n",
      "\n",
      "üè¢ FACILITY TYPE DISTRIBUTION:\n",
      "   ‚Ä¢ Top 3 types: Food Waste Collection, Recycling Center, Glass Collection\n",
      "   ‚Ä¢ Cover 4,145 facilities (88.9% of total)\n",
      "\n",
      "üóëÔ∏è  WASTE TYPE AVAILABILITY:\n",
      "   ‚Ä¢ Most available: Glass (53.6% of facilities)\n",
      "   ‚Ä¢ Total waste types tracked: 14\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CREATE DETAILED ANALYSIS\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DETAILED ANALYSIS FOR TRASH COLLECTION POINTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. IMPROVED FACILITY TYPE BREAKDOWN\n",
    "print(\"\\n1. FACILITY TYPE BREAKDOWN:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Get total count for percentages\n",
    "total_locations = len(combined_df)\n",
    "type_counts = combined_df['Facility_Type'].value_counts()\n",
    "\n",
    "print(\"   Type                              Count   Percentage\")\n",
    "print(\"   \" + \"-\"*44)\n",
    "\n",
    "total_count = 0\n",
    "for type_name, count in type_counts.items():\n",
    "    percentage = (count / total_locations) * 100\n",
    "    total_count += count\n",
    "    print(f\"   {type_name:30} {count:6,}     {percentage:5.1f}%\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n   SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Total facilities: {total_locations:,}\")\n",
    "print(f\"   ‚Ä¢ Top 3 types cover {(type_counts.head(3).sum()/total_locations*100):.1f}% of all facilities\")\n",
    "print(f\"   ‚Ä¢ Most common: {type_counts.index[0]} ({type_counts.iloc[0]:,} locations)\")\n",
    "\n",
    "# 2. WASTE TYPE ACCEPTANCE - IMPROVED BASED ON YOUR DATA\n",
    "print(\"\\n2. WASTE TYPE ACCEPTANCE ANALYSIS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Based on your output, we know the most common types are Glass and Food\n",
    "# Let's create a more detailed analysis\n",
    "\n",
    "# Get all waste columns that exist\n",
    "waste_cols = [col for col in combined_df.columns\n",
    "              if col.startswith('Is_') and col.endswith('_enabled')]\n",
    "\n",
    "# Calculate acceptance rates\n",
    "waste_stats = []\n",
    "for col in waste_cols:\n",
    "    count = int(combined_df[col].sum())\n",
    "    if count > 0:\n",
    "        percentage = (count / total_locations) * 100\n",
    "        waste_name = col.replace('Is_', '').replace('_enabled', '').replace('_', ' ').title()\n",
    "        waste_stats.append((waste_name, count, percentage))\n",
    "\n",
    "# Sort by count\n",
    "waste_stats.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"   Waste Type                    Count   Acceptance Rate\")\n",
    "print(\"   \" + \"-\"*50)\n",
    "\n",
    "# Display with visual indicators\n",
    "top_waste_types = waste_stats[:10]  # Show top 10\n",
    "for name, count, pct in top_waste_types:\n",
    "    # Create a simple bar chart\n",
    "    bar_length = int(pct / 3)  # Scale for better visualization\n",
    "    bar = '‚ñà' * min(bar_length, 30)  # Limit bar length\n",
    "    print(f\"   {name:25} {count:6,}    {pct:5.1f}% {bar}\")\n",
    "\n",
    "# Special insights based on your data\n",
    "print(f\"\\n   KEY INSIGHTS:\")\n",
    "print(f\"   ‚Ä¢ Glass collection is the most available service\")\n",
    "print(f\"   ‚Ä¢ Food waste collection is nearly as widespread as glass\")\n",
    "print(f\"   ‚Ä¢ {len([x for x in waste_stats if x[2] > 50])} waste types are available at >50% of locations\")\n",
    "print(f\"   ‚Ä¢ {len([x for x in waste_stats if x[2] < 5])} specialized services are available at <5% of locations\")\n",
    "\n",
    "# 3. FACILITY SPECIALIZATION ANALYSIS\n",
    "print(\"\\n3. FACILITY SPECIALIZATION:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Count how many waste types each facility accepts\n",
    "combined_df['Num_Waste_Types'] = combined_df[waste_cols].sum(axis=1)\n",
    "\n",
    "# Categorize by specialization level\n",
    "def categorize_specialization(num_types):\n",
    "    if num_types == 0:\n",
    "        return \"No waste types\"\n",
    "    elif num_types == 1:\n",
    "        return \"Single-type\"\n",
    "    elif num_types == 2:\n",
    "        return \"Dual-type\"\n",
    "    elif num_types <= 5:\n",
    "        return \"Multi-type (3-5)\"\n",
    "    else:\n",
    "        return \"Versatile (6+)\"\n",
    "\n",
    "combined_df['Specialization_Level'] = combined_df['Num_Waste_Types'].apply(categorize_specialization)\n",
    "specialization_counts = combined_df['Specialization_Level'].value_counts()\n",
    "\n",
    "print(\"   Specialization Level        Count   Percentage\")\n",
    "print(\"   \" + \"-\"*44)\n",
    "\n",
    "for level, count in specialization_counts.items():\n",
    "    percentage = (count / total_locations) * 100\n",
    "    print(f\"   {level:25} {count:6,}     {percentage:5.1f}%\")\n",
    "\n",
    "# 4. CORRELATION ANALYSIS BETWEEN FACILITY TYPES AND WASTE TYPES\n",
    "print(\"\\n4. CORRELATION ANALYSIS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Top facility types and their waste acceptance profiles\n",
    "top_facility_types = type_counts.head(5).index.tolist()\n",
    "\n",
    "print(\"   Top Facility Types and Their Waste Acceptance Patterns:\")\n",
    "print(\"   \" + \"-\"*55)\n",
    "\n",
    "for facility_type in top_facility_types:\n",
    "    facilities_of_type = combined_df[combined_df['Facility_Type'] == facility_type]\n",
    "    if len(facilities_of_type) > 0:\n",
    "        # Get top 3 waste types for this facility type\n",
    "        waste_acceptance = {}\n",
    "        for col in waste_cols:\n",
    "            waste_name = col.replace('Is_', '').replace('_enabled', '').replace('_', ' ').title()\n",
    "            acceptance_rate = (facilities_of_type[col].sum() / len(facilities_of_type)) * 100\n",
    "            if acceptance_rate > 0:\n",
    "                waste_acceptance[waste_name] = acceptance_rate\n",
    "\n",
    "        # Sort and get top 3\n",
    "        top_wastes = sorted(waste_acceptance.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "        top_waste_str = \", \".join([f\"{name} ({rate:.0f}%)\" for name, rate in top_wastes])\n",
    "        print(f\"   ‚Ä¢ {facility_type:25} {len(facilities_of_type):4,} facilities\")\n",
    "        print(f\"     Accepts: {top_waste_str}\")\n",
    "\n",
    "# 5. GEOGRAPHIC DISTRIBUTION ANALYSIS\n",
    "print(\"\\n5. GEOGRAPHIC DISTRIBUTION:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "if combined_df['Latitude'].notna().any() and combined_df['Longitude'].notna().any():\n",
    "    # Calculate density by facility type\n",
    "    print(\"   Facility Density Analysis:\")\n",
    "\n",
    "    # For top facility types, show their geographic spread\n",
    "    for facility_type in top_facility_types[:3]:  # Top 3 only\n",
    "        facilities_of_type = combined_df[combined_df['Facility_Type'] == facility_type]\n",
    "        with_coords = facilities_of_type['Latitude'].notna().sum()\n",
    "\n",
    "        if with_coords > 0:\n",
    "            lat_std = facilities_of_type['Latitude'].std()\n",
    "            lon_std = facilities_of_type['Longitude'].std()\n",
    "\n",
    "            # Simple spread indicator\n",
    "            if lat_std > 0.05 or lon_std > 0.05:\n",
    "                spread = \"Widespread\"\n",
    "            elif lat_std > 0.02 or lon_std > 0.02:\n",
    "                spread = \"Moderate spread\"\n",
    "            else:\n",
    "                spread = \"Concentrated\"\n",
    "\n",
    "            print(f\"   ‚Ä¢ {facility_type:25} {spread:15} ({with_coords:,} with coordinates)\")\n",
    "\n",
    "# 6. SERVICE COVERAGE GAP ANALYSIS\n",
    "print(\"\\n6. SERVICE COVERAGE GAP ANALYSIS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Find combinations of services that are rarely found together\n",
    "print(\"   Rare Service Combinations:\")\n",
    "\n",
    "# Get all waste types with low coverage\n",
    "low_coverage_types = [name for name, count, pct in waste_stats if pct < 10]\n",
    "\n",
    "if low_coverage_types:\n",
    "    print(f\"   ‚Ä¢ {len(low_coverage_types)} waste types have <10% coverage:\")\n",
    "    for waste_type in low_coverage_types[:5]:  # Show first 5\n",
    "        # Find which facilities offer this service\n",
    "        col_name = f\"Is_{waste_type.lower().replace(' ', '_')}_enabled\"\n",
    "        if col_name in combined_df.columns:\n",
    "            facilities_with_service = combined_df[combined_df[col_name] == 1]\n",
    "            facility_types = facilities_with_service['Facility_Type'].value_counts().head(2)\n",
    "            facility_str = \", \".join([f\"{typ} ({cnt})\" for typ, cnt in facility_types.items()])\n",
    "            print(f\"     - {waste_type:20} found in: {facility_str}\")\n",
    "\n",
    "# 7. RECOMMENDATIONS BASED ON ANALYSIS\n",
    "print(\"\\n7. RECOMMENDATIONS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(\"   Based on the analysis, consider:\")\n",
    "print(f\"   1. Food waste and glass collection have excellent coverage (>50%)\")\n",
    "print(f\"   2. {type_counts.index[0]} facilities are most common - could serve as model for expansion\")\n",
    "print(f\"   3. Specialized services (pharmacy, car repair, ressourcerie) have limited coverage\")\n",
    "print(f\"   4. {specialization_counts.get('Versatile (6+)', 0):,} facilities offer 6+ waste types\")\n",
    "\n",
    "# Check if versatile facilities have coordinates\n",
    "if 'Num_Waste_Types' in combined_df.columns:\n",
    "    versatile_facilities = combined_df[combined_df['Num_Waste_Types'] >= 6]\n",
    "    if len(versatile_facilities) > 0:\n",
    "        with_coords = versatile_facilities['Latitude'].notna().sum()\n",
    "        print(f\"   5. {with_coords}/{len(versatile_facilities)} versatile facilities have coordinates\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE - KEY FINDINGS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create summary statistics\n",
    "print(f\"\\nüìä SUMMARY STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Total collection points: {total_locations:,}\")\n",
    "print(f\"   ‚Ä¢ Average waste types per facility: {combined_df['Num_Waste_Types'].mean():.1f}\")\n",
    "print(f\"   ‚Ä¢ Most versatile facility accepts: {combined_df['Num_Waste_Types'].max()} waste types\")\n",
    "print(f\"   ‚Ä¢ Facility types: {len(type_counts)} distinct categories\")\n",
    "\n",
    "# Facility type distribution\n",
    "print(f\"\\nüè¢ FACILITY TYPE DISTRIBUTION:\")\n",
    "top_3_total = type_counts.head(3).sum()\n",
    "print(f\"   ‚Ä¢ Top 3 types: {', '.join(type_counts.head(3).index.tolist())}\")\n",
    "print(f\"   ‚Ä¢ Cover {top_3_total:,} facilities ({top_3_total/total_locations*100:.1f}% of total)\")\n",
    "\n",
    "# Waste type availability\n",
    "top_waste = waste_stats[0] if waste_stats else (\"None\", 0, 0)\n",
    "print(f\"\\nüóëÔ∏è  WASTE TYPE AVAILABILITY:\")\n",
    "print(f\"   ‚Ä¢ Most available: {top_waste[0]} ({top_waste[2]:.1f}% of facilities)\")\n",
    "print(f\"   ‚Ä¢ Total waste types tracked: {len(waste_stats)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trash-optimizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
