{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e45445c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3c6b618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating optimized trash collection points table\n"
     ]
    }
   ],
   "source": [
    "# Set credentials\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/dariaserbichenko/code/DariaSerb/key-gcp/trash-optimizer-479913-91e59ecc96c9.json\"\n",
    "\n",
    "PROJECT = \"trash-optimizer-479913\"\n",
    "DATASET = \"nantes\"\n",
    "client = bigquery.Client(project=PROJECT)\n",
    "\n",
    "print(\"Creating optimized trash collection points table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b97fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Querying alimentary_garbage_clean (food waste)\n"
     ]
    }
   ],
   "source": [
    "# FIRST QUERY ALIMENTARY_GARBAGE (FOOD WASTE)\n",
    "\n",
    "print(\"\\n1. Querying alimentary_garbage_clean (food waste)\")\n",
    "\n",
    "query1 = f\"\"\"\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER () as ID,\n",
    "  CONCAT('Food Waste - ', COALESCE(commune, 'Nantes')) as Name,\n",
    "  COALESCE(adresse, 'Address not specified') as Address,\n",
    "  lon as Longitude,\n",
    "  lat as Latitude,\n",
    "  0 as Is_Cardboard_enabled,\n",
    "  1 as Is_Food_enabled,\n",
    "  0 as Is_Glass_enabled,\n",
    "  0 as Is_Metal_enabled,\n",
    "  0 as Is_Paper_enabled,\n",
    "  0 as Is_Plastic_enabled,\n",
    "  0 as Is_Textile_enabled,\n",
    "  0 as Is_Vegetation_enabled,\n",
    "  0 as Is_Neon_enabled,\n",
    "  0 as Is_Cartridge_enabled,\n",
    "  0 as Is_Lamp_Light_enabled\n",
    "FROM `{PROJECT}.{DATASET}.alimentary_garbage_clean`\n",
    "WHERE lat IS NOT NULL AND lon IS NOT NULL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d72dcd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1,644 food waste locations\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df1 = client.query(query1).to_dataframe()\n",
    "    print(f\"Retrieved {len(df1):,} food waste locations\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    df1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5a98cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Querying ecopoints with actual columns\n"
     ]
    }
   ],
   "source": [
    "# SECOND QUERY ECOPOINTS (USING ACTUAL COLUMNS FOUND)\n",
    "\n",
    "print(\"\\n2. Querying ecopoints with actual columns\")\n",
    "\n",
    "# From inspection: columns are ['bois', 'carton', 'ferraille', 'cartouche', 'neon', 'papier', 'textile', 'verre']\n",
    "\n",
    "query2 = f\"\"\"\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER () + 10000 as ID,\n",
    "  CONCAT('Recycling Center - ', COALESCE(nom, commune, 'Ecopoint')) as Name,\n",
    "  COALESCE(adresse, 'Address not specified') as Address,\n",
    "  lon as Longitude,\n",
    "  lat as Latitude,\n",
    "\n",
    "  -- Use actual columns found\n",
    "  CASE WHEN UPPER(carton) = 'OUI' THEN 1 ELSE 0 END as Is_Cardboard_enabled,\n",
    "  0 as Is_Food_enabled,\n",
    "  CASE WHEN UPPER(verre) = 'OUI' THEN 1 ELSE 0 END as Is_Glass_enabled,\n",
    "  CASE WHEN UPPER(ferraille) = 'OUI' THEN 1 ELSE 0 END as Is_Metal_enabled,\n",
    "  CASE WHEN UPPER(papier) = 'OUI' THEN 1 ELSE 0 END as Is_Paper_enabled,\n",
    "  CASE WHEN UPPER(dechet_vert) = 'OUI' THEN 1 ELSE 0 END as Is_Vegetation_enabled,\n",
    "  0 as Is_Plastic_enabled,  -- No plastique column\n",
    "  CASE WHEN UPPER(textile) = 'OUI' THEN 1 ELSE 0 END as Is_Textile_enabled,\n",
    "  CASE WHEN UPPER(neon) = 'OUI' THEN 1 ELSE 0 END as Is_Neon_enabled,\n",
    "  CASE WHEN UPPER(cartouche) = 'OUI' THEN 1 ELSE 0 END as Is_Cartridge_enabled,\n",
    "  0 as Is_Lamp_Light_enabled  -- No ampoule column\n",
    "FROM `{PROJECT}.{DATASET}.ecopoints`\n",
    "WHERE lat IS NOT NULL AND lon IS NOT NULL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d33dfd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 15 recycling centers with actual waste types\n",
      "- Waste acceptance in recycling centers:\n",
      "   Cardboard: 15/15 locations\n",
      "   Glass: 14/15 locations\n",
      "   Metal: 14/15 locations\n",
      "   Paper: 15/15 locations\n",
      "   Vegetation: 14/15 locations\n",
      "   Textile: 9/15 locations\n",
      "   Neon: 8/15 locations\n",
      "   Cartridge: 15/15 locations\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df2 = client.query(query2).to_dataframe()\n",
    "    print(f\"Retrieved {len(df2)} recycling centers with actual waste types\")\n",
    "\n",
    "    # Check acceptance rates\n",
    "    waste_cols = [col for col in df2.columns if col.startswith('Is_')]\n",
    "    print(f\"- Waste acceptance in recycling centers:\")\n",
    "    for col in waste_cols:\n",
    "        count = df2[col].sum()\n",
    "        if count > 0:\n",
    "            waste_name = col.replace('Is_', '').replace('_enabled', '').replace('_', ' ').title()\n",
    "            print(f\"   {waste_name}: {count}/{len(df2)} locations\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error: {e}\")\n",
    "    df2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0dcfddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Querying glass collection columns (Verre only)\n",
      "‚úÖ Retrieved 1,079 glass collection columns\n",
      "\n",
      "üìä GLASS COLUMNS SUMMARY:\n",
      "  Total glass columns: 1,079\n",
      "  With valid coordinates: 1,079\n",
      "\n",
      "üëÄ SAMPLE GLASS COLUMNS (first 3):\n",
      "  1. Drop-off points - Underground - Nantes\n",
      "     Address: Rue de la petite Sensive...\n",
      "     Location: (47.260437, -1.561580)\n",
      "     Glass enabled: ‚úì\n",
      "  2. Drop-off points - Underground - Nantes\n",
      "     Address: Rue Blaise Pascal...\n",
      "     Location: (47.256204, -1.566761)\n",
      "     Glass enabled: ‚úì\n",
      "  3. Drop-off points - Underground - Nantes\n",
      "     Address: 2 Rue de Concarneau...\n",
      "     Location: (47.264360, -1.578521)\n",
      "     Glass enabled: ‚úì\n",
      "\n",
      "üóëÔ∏è  WASTE TYPE ENABLEMENT (should be Glass only):\n",
      "  Glass: 1,079/1,079 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# THIRD QUERY FOR GLASS COLLECTION COLUMNS (VERRE ONLY)\n",
    "\n",
    "print(\"\\n3. Querying glass collection columns (Verre only)\")\n",
    "\n",
    "query3 = f\"\"\"\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER () + 30000 as ID,  # Start from 30000 for glass columns\n",
    "  CONCAT(\n",
    "    'Drop-off points - ',\n",
    "    COALESCE(\n",
    "      CASE\n",
    "        WHEN type_colonne IS NOT NULL THEN\n",
    "          CASE type_colonne\n",
    "            WHEN 'colonne enterr√©e' THEN 'Underground'\n",
    "            WHEN 'colonne a√©rienne' THEN 'Above-ground'\n",
    "            ELSE INITCAP(type_colonne)\n",
    "          END\n",
    "        ELSE ''\n",
    "      END,\n",
    "      'Glass Collection'\n",
    "    ),\n",
    "    CASE\n",
    "      WHEN commune IS NOT NULL THEN CONCAT(' - ', commune)\n",
    "      ELSE ' - Nantes'\n",
    "    END\n",
    "  ) as Name,\n",
    "  COALESCE(adresse, 'Nantes M√©tropole') as Address,\n",
    "  lat as Latitude,\n",
    "  lon as Longitude,\n",
    "\n",
    "  -- Waste type capabilities: ONLY GLASS ENABLED\n",
    "  0 as Is_Cardboard_enabled,\n",
    "  0 as Is_Food_enabled,\n",
    "  1 as Is_Glass_enabled,  # All these points are for glass collection\n",
    "  0 as Is_Metal_enabled,\n",
    "  0 as Is_Paper_enabled,\n",
    "  0 as Is_Plastic_enabled,\n",
    "  0 as Is_Textile_enabled,\n",
    "  0 as Is_Vegetation_enabled,\n",
    "  0 as Is_Neon_enabled,\n",
    "  0 as Is_Cartridge_enabled,\n",
    "  0 as Is_Lamp_Light_enabled\n",
    "\n",
    "FROM `{PROJECT}.{DATASET}.location_dropoff_points_nantes`\n",
    "WHERE\n",
    "  lat IS NOT NULL\n",
    "  AND lon IS NOT NULL\n",
    "  AND LOWER(TRIM(type_dechet)) = 'verre'  # Only glass collection points\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df3 = client.query(query3).to_dataframe()\n",
    "    print(f\"‚úÖ Retrieved {len(df3):,} glass collection columns\")\n",
    "\n",
    "    # Show summary\n",
    "    print(f\"\\nüìä GLASS COLUMNS SUMMARY:\")\n",
    "    print(f\"  Total glass columns: {len(df3):,}\")\n",
    "\n",
    "    # Check coordinate validity\n",
    "    valid_coords = df3['Latitude'].notna().sum()\n",
    "    print(f\"  With valid coordinates: {valid_coords:,}\")\n",
    "\n",
    "    # Show sample\n",
    "    print(f\"\\nüëÄ SAMPLE GLASS COLUMNS (first 3):\")\n",
    "    for i in range(min(3, len(df3))):\n",
    "        row = df3.iloc[i]\n",
    "        print(f\"  {i+1}. {row['Name']}\")\n",
    "        print(f\"     Address: {row['Address'][:60]}...\")\n",
    "        print(f\"     Location: ({row['Latitude']:.6f}, {row['Longitude']:.6f})\")\n",
    "        print(f\"     Glass enabled: {'‚úì' if row['Is_Glass_enabled'] == 1 else '‚úó'}\")\n",
    "\n",
    "    # Show waste type summary\n",
    "    print(f\"\\nüóëÔ∏è  WASTE TYPE ENABLEMENT (should be Glass only):\")\n",
    "    waste_cols = [col for col in df3.columns if col.startswith('Is_')]\n",
    "    for col in waste_cols:\n",
    "        count = df3[col].sum()\n",
    "        if count > 0:\n",
    "            waste_name = col.replace('Is_', '').replace('_enabled', '').replace('_', ' ').title()\n",
    "            print(f\"  {waste_name}: {count:,}/{len(df3):,} ({count/len(df3)*100:.1f}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error querying glass columns: {e}\")\n",
    "\n",
    "    # Debug: Check what types of waste exist in the table\n",
    "    print(\"\\nüîç Debug: Checking available waste types in the table...\")\n",
    "    try:\n",
    "        debug_query = f\"\"\"\n",
    "        SELECT\n",
    "          type_dechet,\n",
    "          COUNT(*) as count\n",
    "        FROM `{PROJECT}.{DATASET}.location_dropoff_points_nantes`\n",
    "        WHERE type_dechet IS NOT NULL\n",
    "        GROUP BY type_dechet\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        waste_types = client.query(debug_query).to_dataframe()\n",
    "        print(f\"Available waste types in table:\")\n",
    "        print(waste_types.to_string(index=False))\n",
    "    except:\n",
    "        print(\"Could not check waste types\")\n",
    "\n",
    "    df3 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27e64d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING FINAL TRASH COLLECTION POINTS TABLE\n",
      "Food waste points: 1,644\n",
      "Recycling centers: 15\n",
      "Underground containers: 1,079\n",
      "FINAL TABLE: 2,738 total trash collection points\n",
      "CSV saved: 'trash_collection_points_final_optimized.csv'\n",
      "Uploading to BigQuery\n",
      "BigQuery table created: trash-optimizer-479913.nantes.trash_collection_points\n",
      "   Rows: 2,738\n",
      "   Size: 0.43 MB\n"
     ]
    }
   ],
   "source": [
    "# COMBINE AND CREATE FINAL TABLE\n",
    "\n",
    "print(\"CREATING FINAL TRASH COLLECTION POINTS TABLE\")\n",
    "\n",
    "all_dataframes = []\n",
    "\n",
    "if not df1.empty:\n",
    "    all_dataframes.append(df1)\n",
    "    print(f\"Food waste points: {len(df1):,}\")\n",
    "if not df2.empty:\n",
    "    all_dataframes.append(df2)\n",
    "    print(f\"Recycling centers: {len(df2)}\")\n",
    "if not df3.empty:\n",
    "    all_dataframes.append(df3)\n",
    "    print(f\"Underground containers: {len(df3):,}\")\n",
    "\n",
    "if all_dataframes:\n",
    "    # Combine all data\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # Reset ID to be sequential\n",
    "    combined_df['ID'] = range(1, len(combined_df) + 1)\n",
    "\n",
    "    # Define final structure\n",
    "    final_columns = [\n",
    "        'ID', 'Name', 'Address', 'Longitude', 'Latitude',\n",
    "        'Is_Cardboard_enabled', 'Is_Food_enabled', 'Is_Glass_enabled',\n",
    "        'Is_Metal_enabled', 'Is_Paper_enabled', 'Is_Plastic_enabled',\n",
    "        'Is_Textile_enabled', 'Is_Vegetation_enabled', 'Is_Neon_enabled',\n",
    "        'Is_Cartridge_enabled', 'Is_Lamp_Light_enabled'\n",
    "    ]\n",
    "\n",
    "    # Ensure all columns exist\n",
    "    for col in final_columns:\n",
    "        if col not in combined_df.columns:\n",
    "            if col.startswith('Is_'):\n",
    "                combined_df[col] = 0\n",
    "\n",
    "    # Convert to proper types\n",
    "    for col in combined_df.columns:\n",
    "        if col.startswith('Is_'):\n",
    "            combined_df[col] = combined_df[col].astype(int)\n",
    "        elif col in ['Longitude', 'Latitude']:\n",
    "            combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "\n",
    "    # Reorder\n",
    "    combined_df = combined_df[final_columns]\n",
    "\n",
    "    total_locations = len(combined_df)\n",
    "    print(f\"FINAL TABLE: {total_locations:,} total trash collection points\")\n",
    "\n",
    "    # Save to CSV\n",
    "    output_csv = 'trash_collection_points_final_optimized.csv'\n",
    "    combined_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"CSV saved: '{output_csv}'\")\n",
    "\n",
    "    # ===== UPLOAD TO BIGQUERY =====\n",
    "    print(f\"Uploading to BigQuery\")\n",
    "    table_id = f\"{PROJECT}.{DATASET}.trash_collection_points\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "        autodetect=True,\n",
    "        max_bad_records=100\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(combined_df, table_id, job_config=job_config)\n",
    "        job.result()\n",
    "\n",
    "        table = client.get_table(table_id)\n",
    "        print(f\"BigQuery table created: {table_id}\")\n",
    "        print(f\"   Rows: {table.num_rows:,}\")\n",
    "        print(f\"   Size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"BigQuery upload failed: {e}\")\n",
    "        print(f\"   Data saved locally: '{output_csv}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65bc994c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAILED ANALYSIS FOR TRASH COLLECTION POINTS\n",
      "\n",
      "1. FACILITY TYPES:\n",
      "   Food Waste            1,644 locations ( 60.0%)\n",
      "\n",
      "2. WASTE TYPE ACCEPTANCE:\n",
      "   Food                  1,644 locations ( 60.0%)\n",
      "   Glass                 1,093 locations ( 39.9%)\n",
      "   Cardboard                15 locations (  0.5%)\n",
      "   Paper                    15 locations (  0.5%)\n",
      "   Cartridge                15 locations (  0.5%)\n",
      "   Metal                    14 locations (  0.5%)\n",
      "   Vegetation               14 locations (  0.5%)\n",
      "   Textile                   9 locations (  0.3%)\n",
      "   Neon                      8 locations (  0.3%)\n",
      "   Plastic                   0 locations (  0.0%)\n",
      "   Lamp Light                0 locations (  0.0%)\n",
      "\n",
      "3. GEOGRAPHIC COVERAGE:\n",
      "   Latitude range:  47.1225 to 47.3335\n",
      "   Longitude range: -1.8177 to -1.3820\n",
      "   Center point:    (47.2255, -1.5594)\n",
      "   Recycling Center         15 locations (  0.5%)\n",
      "\n",
      "2. WASTE TYPE ACCEPTANCE:\n",
      "   Food                  1,644 locations ( 60.0%)\n",
      "   Glass                 1,093 locations ( 39.9%)\n",
      "   Cardboard                15 locations (  0.5%)\n",
      "   Paper                    15 locations (  0.5%)\n",
      "   Cartridge                15 locations (  0.5%)\n",
      "   Metal                    14 locations (  0.5%)\n",
      "   Vegetation               14 locations (  0.5%)\n",
      "   Textile                   9 locations (  0.3%)\n",
      "   Neon                      8 locations (  0.3%)\n",
      "   Plastic                   0 locations (  0.0%)\n",
      "   Lamp Light                0 locations (  0.0%)\n",
      "\n",
      "3. GEOGRAPHIC COVERAGE:\n",
      "   Latitude range:  47.1225 to 47.3335\n",
      "   Longitude range: -1.8177 to -1.3820\n",
      "   Center point:    (47.2255, -1.5594)\n"
     ]
    }
   ],
   "source": [
    "# CREATE DETAILED ANALYSIS\n",
    "\n",
    "print(f\"DETAILED ANALYSIS FOR TRASH COLLECTION POINTS\")\n",
    "\n",
    "# 1. Facility type breakdown\n",
    "\n",
    "print(f\"\\n1. FACILITY TYPES:\")\n",
    "facility_summary = combined_df['Name'].str.extract(r'^(Food Waste|Recycling Center|Underground containers)')[0]\n",
    "type_counts = facility_summary.value_counts()\n",
    "\n",
    "for type_name, count in type_counts.items():\n",
    "    percentage = (count / total_locations) * 100\n",
    "    print(f\"   {type_name:20} {count:6,} locations ({percentage:5.1f}%)\")\n",
    "\n",
    "# 2. Waste type acceptance\n",
    "\n",
    "    print(f\"\\n2. WASTE TYPE ACCEPTANCE:\")\n",
    "    waste_cols = [col for col in combined_df.columns if col.startswith('Is_')]\n",
    "\n",
    "    waste_stats = []\n",
    "    for col in waste_cols:\n",
    "        count = combined_df[col].sum()\n",
    "        percentage = (count / total_locations) * 100\n",
    "        waste_name = col.replace('Is_', '').replace('_enabled', '').replace('_', ' ').title()\n",
    "        waste_stats.append((waste_name, count, percentage))\n",
    "\n",
    "# Sort by most accepted\n",
    "\n",
    "    waste_stats.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for name, count, pct in waste_stats:\n",
    "        print(f\"   {name:20} {count:6,} locations ({pct:5.1f}%)\")\n",
    "\n",
    "# 3. Geographic coverage\n",
    "\n",
    "    print(f\"\\n3. GEOGRAPHIC COVERAGE:\")\n",
    "    if combined_df['Latitude'].notna().any() and combined_df['Longitude'].notna().any():\n",
    "        min_lat = combined_df['Latitude'].min()\n",
    "        max_lat = combined_df['Latitude'].max()\n",
    "        min_lon = combined_df['Longitude'].min()\n",
    "        max_lon = combined_df['Longitude'].max()\n",
    "\n",
    "        print(f\"   Latitude range:  {min_lat:.4f} to {max_lat:.4f}\")\n",
    "        print(f\"   Longitude range: {min_lon:.4f} to {max_lon:.4f}\")\n",
    "        print(f\"   Center point:    ({combined_df['Latitude'].mean():.4f}, {combined_df['Longitude'].mean():.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trash-optimizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
