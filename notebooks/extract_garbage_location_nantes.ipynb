{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X7K29fAhDikX"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import NotFound, BadRequest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5oUFiuFdXO4",
        "outputId": "acc1f19e-d846-48e9-b307-4aa732f5e1a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records: 2782\n",
            "    micro_quartier                      adresse domaine identifiant  \\\n",
            "0      La Botti√®re        2 Rue de la Basinerie  Public    BIOD0582   \n",
            "1      Le Landreau           18 Rue Anita Conti   Priv√©    BIOD0514   \n",
            "2  Rte de Ste Luce  276 Route de Sainte de Luce  Public    BIOD0410   \n",
            "3  Rte de Ste Luce     288 Route de Sainte Luce  Public    BIOD0503   \n",
            "4  Rte de Ste Luce           1 Rue Henri Loiret  Public    BIOD0632   \n",
            "\n",
            "                    date_mes commune fournisseur  \\\n",
            "0  2022-12-12T01:00:00+00:00  Nantes        SUEZ   \n",
            "1  2022-12-20T01:00:00+00:00  Nantes        SUEZ   \n",
            "2  2022-12-01T01:00:00+00:00  Nantes        SUEZ   \n",
            "3  2022-12-01T01:00:00+00:00  Nantes        SUEZ   \n",
            "4  2022-12-01T01:00:00+00:00  Nantes        SUEZ   \n",
            "\n",
            "                                geo_point_2d plaque_id  \n",
            "0    [47.23819860017184, -1.517678599864239]       NaN  \n",
            "1   [47.238733199907195, -1.511478400639151]       NaN  \n",
            "2   [47.24174179981523, -1.5074455997902827]       NaN  \n",
            "3   [47.24231100038424, -1.5062445999470977]       NaN  \n",
            "4  [47.243990000355396, -1.5045800003122118]       NaN  \n"
          ]
        }
      ],
      "source": [
        "# Point de collecte d√©chets alimentaires (biod√©chet) de Nantes\n",
        "BASE_URL = \"https://data.nantesmetropole.fr/api/records/1.0/search/\"\n",
        "\n",
        "params = {\n",
        "    \"dataset\": \"244400404_point-collecte-dechets-alimentaires-biodechet-nantes\",\n",
        "    \"rows\": 10000}\n",
        "\n",
        "response = requests.get(BASE_URL, params=params)\n",
        "response.raise_for_status()\n",
        "data = response.json()\n",
        "\n",
        "# Extract records\n",
        "records = []\n",
        "for record in data['records']:\n",
        "    fields = record['fields']\n",
        "    records.append(fields)\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "print(f\"Total records: {len(df)}\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sWUJch6KaHY",
        "outputId": "b942bdd5-8f35-4380-da20-e4372c293568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2782 entries, 0 to 2781\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   micro_quartier  2781 non-null   object\n",
            " 1   adresse         2782 non-null   object\n",
            " 2   domaine         2752 non-null   object\n",
            " 3   identifiant     2775 non-null   object\n",
            " 4   date_mes        1464 non-null   object\n",
            " 5   commune         2782 non-null   object\n",
            " 6   fournisseur     611 non-null    object\n",
            " 7   geo_point_2d    2782 non-null   object\n",
            " 8   plaque_id       220 non-null    object\n",
            "dtypes: object(9)\n",
            "memory usage: 195.7+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8XBA2_-fl_R",
        "outputId": "88388170-3e40-451c-88fc-406b7d3a7456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of first element: <class 'list'>\n",
            "First element: [47.23819860017184, -1.517678599864239]\n",
            "First element length: 2\n"
          ]
        }
      ],
      "source": [
        "# Check what's in the geo_point_2d column\n",
        "print(f\"Type of first element: {type(df['geo_point_2d'][0])}\")\n",
        "print(f\"First element: {df['geo_point_2d'][0]}\")\n",
        "print(f\"First element length: {len(df['geo_point_2d'][0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DEDUPLICATION STRATEGIES ===\n",
            "\n",
            "Original rows: 2782\n",
            "After coordinates deduplication: 1644\n",
            "Removed 1138 duplicates\n",
            "\n",
            "Original rows: 2782\n",
            "After address deduplication: 1690\n",
            "Removed 1092 duplicates\n",
            "\n",
            "Original rows: 2782\n",
            "After strict deduplication: 2782\n",
            "Removed 0 duplicates\n",
            "\n",
            "Original shape: (2782, 9)\n",
            "Original rows: 2782\n",
            "After coordinates deduplication: 1644\n",
            "Removed 1138 duplicates\n",
            "\n",
            "Cleaned data shape: (1644, 9)\n",
            "\n",
            "First few rows of cleaned data:\n",
            "                       adresse commune  \\\n",
            "0        2 Rue de la Basinerie  Nantes   \n",
            "1           18 Rue Anita Conti  Nantes   \n",
            "2  276 Route de Sainte de Luce  Nantes   \n",
            "3     288 Route de Sainte Luce  Nantes   \n",
            "4           1 Rue Henri Loiret  Nantes   \n",
            "\n",
            "                                geo_point_2d  \n",
            "0    [47.23819860017184, -1.517678599864239]  \n",
            "1   [47.238733199907195, -1.511478400639151]  \n",
            "2   [47.24174179981523, -1.5074455997902827]  \n",
            "3   [47.24231100038424, -1.5062445999470977]  \n",
            "4  [47.243990000355396, -1.5045800003122118]  \n"
          ]
        }
      ],
      "source": [
        "def clean_duplicates(df, strategy='coordinates'):\n",
        "    \"\"\"\n",
        "    Remove duplicates based on different strategies\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    strategy: 'coordinates', 'address', or 'strict'\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Extract coordinates\n",
        "\n",
        "    df_clean['lat'] = df_clean['geo_point_2d'].apply(lambda x: round(x[0], 6) if isinstance(x, list) else None)\n",
        "    df_clean['lon'] = df_clean['geo_point_2d'].apply(lambda x: round(x[1], 6) if isinstance(x, list) else None)\n",
        "    df_clean['adresse_clean'] = df_clean['adresse'].str.lower().str.strip()\n",
        "\n",
        "    # Choose deduplication strategy\n",
        "\n",
        "    if strategy == 'coordinates':\n",
        "        # Keep first entry for each unique coordinate\n",
        "        df_deduped = df_clean.drop_duplicates(subset=['lat', 'lon'], keep='first')\n",
        "\n",
        "    elif strategy == 'address':\n",
        "        # Keep first entry for each unique address/commune\n",
        "        df_deduped = df_clean.drop_duplicates(subset=['adresse_clean', 'commune'], keep='first')\n",
        "\n",
        "    elif strategy == 'strict':\n",
        "        # Keep first entry for exact matches (excluding geo_point_2d list)\n",
        "        cols = [col for col in df_clean.columns if col not in ['geo_point_2d']]\n",
        "        df_deduped = df_clean.drop_duplicates(subset=cols, keep='first')\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Strategy must be 'coordinates', 'address', or 'strict'\")\n",
        "\n",
        "    # Clean up temporary columns\n",
        "\n",
        "    df_deduped = df_deduped.drop(columns=['lat', 'lon', 'adresse_clean'], errors='ignore')\n",
        "\n",
        "    print(f\"Original rows: {len(df)}\")\n",
        "    print(f\"After {strategy} deduplication: {len(df_deduped)}\")\n",
        "    print(f\"Removed {len(df) - len(df_deduped)} duplicates\")\n",
        "\n",
        "    return df_deduped\n",
        "\n",
        "# Try different strategies\n",
        "print(\"=== DEDUPLICATION STRATEGIES ===\\n\")\n",
        "for strategy in ['coordinates', 'address', 'strict']:\n",
        "    df_clean = clean_duplicates(df, strategy=strategy)\n",
        "    print()\n",
        "\n",
        "# For our use case, it's useful coordinate-based deduplication:\n",
        "\n",
        "print(\"Original shape:\", df.shape)\n",
        "df_clean = clean_duplicates(df, strategy='coordinates')\n",
        "print(\"\\nCleaned data shape:\", df_clean.shape)\n",
        "print(\"\\nFirst few rows of cleaned data:\")\n",
        "print(df_clean[['adresse', 'commune', 'geo_point_2d']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of                    micro_quartier                      adresse    domaine  \\\n",
              "0                     La Botti√®re        2 Rue de la Basinerie     Public   \n",
              "1                     Le Landreau           18 Rue Anita Conti      Priv√©   \n",
              "2                 Rte de Ste Luce  276 Route de Sainte de Luce     Public   \n",
              "3                 Rte de Ste Luce     288 Route de Sainte Luce     Public   \n",
              "4                 Rte de Ste Luce           1 Rue Henri Loiret     Public   \n",
              "...                           ...                          ...        ...   \n",
              "2769          St Jacques - Pirmil          149 rue Bonne Garde     Public   \n",
              "2772  Gde Gr√®neraie - Clos Toreau            12 rue des Herses     Public   \n",
              "2774      St Jacques - Ripossi√®re          35 rue Ledru Rollin     Public   \n",
              "2776  Gde Gr√®neraie - Clos Toreau          11 route de Clisson  Priv√© NMH   \n",
              "2779                     Malakoff       10 rue de l'Angleterre     Public   \n",
              "\n",
              "     identifiant                   date_mes commune fournisseur  \\\n",
              "0       BIOD0582  2022-12-12T01:00:00+00:00  Nantes        SUEZ   \n",
              "1       BIOD0514  2022-12-20T01:00:00+00:00  Nantes        SUEZ   \n",
              "2       BIOD0410  2022-12-01T01:00:00+00:00  Nantes        SUEZ   \n",
              "3       BIOD0503  2022-12-01T01:00:00+00:00  Nantes        SUEZ   \n",
              "4       BIOD0632  2022-12-01T01:00:00+00:00  Nantes        SUEZ   \n",
              "...          ...                        ...     ...         ...   \n",
              "2769    BIOD0958  2023-02-23T01:00:00+00:00  Nantes         NaN   \n",
              "2772    BIOD0990  2023-03-01T01:00:00+00:00  Nantes         NaN   \n",
              "2774    BIOD0996  2023-03-01T01:00:00+00:00  Nantes         NaN   \n",
              "2776    BIOD1012  2023-04-04T02:00:00+00:00  Nantes         NaN   \n",
              "2779    BIOD1032  2023-03-14T01:00:00+00:00  Nantes         NaN   \n",
              "\n",
              "                                   geo_point_2d plaque_id  \n",
              "0       [47.23819860017184, -1.517678599864239]       NaN  \n",
              "1      [47.238733199907195, -1.511478400639151]       NaN  \n",
              "2      [47.24174179981523, -1.5074455997902827]       NaN  \n",
              "3      [47.24231100038424, -1.5062445999470977]       NaN  \n",
              "4     [47.243990000355396, -1.5045800003122118]       NaN  \n",
              "...                                         ...       ...  \n",
              "2769   [47.19609210014152, -1.5359765005384145]       NaN  \n",
              "2772   [47.19401270031509, -1.5297363994331488]       NaN  \n",
              "2774   [47.19376700025901, -1.5355565006657705]       NaN  \n",
              "2776    [47.19326359977757, -1.529158800523084]       NaN  \n",
              "2779   [47.214726300399235, -1.527392400511925]       NaN  \n",
              "\n",
              "[1644 rows x 9 columns]>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_clean.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMJIIm0OgG1X",
        "outputId": "18b2eeb0-8bec-4f60-cda1-d42656019eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully extracted coordinates as [lon, lat]\n"
          ]
        }
      ],
      "source": [
        "# It's a list of two numbers [lon, lat]\n",
        "\n",
        "df_clean['lat'] = df_clean['geo_point_2d'].apply(lambda x: float(x[0]) if isinstance(x, list)\n",
        "and len(x) > 0 else None)\n",
        "\n",
        "df_clean['lon'] = df_clean['geo_point_2d'].apply(lambda x: float(x[1]) if isinstance(x, list)\n",
        "and len(x) > 1 else None)\n",
        "print(\"Successfully extracted coordinates as [lon, lat]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6iWs_wZL263",
        "outputId": "7701b710-db15-4dda-cba4-f1a86d23cb80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame saved as 'alimentary_garbage.csv'\n"
          ]
        }
      ],
      "source": [
        "# Save df with name alimentary garbage (index = False)\n",
        "\n",
        "df_clean.to_csv('alimentary_garbage.csv', index=False)\n",
        "print(\"DataFrame saved as 'alimentary_garbage.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ BigQuery client initialized successfully\n",
            "‚úÖ Dataset 'nantes' exists\n",
            "üìä Original data: 2782 rows\n",
            "üìä Clean data to upload: 1644 rows, 11 columns\n",
            "üìà Duplicates removed: 1138 rows\n",
            "üßπ Cleaning DataFrame for BigQuery...\n",
            "   Renamed columns: {'micro_quartier': 'micro_quartier', 'adresse': 'adresse', 'domaine': 'domaine', 'identifiant': 'identifiant', 'date_mes': 'date_mes', 'commune': 'commune', 'fournisseur': 'fournisseur', 'geo_point_2d': 'geo_point_2d', 'plaque_id': 'plaque_id', 'lat': 'lat', 'lon': 'lon'}\n",
            "   Processing geo_point_2d column...\n",
            "   Extracted coordinates: 1644 valid lat/lon pairs\n",
            "   Dropped original geo_point_2d column (kept string version)\n",
            "   Final columns: ['micro_quartier', 'adresse', 'domaine', 'identifiant', 'date_mes', 'commune', 'fournisseur', 'plaque_id', 'lat', 'lon', 'geo_point_2d_str', 'latitude', 'longitude']\n",
            "\n",
            "üîç Data types after cleaning:\n",
            "micro_quartier       object\n",
            "adresse              object\n",
            "domaine              object\n",
            "identifiant          object\n",
            "date_mes             object\n",
            "commune              object\n",
            "fournisseur          object\n",
            "plaque_id            object\n",
            "lat                 float64\n",
            "lon                 float64\n",
            "geo_point_2d_str     object\n",
            "latitude            float64\n",
            "longitude           float64\n",
            "dtype: object\n",
            "\n",
            "üì§ Converting DataFrame to CSV...\n",
            "   CSV size: 328.42 KB\n",
            "\n",
            "‚¨ÜÔ∏è  Uploading 1644 cleaned rows to trash-optimizer-479913.nantes.alimentary_garbage_clean...\n",
            "   Job submitted. Waiting for completion...\n",
            "\n",
            "üéâ SUCCESS!\n",
            "   Table: trash-optimizer-479913.nantes.alimentary_garbage_clean\n",
            "   Rows uploaded: 1,644\n",
            "   Size: 0.24 MB\n",
            "\n",
            "üìã Table schema:\n",
            "   - micro_quartier: STRING\n",
            "   - adresse: STRING\n",
            "   - domaine: STRING\n",
            "   - identifiant: STRING\n",
            "   - date_mes: TIMESTAMP\n",
            "   - commune: STRING\n",
            "   - fournisseur: STRING\n",
            "   - plaque_id: INTEGER\n",
            "   - lat: FLOAT\n",
            "   - lon: FLOAT\n",
            "   - geo_point_2d_str: STRING\n",
            "   - latitude: FLOAT\n",
            "   - longitude: FLOAT\n"
          ]
        }
      ],
      "source": [
        "# CSV UPLOAD METHOD for df_clean (deduplicated data)\n",
        "# Set credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/dariaserbichenko/code/DariaSerb/key-gcp/trash-optimizer-479913-91e59ecc96c9.json\"\n",
        "\n",
        "# BigQuery setup\n",
        "PROJECT = \"trash-optimizer-479913\"\n",
        "DATASET = \"nantes\"\n",
        "TABLE = \"alimentary_garbage_clean\"  # Changed table name to distinguish from original\n",
        "table_id = f\"{PROJECT}.{DATASET}.{TABLE}\"\n",
        "\n",
        "# Initialize client\n",
        "client = bigquery.Client(project=PROJECT)\n",
        "print(f\"‚úÖ BigQuery client initialized successfully\")\n",
        "\n",
        "# Check dataset\n",
        "dataset_ref = f\"{PROJECT}.{DATASET}\"\n",
        "try:\n",
        "    dataset = client.get_dataset(dataset_ref)\n",
        "    print(f\"‚úÖ Dataset '{DATASET}' exists\")\n",
        "except NotFound:\n",
        "    print(f\"üìÅ Creating dataset '{DATASET}'...\")\n",
        "    dataset = bigquery.Dataset(dataset_ref)\n",
        "    dataset.location = \"EU\"\n",
        "    dataset = client.create_dataset(dataset, timeout=30)\n",
        "    print(f\"‚úÖ Dataset created\")\n",
        "\n",
        "print(f\"üìä Original data: {len(df)} rows\")\n",
        "print(f\"üìä Clean data to upload: {len(df_clean)} rows, {len(df_clean.columns)} columns\")\n",
        "print(f\"üìà Duplicates removed: {len(df) - len(df_clean)} rows\")\n",
        "\n",
        "# Clean data (updated for df_clean's structure)\n",
        "def clean_dataframe_for_bq(df_input):\n",
        "    \"\"\"Basic cleaning for BigQuery, preserving coordinate structure\"\"\"\n",
        "    df_clean_bq = df_input.copy()\n",
        "\n",
        "    print(\"üßπ Cleaning DataFrame for BigQuery...\")\n",
        "\n",
        "    # 1. Fix column names\n",
        "    original_cols = df_clean_bq.columns.tolist()\n",
        "    df_clean_bq.columns = [str(col).replace(' ', '_').replace('-', '_').replace('.', '_').lower()\n",
        "                          for col in df_clean_bq.columns]\n",
        "\n",
        "    print(f\"   Renamed columns: {dict(zip(original_cols, df_clean_bq.columns))}\")\n",
        "\n",
        "    # 2. Handle geo_point_2d - keep as string or extract coordinates\n",
        "    if 'geo_point_2d' in df_clean_bq.columns:\n",
        "        print(\"   Processing geo_point_2d column...\")\n",
        "\n",
        "        # Option 1: Keep as string (if you want to preserve the list structure as text)\n",
        "        df_clean_bq['geo_point_2d_str'] = df_clean_bq['geo_point_2d'].astype(str)\n",
        "\n",
        "        # Option 2: Extract latitude and longitude as separate columns\n",
        "        try:\n",
        "            df_clean_bq['latitude'] = df_clean_bq['geo_point_2d'].apply(\n",
        "                lambda x: float(x[0]) if isinstance(x, list) and len(x) > 0 else None\n",
        "            )\n",
        "            df_clean_bq['longitude'] = df_clean_bq['geo_point_2d'].apply(\n",
        "                lambda x: float(x[1]) if isinstance(x, list) and len(x) > 1 else None\n",
        "            )\n",
        "            print(f\"   Extracted coordinates: {df_clean_bq['latitude'].notna().sum()} valid lat/lon pairs\")\n",
        "        except Exception as e:\n",
        "            print(f\"   Warning: Could not extract coordinates: {e}\")\n",
        "\n",
        "    # 3. Convert other lists/dicts to strings\n",
        "    for col in df_clean_bq.columns:\n",
        "        if col != 'geo_point_2d':  # Skip the original geo_point_2d\n",
        "            if df_clean_bq[col].apply(lambda x: isinstance(x, (list, dict, tuple))).any():\n",
        "                df_clean_bq[col] = df_clean_bq[col].astype(str)\n",
        "                print(f\"   Converted {col} to string (contains lists/dicts)\")\n",
        "\n",
        "    # 4. Fill NaN values\n",
        "    for col in df_clean_bq.columns:\n",
        "        if df_clean_bq[col].dtype == 'object':\n",
        "            df_clean_bq[col] = df_clean_bq[col].fillna('')\n",
        "        elif pd.api.types.is_numeric_dtype(df_clean_bq[col]):\n",
        "            # For numeric columns, you might want to keep NaN or fill with 0\n",
        "            # df_clean_bq[col] = df_clean_bq[col].fillna(0)  # Uncomment if needed\n",
        "            pass\n",
        "\n",
        "    # 5. Remove the original list column if we created string version\n",
        "    if 'geo_point_2d' in df_clean_bq.columns and 'geo_point_2d_str' in df_clean_bq.columns:\n",
        "        df_clean_bq = df_clean_bq.drop(columns=['geo_point_2d'])\n",
        "        print(\"   Dropped original geo_point_2d column (kept string version)\")\n",
        "\n",
        "    print(f\"   Final columns: {list(df_clean_bq.columns)}\")\n",
        "\n",
        "    return df_clean_bq\n",
        "\n",
        "# Apply cleaning\n",
        "df_bq_ready = clean_dataframe_for_bq(df_clean)\n",
        "\n",
        "# Check data types\n",
        "print(\"\\nüîç Data types after cleaning:\")\n",
        "print(df_bq_ready.dtypes)\n",
        "\n",
        "# Convert DataFrame to CSV in memory\n",
        "print(\"\\nüì§ Converting DataFrame to CSV...\")\n",
        "csv_buffer = io.StringIO()\n",
        "df_bq_ready.to_csv(csv_buffer, index=False, encoding='utf-8')\n",
        "csv_content = csv_buffer.getvalue().encode('utf-8')\n",
        "\n",
        "print(f\"   CSV size: {len(csv_content) / 1024:.2f} KB\")\n",
        "\n",
        "# Create job configuration\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_TRUNCATE\",  # Replaces entire table\n",
        "    autodetect=True,  # Let BigQuery detect schema\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,  # Skip header\n",
        "    max_bad_records=100,  # Allow some bad records\n",
        "    encoding='UTF-8',\n",
        "    allow_quoted_newlines=True  # Important for text fields\n",
        ")\n",
        "\n",
        "print(f\"\\n‚¨ÜÔ∏è  Uploading {len(df_bq_ready)} cleaned rows to {table_id}...\")\n",
        "\n",
        "# Upload from CSV\n",
        "try:\n",
        "    # Create file-like object\n",
        "    file_obj = io.BytesIO(csv_content)\n",
        "\n",
        "    # Submit job\n",
        "    job = client.load_table_from_file(\n",
        "        file_obj,\n",
        "        table_id,\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(\"   Job submitted. Waiting for completion...\")\n",
        "    job.result()  # Wait for completion\n",
        "\n",
        "    # Verify\n",
        "    table = client.get_table(table_id)\n",
        "    print(f\"\\nüéâ SUCCESS!\")\n",
        "    print(f\"   Table: {table_id}\")\n",
        "    print(f\"   Rows uploaded: {table.num_rows:,}\")\n",
        "    print(f\"   Size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
        "\n",
        "    # Show table schema\n",
        "    print(f\"\\nüìã Table schema:\")\n",
        "    for field in table.schema:\n",
        "        print(f\"   - {field.name}: {field.field_type}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Upload failed: {e}\")\n",
        "    print(\"Trying alternative approach...\")\n",
        "\n",
        "    # Alternative: Use pandas_gbq for better error handling\n",
        "    try:\n",
        "        print(\"Trying pandas_gbq...\")\n",
        "        import pandas_gbq\n",
        "\n",
        "        df_bq_ready.to_gbq(\n",
        "            destination_table=table_id,\n",
        "            project_id=PROJECT,\n",
        "            if_exists='replace',\n",
        "            progress_bar=True\n",
        "        )\n",
        "        print(\"‚úÖ Success with pandas_gbq!\")\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå Also failed: {e2}\")\n",
        "\n",
        "        # Last resort: Save to local CSV and inspect\n",
        "        local_csv = \"debug_cleaned_data.csv\"\n",
        "        df_bq_ready.to_csv(local_csv, index=False, encoding='utf-8')\n",
        "        print(f\"üìÅ Saved data to {local_csv} for debugging\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy5I5ukQvZGb",
        "outputId": "1a212ea5-4224-440a-8992-deeca780e3c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records retrieved: 15\n"
          ]
        }
      ],
      "source": [
        "# D√©ch√®teries-√©copoints de Nantes M√©tropoles\n",
        "\n",
        "BASE_URL = \"https://data.nantesmetropole.fr/api/explore/v2.1/catalog/datasets/244400404_decheteries-ecopoints-nantes-metropole/records\"\n",
        "\n",
        "# To get all records, we use limit=-1\n",
        "params = {\"limit\": -1}\n",
        "\n",
        "response = requests.get(BASE_URL, params=params)\n",
        "response.raise_for_status()\n",
        "data = response.json()\n",
        "\n",
        "records = data.get('results', [])\n",
        "print(f\"Total records retrieved: {len(records)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ofV5tIDjt53Q"
      },
      "outputs": [],
      "source": [
        "df1 = pd.DataFrame(response.json()['results'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "974i7SN10Kt-",
        "outputId": "388a9e9d-f862-4536-d63e-e59d934e518a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   identifiant                                    nom        type code_postal  \\\n",
            "0         1015                       Ecopoint Auvours    Ecopoint       44000   \n",
            "1         2152          D√©ch√®terie de Saint S√©bastien  D√©ch√®terie       44230   \n",
            "2         3139                D√©ch√®terie de Carquefou  D√©ch√®terie       44470   \n",
            "3         3144  D√©ch√®terie de Saint Aignan Grand Lieu  D√©ch√®terie       44860   \n",
            "4         1012                     Ecopoint Chantenay    Ecopoint       44100   \n",
            "\n",
            "                     commune                              adresse bois carton  \\\n",
            "0                     Nantes                    20 Rue du Bourget  oui    oui   \n",
            "1  Saint-S√©bastien-sur-Loire                   Rue de la Pyramide  oui    oui   \n",
            "2                  Carquefou                    Route du Prouzeau  oui    oui   \n",
            "3     Saint-Aignan-Grandlieu                    Route de la For√™t  oui    oui   \n",
            "4                     Nantes  42 Boulevard Mar√©chal Alphonse Juin  oui    oui   \n",
            "\n",
            "  deee pneus  ... pile cartouche neon dechet_dangereux bouteille_gaz  \\\n",
            "0  oui   non  ...  oui       oui  oui              oui           non   \n",
            "1  oui   non  ...  oui       oui  oui              oui           non   \n",
            "2  oui   non  ...  oui       oui  oui              oui           non   \n",
            "3  oui   non  ...  non       oui  non              oui           non   \n",
            "4  oui   oui  ...  oui       oui  oui              oui           non   \n",
            "\n",
            "  polystyrene huile_alimentaire ressourcerie           horaire_ressourcerie  \\\n",
            "0         oui              None          oui  Mardi au samedi 12h15 √† 17h15   \n",
            "1         oui              None          oui    Mardi au samedi 11h √† 17h45   \n",
            "2         non              None          oui   Mardi √† vendredi 10h √† 17h45   \n",
            "3         non              None          oui     Lu ma me ve sa 10h √† 17h45   \n",
            "4         non              None          non                           None   \n",
            "\n",
            "                                        geo_point_2d  \n",
            "0  {'lon': -1.564511605134781, 'lat': 47.22263310...  \n",
            "1  {'lon': -1.4837289231537751, 'lat': 47.1992747...  \n",
            "2  {'lon': -1.4411134327679573, 'lat': 47.2903598...  \n",
            "3  {'lon': -1.6236670419276427, 'lat': 47.1495463...  \n",
            "4  {'lon': -1.6017517875229923, 'lat': 47.1984376...  \n",
            "\n",
            "[5 rows x 32 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df1.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['identifiant', 'nom', 'type', 'code_postal', 'commune', 'adresse',\n",
            "       'bois', 'carton', 'deee', 'pneus', 'verre', 'mobilier', 'extincteur',\n",
            "       'batterie', 'gravat', 'encombrant', 'ferraille', 'huile_moteur',\n",
            "       'papier', 'placoplatre', 'textile', 'dechet_vert', 'pile', 'cartouche',\n",
            "       'neon', 'dechet_dangereux', 'bouteille_gaz', 'polystyrene',\n",
            "       'huile_alimentaire', 'ressourcerie', 'horaire_ressourcerie',\n",
            "       'geo_point_2d'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df1.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking ID uniqueness (identifiant column):\n",
            "Total IDs: 15\n",
            "Unique IDs: 15\n",
            "Duplicate IDs: 0\n"
          ]
        }
      ],
      "source": [
        "# The MOST IMPORTANT check - IDs should be unique\n",
        "\n",
        "print(\"Checking ID uniqueness (identifiant column):\")\n",
        "\n",
        "if 'identifiant' in df1.columns:\n",
        "    total_ids = len(df1['identifiant'])\n",
        "    unique_ids = df1['identifiant'].nunique()\n",
        "    duplicate_id_count = df1['identifiant'].duplicated().sum()\n",
        "\n",
        "    print(f\"Total IDs: {total_ids}\")\n",
        "    print(f\"Unique IDs: {unique_ids}\")\n",
        "    print(f\"Duplicate IDs: {duplicate_id_count}\")\n",
        "\n",
        "    if duplicate_id_count > 0:\n",
        "        print(\"DUPLICATE IDs FOUND:\")\n",
        "        dup_ids = df1[df1['identifiant'].duplicated(keep=False)]\n",
        "        for id_val in dup_ids['identifiant'].unique():\n",
        "            id_rows = dup_ids[dup_ids['identifiant'] == id_val]\n",
        "            print(f\"\\nID {id_val} appears {len(id_rows)} times:\")\n",
        "            for _, row in id_rows.iterrows():\n",
        "                print(f\"  - {row.get('nom', 'Unknown')} | {row.get('adresse', 'No address')}\")\n",
        "else:\n",
        "    print(\"No 'identifiant' column found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suEb21n00f2j",
        "outputId": "5f2ce784-4522-4a32-88bd-6de020abb76c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully extracted coordinates for 15 rows\n"
          ]
        }
      ],
      "source": [
        "# Extract coordinates from dictionaries\n",
        "\n",
        "df1['lon'] = df1['geo_point_2d'].apply(\n",
        "    lambda x: float(x['lon']) if isinstance(x, dict) and 'lon' in x else None\n",
        ")\n",
        "df1['lat'] = df1['geo_point_2d'].apply(\n",
        "    lambda x: float(x['lat']) if isinstance(x, dict) and 'lat' in x else None\n",
        ")\n",
        "\n",
        "print(f\"Successfully extracted coordinates for {df1['lon'].notna().sum()} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDqrQf9mup4c",
        "outputId": "c08fea15-6686-4f44-dd99-1ae5c5e09cdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame saved as 'ecopoints.csv'\n"
          ]
        }
      ],
      "source": [
        "# Save df with name ecopoints (index = False)\n",
        "\n",
        "df1.to_csv('ecopoints.csv', index=False)\n",
        "print(\"DataFrame saved as 'ecopoints.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ BigQuery client initialized successfully\n",
            "‚úÖ Dataset 'nantes' exists\n",
            "üìä Data to upload: 15 rows, 34 columns\n",
            "   Converting geo_point_2d to string\n",
            "Converting DataFrame to CSV...\n",
            "‚¨ÜÔ∏è  Uploading 1644 rows...\n",
            "   Job submitted. Waiting...\n",
            "\n",
            "‚úÖ SUCCESS!\n",
            "   Table: trash-optimizer-479913.nantes.ecopoints\n",
            "   Rows uploaded: 15\n",
            "   Size: 0.00 MB\n"
          ]
        }
      ],
      "source": [
        "# CSV UPLOAD METHOD\n",
        "# Set credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/dariaserbichenko/code/DariaSerb/key-gcp/trash-optimizer-479913-91e59ecc96c9.json\"\n",
        "\n",
        "# BigQuery setup\n",
        "PROJECT = \"trash-optimizer-479913\"\n",
        "DATASET = \"nantes\"\n",
        "TABLE = \"ecopoints\"\n",
        "table_id = f\"{PROJECT}.{DATASET}.{TABLE}\"\n",
        "\n",
        "# Initialize client\n",
        "client = bigquery.Client(project=PROJECT)\n",
        "print(f\"‚úÖ BigQuery client initialized successfully\")\n",
        "\n",
        "# Check dataset\n",
        "dataset_ref = f\"{PROJECT}.{DATASET}\"\n",
        "try:\n",
        "    dataset = client.get_dataset(dataset_ref)\n",
        "    print(f\"‚úÖ Dataset '{DATASET}' exists\")\n",
        "except NotFound:\n",
        "    print(f\"üìÅ Creating dataset '{DATASET}'...\")\n",
        "    dataset = bigquery.Dataset(dataset_ref)\n",
        "    dataset.location = \"EU\"\n",
        "    dataset = client.create_dataset(dataset, timeout=30)\n",
        "    print(f\"‚úÖ Dataset created\")\n",
        "\n",
        "print(f\"üìä Data to upload: {len(df1)} rows, {len(df1.columns)} columns\")\n",
        "\n",
        "# Prepare DataFrame - ensure no lists/dicts\n",
        "df1_clean = df1.copy()\n",
        "for col in df1_clean.columns:\n",
        "    if df1_clean[col].apply(lambda x: isinstance(x, (list, dict))).any():\n",
        "        print(f\"   Converting {col} to string\")\n",
        "        df1_clean[col] = df1_clean[col].astype(str)\n",
        "\n",
        "# Convert DataFrame to CSV in memory\n",
        "print(\"Converting DataFrame to CSV...\")\n",
        "csv_buffer = io.StringIO()\n",
        "df1_clean.to_csv(csv_buffer, index=False, encoding='utf-8')\n",
        "csv_content = csv_buffer.getvalue().encode('utf-8')\n",
        "\n",
        "# Create job configuration\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_TRUNCATE\",\n",
        "    autodetect=True,\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,  # Skip header\n",
        "    max_bad_records=100,\n",
        "    encoding='UTF-8'\n",
        ")\n",
        "\n",
        "print(f\"‚¨ÜÔ∏è  Uploading {len(df_clean)} rows...\")\n",
        "\n",
        "# Upload from CSV\n",
        "try:\n",
        "    # Create file-like object\n",
        "    file_obj = io.BytesIO(csv_content)\n",
        "\n",
        "    # Submit job\n",
        "    job = client.load_table_from_file(\n",
        "        file_obj,\n",
        "        table_id,\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(\"   Job submitted. Waiting...\")\n",
        "    job.result()  # Wait for completion\n",
        "\n",
        "    # Verify\n",
        "    table = client.get_table(table_id)\n",
        "    print(f\"\\n‚úÖ SUCCESS!\")\n",
        "    print(f\"   Table: {table_id}\")\n",
        "    print(f\"   Rows uploaded: {table.num_rows:,}\")\n",
        "    print(f\"   Size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Upload failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading CSV data...\n",
            "Dataset loaded: 328 records\n",
            "\n",
            "Columns: ['C_REGION', 'L_REGION', 'C_DEPT', 'N_DEPT', 'ANNEE', 'C_SERVICE', 'N_SERVICE', 'TEL_SERVICE', 'D_OUV', 'AD1_SITE', 'AD2_SITE', 'insee_commune_actuel', 'N_COMM_SITE', 'CP_SITE', 'epci', 'LOV_MO_GEST', 'GPS_Y', 'GPS_X', 'C_ACTEUR', 'N_ACTEUR', 'L_TYP_ACTEUR', 'AD1_ACTEUR', 'AD2_ACTEUR', 'CP_ACTEUR', 'L_VILLE_ACTEUR', 'TEL_ACTEUR', 'position', 'D_MODIF', 'ORIGINE_DECHET_ACC', 'GPS_LONG', 'GPS_LAT']\n",
            "\n",
            "First few rows:\n",
            "   C_REGION          L_REGION  C_DEPT            N_DEPT  ANNEE  C_SERVICE  \\\n",
            "0        52  Pays de la Loire      44  Loire-Atlantique   2025       2459   \n",
            "1        52  Pays de la Loire      44  Loire-Atlantique   2025       2494   \n",
            "2        52  Pays de la Loire      44  Loire-Atlantique   2025       2500   \n",
            "3        52  Pays de la Loire      44  Loire-Atlantique   2025       2508   \n",
            "4        52  Pays de la Loire      44  Loire-Atlantique   2025       5163   \n",
            "\n",
            "                              N_SERVICE  TEL_SERVICE    D_OUV  \\\n",
            "0  D√©ch√®terie de Saint-nazaire (cuneix)  810110570.0  1986-05   \n",
            "1                D√©ch√®terie de la Baule  240608566.0  1992-01   \n",
            "2       D√©ch√®terie d'Herbignac (pompas)  240913606.0  2001-01   \n",
            "3               D√©ch√®terie de Missillac          NaN  1997-03   \n",
            "4                  D√©ch√®terie d'Avessac  299703434.0  1996-05   \n",
            "\n",
            "              AD1_SITE  ...              AD1_ACTEUR  AD2_ACTEUR CP_ACTEUR  \\\n",
            "0  Les hauts de cuneix  ...              BAS CUNEIX         NaN   44600.0   \n",
            "1      Route du Rocher  ...            3 Rue Launay         NaN   44800.0   \n",
            "2  Lieu-dit les Pompas  ...   3, Avenue des No√´lles       Bp 64   44500.0   \n",
            "3      Za la Pommeraie  ...  2 Rue des Ch√¢taigniers         NaN   44160.0   \n",
            "4      Zone Artisanale  ...  3, Rue Charles Sillard    Cs 40264   35600.0   \n",
            "\n",
            "    L_VILLE_ACTEUR   TEL_ACTEUR          position     D_MODIF  \\\n",
            "0    SAINT-NAZAIRE  240225097.0   47.304,-2.18746  2025-10-15   \n",
            "1   SAINT-HERBLAIN  240952019.0  47.2927,-2.38185  2025-10-02   \n",
            "2  Baule-escoublac  251769616.0  47.4124,-2.34552  2025-10-01   \n",
            "3      Pontch√¢teau  240450530.0  47.4697,-2.15465  2025-04-29   \n",
            "4            Redon  299703434.0  47.6466,-1.99769  2025-07-22   \n",
            "\n",
            "   ORIGINE_DECHET_ACC  GPS_LONG  GPS_LAT  \n",
            "0             DMA/PRO  -2.18746  47.3040  \n",
            "1             DMA/PRO  -2.38185  47.2927  \n",
            "2             DMA/PRO  -2.34552  47.4124  \n",
            "3             DMA/PRO  -2.15465  47.4697  \n",
            "4             DMA/PRO  -1.99769  47.6466  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "\n",
        "# Annuaire des d√©ch√®teries des d√©chets m√©nagers et assimil√©s en Pays de la Loire\n",
        "# Direct CSV download URL from the web page\n",
        "CSV_URL = \"https://data.nantesmetropole.fr/explore/dataset/837810944_annuairedesdecheteriesdma_pdl@data-teo-paysdelaloire/download/?format=csv&timezone=Europe/Berlin&lang=fr&use_labels_for_header=true&csv_separator=%3B\"\n",
        "\n",
        "# Download the CSV\n",
        "print(\"Downloading CSV data...\")\n",
        "response = requests.get(CSV_URL)\n",
        "response.raise_for_status()\n",
        "\n",
        "# Read CSV directly from the response content\n",
        "df2 = pd.read_csv(io.StringIO(response.content.decode('utf-8')), sep=';')\n",
        "print(f\"Dataset loaded: {len(df2)} records\")\n",
        "print(\"\\nColumns:\", list(df2.columns))\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df2.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 328 entries, 0 to 327\n",
            "Data columns (total 31 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   C_REGION              328 non-null    int64  \n",
            " 1   L_REGION              328 non-null    object \n",
            " 2   C_DEPT                328 non-null    int64  \n",
            " 3   N_DEPT                328 non-null    object \n",
            " 4   ANNEE                 328 non-null    int64  \n",
            " 5   C_SERVICE             328 non-null    int64  \n",
            " 6   N_SERVICE             328 non-null    object \n",
            " 7   TEL_SERVICE           258 non-null    float64\n",
            " 8   D_OUV                 328 non-null    object \n",
            " 9   AD1_SITE              328 non-null    object \n",
            " 10  AD2_SITE              63 non-null     object \n",
            " 11  insee_commune_actuel  327 non-null    float64\n",
            " 12  N_COMM_SITE           327 non-null    object \n",
            " 13  CP_SITE               327 non-null    float64\n",
            " 14  epci                  325 non-null    object \n",
            " 15  LOV_MO_GEST           328 non-null    object \n",
            " 16  GPS_Y                 328 non-null    float64\n",
            " 17  GPS_X                 328 non-null    float64\n",
            " 18  C_ACTEUR              324 non-null    float64\n",
            " 19  N_ACTEUR              324 non-null    object \n",
            " 20  L_TYP_ACTEUR          324 non-null    object \n",
            " 21  AD1_ACTEUR            324 non-null    object \n",
            " 22  AD2_ACTEUR            171 non-null    object \n",
            " 23  CP_ACTEUR             324 non-null    float64\n",
            " 24  L_VILLE_ACTEUR        324 non-null    object \n",
            " 25  TEL_ACTEUR            317 non-null    float64\n",
            " 26  position              328 non-null    object \n",
            " 27  D_MODIF               328 non-null    object \n",
            " 28  ORIGINE_DECHET_ACC    328 non-null    object \n",
            " 29  GPS_LONG              328 non-null    float64\n",
            " 30  GPS_LAT               328 non-null    float64\n",
            "dtypes: float64(10), int64(4), object(17)\n",
            "memory usage: 79.6+ KB\n"
          ]
        }
      ],
      "source": [
        "df2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUICK df2 DUPLICATE CHECK\n",
            "Total rows: 328\n",
            "Exact duplicates: 0\n",
            "Unique rows: 328\n",
            "Key column duplicates:\n",
            "  C_SERVICE: 0 duplicates (328 unique values)\n",
            "  N_SERVICE: 0 duplicates (328 unique values)\n",
            "  AD1_SITE: 9 duplicates (319 unique values)\n",
            "  GPS_LAT: 0 duplicates (328 unique values)\n",
            "  GPS_LONG: 0 duplicates (328 unique values)\n"
          ]
        }
      ],
      "source": [
        "print(\"QUICK df2 DUPLICATE CHECK\")\n",
        "\n",
        "# Simple exact duplicate check\n",
        "print(f\"Total rows: {len(df2)}\")\n",
        "print(f\"Exact duplicates: {df2.duplicated().sum()}\")\n",
        "print(f\"Unique rows: {df2.drop_duplicates().shape[0]}\")\n",
        "\n",
        "# Check key columns\n",
        "print(\"Key column duplicates:\")\n",
        "key_columns = ['C_SERVICE', 'N_SERVICE', 'AD1_SITE', 'GPS_LAT', 'GPS_LONG']\n",
        "for col in key_columns:\n",
        "    if col in df2.columns:\n",
        "        dup_count = df2[col].duplicated().sum()\n",
        "        unique_count = df2[col].nunique()\n",
        "        print(f\"  {col}: {dup_count} duplicates ({unique_count} unique values)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully extracted coordinates for 328 rows\n"
          ]
        }
      ],
      "source": [
        "df2[['lat', 'lon']] = df2['position'].str.split(',', expand=True).astype(float)\n",
        "\n",
        "print(f\"Successfully extracted coordinates for {df2['lon'].notna().sum()} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of      C_REGION          L_REGION  C_DEPT            N_DEPT  ANNEE  C_SERVICE  \\\n",
              "0          52  Pays de la Loire      44  Loire-Atlantique   2025       2459   \n",
              "1          52  Pays de la Loire      44  Loire-Atlantique   2025       2494   \n",
              "2          52  Pays de la Loire      44  Loire-Atlantique   2025       2500   \n",
              "3          52  Pays de la Loire      44  Loire-Atlantique   2025       2508   \n",
              "4          52  Pays de la Loire      44  Loire-Atlantique   2025       5163   \n",
              "..        ...               ...     ...               ...    ...        ...   \n",
              "323        52  Pays de la Loire      72            Sarthe   2025      65448   \n",
              "324        52  Pays de la Loire      85            Vend√©e   2025       4040   \n",
              "325        52  Pays de la Loire      85            Vend√©e   2025       4386   \n",
              "326        52  Pays de la Loire      85            Vend√©e   2025       4757   \n",
              "327        52  Pays de la Loire      85            Vend√©e   2025     118549   \n",
              "\n",
              "                                N_SERVICE  TEL_SERVICE    D_OUV  \\\n",
              "0    D√©ch√®terie de Saint-nazaire (cuneix)  810110570.0  1986-05   \n",
              "1                  D√©ch√®terie de la Baule  240608566.0  1992-01   \n",
              "2         D√©ch√®terie d'Herbignac (pompas)  240913606.0  2001-01   \n",
              "3                 D√©ch√®terie de Missillac          NaN  1997-03   \n",
              "4                    D√©ch√®terie d'Avessac  299703434.0  1996-05   \n",
              "..                                    ...          ...      ...   \n",
              "323       D√©ch√®terie Saint-denis-d'orques  243390162.0  2008-05   \n",
              "324        D√©ch√®terie de La-faute-sur-mer  251280511.0  2000-12   \n",
              "325          D√©ch√®terie de l'Ile d'Olonne  251238605.0  1998-01   \n",
              "326        D√©ch√®terie de Beauvoir-sur-mer  251490674.0  2000-03   \n",
              "327                   D√©ch√®terie Martinet  251059449.0  2021-11   \n",
              "\n",
              "                      AD1_SITE  ... CP_ACTEUR       L_VILLE_ACTEUR  \\\n",
              "0          Les hauts de cuneix  ...   44600.0        SAINT-NAZAIRE   \n",
              "1              Route du Rocher  ...   44800.0       SAINT-HERBLAIN   \n",
              "2          Lieu-dit les Pompas  ...   44500.0      Baule-escoublac   \n",
              "3              Za la Pommeraie  ...   44160.0          Pontch√¢teau   \n",
              "4              Zone Artisanale  ...   35600.0                Redon   \n",
              "..                         ...  ...       ...                  ...   \n",
              "323              Bd De la gare  ...   72300.0     SABLE-SUR-SARTHE   \n",
              "324  Rte De la tranche sur mer  ...   85400.0                Lu√ßon   \n",
              "325               Les demeries  ...   85100.0  les Sables-d'olonne   \n",
              "326                 Za Du daim  ...   85160.0  Saint-jean-de-monts   \n",
              "327           Za Les echoignes  ...   85150.0   la Chapelle-achard   \n",
              "\n",
              "      TEL_ACTEUR           position     D_MODIF ORIGINE_DECHET_ACC  GPS_LONG  \\\n",
              "0    240225097.0    47.304,-2.18746  2025-10-15            DMA/PRO -2.187460   \n",
              "1    240952019.0   47.2927,-2.38185  2025-10-02            DMA/PRO -2.381850   \n",
              "2    251769616.0   47.4124,-2.34552  2025-10-01            DMA/PRO -2.345520   \n",
              "3    240450530.0   47.4697,-2.15465  2025-04-29            DMA/PRO -2.154650   \n",
              "4    299703434.0   47.6466,-1.99769  2025-07-22            DMA/PRO -1.997690   \n",
              "..           ...                ...         ...                ...       ...   \n",
              "323  243920811.0  48.0262,-0.266819  2025-09-04            DMA/PRO -0.266819   \n",
              "324  251976464.0   46.3452,-1.34327  2025-09-09            DMA/PRO -1.343270   \n",
              "325  251238605.0   46.5741,-1.77303  2025-09-09            DMA/PRO -1.773030   \n",
              "326  251590666.0   46.9246,-2.04198  2025-09-09            DMA/PRO -2.041980   \n",
              "327  251059449.0   46.6762,-1.67246  2025-09-09            DMA/PRO -1.672460   \n",
              "\n",
              "     GPS_LAT      lat       lon  \n",
              "0    47.3040  47.3040 -2.187460  \n",
              "1    47.2927  47.2927 -2.381850  \n",
              "2    47.4124  47.4124 -2.345520  \n",
              "3    47.4697  47.4697 -2.154650  \n",
              "4    47.6466  47.6466 -1.997690  \n",
              "..       ...      ...       ...  \n",
              "323  48.0262  48.0262 -0.266819  \n",
              "324  46.3452  46.3452 -1.343270  \n",
              "325  46.5741  46.5741 -1.773030  \n",
              "326  46.9246  46.9246 -2.041980  \n",
              "327  46.6762  46.6762 -1.672460  \n",
              "\n",
              "[328 rows x 33 columns]>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame saved as 'collection_centres_PdL_region.csv'\n"
          ]
        }
      ],
      "source": [
        "# Save df with name collection centres Pays dde la Loire region (index = False)\n",
        "\n",
        "df2.to_csv('collection_centres_PdL_region.csv', index=False)\n",
        "print(\"DataFrame saved as 'collection_centres_PdL_region.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "UPLOADING TO BIGQUERY\n",
            "============================================================\n",
            "‚úÖ BigQuery client initialized successfully\n",
            "‚úÖ Dataset 'nantes' exists\n",
            "   Location: EU\n",
            "\n",
            "üìä Data to upload:\n",
            "   Rows: 328\n",
            "   Columns: 33\n",
            "   Columns: ['C_REGION', 'L_REGION', 'C_DEPT', 'N_DEPT', 'ANNEE', 'C_SERVICE', 'N_SERVICE', 'TEL_SERVICE', 'D_OUV', 'AD1_SITE', 'AD2_SITE', 'insee_commune_actuel', 'N_COMM_SITE', 'CP_SITE', 'epci', 'LOV_MO_GEST', 'GPS_Y', 'GPS_X', 'C_ACTEUR', 'N_ACTEUR', 'L_TYP_ACTEUR', 'AD1_ACTEUR', 'AD2_ACTEUR', 'CP_ACTEUR', 'L_VILLE_ACTEUR', 'TEL_ACTEUR', 'position', 'D_MODIF', 'ORIGINE_DECHET_ACC', 'GPS_LONG', 'GPS_LAT', 'lat', 'lon']\n",
            "\n",
            "üßπ Cleaning data for BigQuery...\n",
            "   Found 533 NaN values\n",
            "   Cleaned shape: (328, 33)\n",
            "\n",
            "üìÑ Converting DataFrame to CSV in memory...\n",
            "\n",
            "‚¨ÜÔ∏è  Uploading 328 rows to table 'collection_centres_pdl'...\n",
            "   Job submitted. Waiting for completion...\n",
            "\n",
            "‚úÖ SUCCESS!\n",
            "   Table: trash-optimizer-479913.nantes.collection_centres_pdl\n",
            "   Rows uploaded: 328\n",
            "   Table size: 0.13 MB\n",
            "   Created: 2025-12-02 12:49:35\n",
            "\n",
            "üìê Schema preview (first 5 columns):\n",
            "   1. C_REGION             : INTEGER\n",
            "   2. L_REGION             : STRING\n",
            "   3. C_DEPT               : INTEGER\n",
            "   4. N_DEPT               : STRING\n",
            "   5. ANNEE                : INTEGER\n",
            "   ... and 28 more columns\n",
            "Tables in dataset 'nantes':\n",
            "   ‚Ä¢ alimentary_garbage\n",
            "   ‚Ä¢ alimentary_garbage_clean\n",
            "   ‚Ä¢ all_trash_locations\n",
            "   ‚úÖ collection_centres_pdl (just uploaded)\n",
            "   ‚Ä¢ ecopoints\n",
            "   ‚Ä¢ trash_collection_points\n"
          ]
        }
      ],
      "source": [
        "# CSV UPLOAD METHOD TO BIGQUERY\n",
        "# Set credentials\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/dariaserbichenko/code/DariaSerb/key-gcp/trash-optimizer-479913-91e59ecc96c9.json\"\n",
        "\n",
        "# BigQuery setup\n",
        "PROJECT = \"trash-optimizer-479913\"\n",
        "DATASET = \"nantes\"\n",
        "TABLE = \"collection_centres_pdl\"\n",
        "table_id = f\"{PROJECT}.{DATASET}.{TABLE}\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"UPLOADING TO BIGQUERY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize client\n",
        "client = bigquery.Client(project=PROJECT)\n",
        "print(f\"‚úÖ BigQuery client initialized successfully\")\n",
        "\n",
        "# Check dataset\n",
        "dataset_ref = f\"{PROJECT}.{DATASET}\"\n",
        "try:\n",
        "    dataset = client.get_dataset(dataset_ref)\n",
        "    print(f\"‚úÖ Dataset '{DATASET}' exists\")\n",
        "    print(f\"   Location: {dataset.location}\")\n",
        "except NotFound:\n",
        "    print(f\"üìÅ Creating dataset '{DATASET}'...\")\n",
        "    dataset = bigquery.Dataset(dataset_ref)\n",
        "    dataset.location = \"EU\"\n",
        "    dataset = client.create_dataset(dataset, timeout=30)\n",
        "    print(f\"‚úÖ Dataset created\")\n",
        "    print(f\"   Location: {dataset.location}\")\n",
        "\n",
        "# Display DataFrame info\n",
        "print(f\"\\nüìä Data to upload:\")\n",
        "print(f\"   Rows: {len(df2):,}\")\n",
        "print(f\"   Columns: {len(df2.columns)}\")\n",
        "print(f\"   Columns: {list(df2.columns)}\")\n",
        "\n",
        "# Check if we need to split the position column\n",
        "if 'position' in df2.columns and 'lat' not in df2.columns and 'lon' not in df2.columns:\n",
        "    print(\"\\nüîç Splitting 'position' column into lat/lon...\")\n",
        "    # Split coordinates if format is \"lat,lon\"\n",
        "    df2[['lat', 'lon']] = df2['position'].str.split(',', expand=True)\n",
        "    df2['lat'] = pd.to_numeric(df2['lat'], errors='coerce')\n",
        "    df2['lon'] = pd.to_numeric(df2['lon'], errors='coerce')\n",
        "    print(f\"‚úÖ Coordinates split: {df2['lat'].notna().sum()} valid coordinates\")\n",
        "\n",
        "# Prepare DataFrame - ensure no lists/dicts\n",
        "df2_clean = df2.copy()\n",
        "\n",
        "# Clean column names for BigQuery compatibility\n",
        "df2_clean.columns = df2_clean.columns.str.replace('[^a-zA-Z0-9_]', '_', regex=True)\n",
        "print(f\"\\nüßπ Cleaning data for BigQuery...\")\n",
        "\n",
        "conversions = 0\n",
        "for col in df2_clean.columns:\n",
        "    # Convert lists/dicts to strings\n",
        "    if df2_clean[col].apply(lambda x: isinstance(x, (list, dict, tuple))).any():\n",
        "        df2_clean[col] = df2_clean[col].astype(str)\n",
        "        conversions += 1\n",
        "        print(f\"   Converted {col} to string\")\n",
        "\n",
        "# Fill NaN values for string columns\n",
        "nan_count = df2_clean.isna().sum().sum()\n",
        "if nan_count > 0:\n",
        "    print(f\"   Found {nan_count} NaN values\")\n",
        "    for col in df2_clean.columns:\n",
        "        if df2_clean[col].dtype == 'object':\n",
        "            df2_clean[col] = df2_clean[col].fillna('')\n",
        "\n",
        "print(f\"   Cleaned shape: {df2_clean.shape}\")\n",
        "\n",
        "# Convert DataFrame to CSV in memory\n",
        "print(\"\\nüìÑ Converting DataFrame to CSV in memory...\")\n",
        "csv_buffer = io.StringIO()\n",
        "df2_clean.to_csv(csv_buffer, index=False, encoding='utf-8')\n",
        "csv_content = csv_buffer.getvalue().encode('utf-8')\n",
        "\n",
        "# Create job configuration\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_TRUNCATE\",  # Will replace existing table\n",
        "    autodetect=True,                     # Let BigQuery detect schema\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,                 # Skip header row\n",
        "    max_bad_records=100,                 # Allow some bad records\n",
        "    encoding='UTF-8'\n",
        ")\n",
        "\n",
        "print(f\"\\n‚¨ÜÔ∏è  Uploading {len(df2_clean):,} rows to table '{TABLE}'...\")\n",
        "\n",
        "# Upload from CSV\n",
        "try:\n",
        "    # Create file-like object\n",
        "    file_obj = io.BytesIO(csv_content)\n",
        "\n",
        "    # Submit job\n",
        "    job = client.load_table_from_file(\n",
        "        file_obj,\n",
        "        table_id,\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(\"   Job submitted. Waiting for completion...\")\n",
        "    job.result()  # Wait for completion\n",
        "\n",
        "    # Verify upload\n",
        "    table = client.get_table(table_id)\n",
        "    print(f\"\\n‚úÖ SUCCESS!\")\n",
        "    print(f\"   Table: {table_id}\")\n",
        "    print(f\"   Rows uploaded: {table.num_rows:,}\")\n",
        "    print(f\"   Table size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
        "    print(f\"   Created: {table.created.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Show schema preview\n",
        "    print(f\"\\nüìê Schema preview (first 5 columns):\")\n",
        "    for i, field in enumerate(table.schema[:5], 1):\n",
        "        print(f\"   {i}. {field.name:20} : {field.field_type}\")\n",
        "\n",
        "    if len(table.schema) > 5:\n",
        "        print(f\"   ... and {len(table.schema) - 5} more columns\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Upload failed: {e}\")\n",
        "\n",
        "    # Try alternative method\n",
        "    print(\"\\nüîÑ Trying alternative upload method...\")\n",
        "    try:\n",
        "        # Try direct DataFrame upload\n",
        "        direct_job_config = bigquery.LoadJobConfig(\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "            autodetect=True,\n",
        "            max_bad_records=100\n",
        "        )\n",
        "\n",
        "        direct_job = client.load_table_from_dataframe(df2_clean, table_id, job_config=direct_job_config)\n",
        "        direct_job.result()\n",
        "\n",
        "        table = client.get_table(table_id)\n",
        "        print(f\"‚úÖ Direct upload successful!\")\n",
        "        print(f\"   Rows uploaded: {table.num_rows:,}\")\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå Alternative method also failed: {e2}\")\n",
        "        print(\"\\nüí° You can:\")\n",
        "        print(\"1. Check the saved CSV file: 'collection_centres_PdL_region.csv'\")\n",
        "        print(\"2. Upload it manually via Google Cloud Console\")\n",
        "        print(\"3. Or check for data format issues\")\n",
        "\n",
        "try:\n",
        "    # List all tables in dataset\n",
        "    tables = list(client.list_tables(DATASET))\n",
        "    table_names = [t.table_id for t in tables]\n",
        "\n",
        "    print(f\"Tables in dataset '{DATASET}':\")\n",
        "    for name in sorted(table_names):\n",
        "        if name == TABLE:\n",
        "            print(f\"   ‚úÖ {name} (just uploaded)\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ {name}\")\n",
        "\n",
        "    if TABLE not in table_names:\n",
        "        print(f\"\\n‚ö†Ô∏è  Warning: Table '{TABLE}' not found in dataset!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error listing tables: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Fetching 100 records from offset 0...\n",
            "  Total records available: 2575\n",
            "  Fetching 100 records from offset 100...\n",
            "  Fetching 100 records from offset 200...\n",
            "  Fetching 100 records from offset 300...\n",
            "  Fetching 100 records from offset 400...\n",
            "  Fetching 100 records from offset 500...\n",
            "  Fetching 100 records from offset 600...\n",
            "  Fetching 100 records from offset 700...\n",
            "  Fetching 100 records from offset 800...\n",
            "  Fetching 100 records from offset 900...\n",
            "  Fetching 100 records from offset 1000...\n",
            "  Fetching 100 records from offset 1100...\n",
            "  Fetching 100 records from offset 1200...\n",
            "  Fetching 100 records from offset 1300...\n",
            "  Fetching 100 records from offset 1400...\n",
            "  Fetching 100 records from offset 1500...\n",
            "  Fetching 100 records from offset 1600...\n",
            "  Fetching 100 records from offset 1700...\n",
            "  Fetching 100 records from offset 1800...\n",
            "  Fetching 100 records from offset 1900...\n",
            "  Fetching 100 records from offset 2000...\n",
            "  Fetching 100 records from offset 2100...\n",
            "  Fetching 100 records from offset 2200...\n",
            "  Fetching 100 records from offset 2300...\n",
            "  Fetching 100 records from offset 2400...\n",
            "  Fetching 100 records from offset 2500...\n"
          ]
        }
      ],
      "source": [
        "# Localisation des colonnes d‚Äôapports volontaires de Nantes M√©tropole\n",
        "\n",
        "BASE_URL = \"https://data.nantesmetropole.fr/api/explore/v2.1/catalog/datasets/244400404_localisation-des-colonnes-apports-volontaires-de-nantes-metropole/records\"\n",
        "\n",
        "all_records = []\n",
        "limit = 100  # Records per page\n",
        "offset = 0\n",
        "total_count = None\n",
        "\n",
        "while True:\n",
        "\n",
        "# Build URL with current offset\n",
        "    url = f\"{BASE_URL}?limit={limit}&offset={offset}\"\n",
        "    print(f\"  Fetching {limit} records from offset {offset}...\")\n",
        "\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "\n",
        "# Get total count on first request\n",
        "    if total_count is None:\n",
        "        total_count = data.get('total_count', 0)\n",
        "        print(f\"  Total records available: {total_count}\")\n",
        "\n",
        "# Add records from this page\n",
        "    page_records = data.get('results', [])\n",
        "    all_records.extend(page_records)\n",
        "\n",
        "    # Update offset\n",
        "    offset += limit\n",
        "\n",
        "# Stop if we have all records or if page is empty\n",
        "    if not page_records or offset >= total_count:\n",
        "        break\n",
        "\n",
        "# Create DataFrame\n",
        "df3 = pd.DataFrame(all_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2575 entries, 0 to 2574\n",
            "Data columns (total 27 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   id_colonne            1424 non-null   object \n",
            " 1   id_colonne_ancien     2571 non-null   object \n",
            " 2   type_colonne          2575 non-null   object \n",
            " 3   type_dechet           2575 non-null   object \n",
            " 4   adresse               2574 non-null   object \n",
            " 5   mot_directeur         2403 non-null   object \n",
            " 6   volume_colonne        2370 non-null   float64\n",
            " 7   volume_fosse          1438 non-null   float64\n",
            " 8   matiere               362 non-null    object \n",
            " 9   modele                1097 non-null   object \n",
            " 10  fournisseur           1973 non-null   object \n",
            " 11  prehension            2237 non-null   object \n",
            " 12  type_avaloir          1032 non-null   object \n",
            " 13  date_mise_en_place    1261 non-null   object \n",
            " 14  date_mise_en_service  987 non-null    object \n",
            " 15  numero_serie          374 non-null    object \n",
            " 16  investisseur          1063 non-null   object \n",
            " 17  domanialite           1264 non-null   object \n",
            " 18  changement_colonne    115 non-null    object \n",
            " 19  nouvelle_convention   1 non-null      float64\n",
            " 20  operateur_collecte    1099 non-null   object \n",
            " 21  commune               2575 non-null   object \n",
            " 22  pole                  2575 non-null   object \n",
            " 23  observation           415 non-null    object \n",
            " 24  gid                   2575 non-null   object \n",
            " 25  globalid              2575 non-null   object \n",
            " 26  geo_point_2d          2575 non-null   object \n",
            "dtypes: float64(3), object(24)\n",
            "memory usage: 543.3+ KB\n"
          ]
        }
      ],
      "source": [
        "df3.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['id_colonne', 'id_colonne_ancien', 'type_colonne', 'type_dechet',\n",
            "       'adresse', 'mot_directeur', 'volume_colonne', 'volume_fosse', 'matiere',\n",
            "       'modele', 'fournisseur', 'prehension', 'type_avaloir',\n",
            "       'date_mise_en_place', 'date_mise_en_service', 'numero_serie',\n",
            "       'investisseur', 'domanialite', 'changement_colonne',\n",
            "       'nouvelle_convention', 'operateur_collecte', 'commune', 'pole',\n",
            "       'observation', 'gid', 'globalid', 'geo_point_2d', 'lon', 'lat'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df3.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUICK df3 DUPLICATE CHECK (FIXED)\n",
            "Total rows: 2575\n",
            "Exact duplicates: 0\n",
            "Unique rows: 2575\n",
            "\n",
            "Key column duplicates (actual df3 columns):\n",
            "  id_colonne: 1151 duplicates (1423 unique values)\n",
            "  adresse: 1206 duplicates (1368 unique values)\n",
            "  commune: 2551 duplicates (24 unique values)\n",
            "  type_dechet: 2570 duplicates (5 unique values)\n",
            "  type_colonne: 2573 duplicates (2 unique values)\n"
          ]
        }
      ],
      "source": [
        "print(\"QUICK df3 DUPLICATE CHECK\")\n",
        "\n",
        "# Create a string version for duplicate checking\n",
        "\n",
        "df3_str = df3.copy()\n",
        "\n",
        "# Convert any dictionary columns to strings\n",
        "for col in df3_str.columns:\n",
        "    if df3_str[col].apply(lambda x: isinstance(x, dict)).any():\n",
        "        df3_str[col] = df3_str[col].astype(str)\n",
        "\n",
        "# Now check duplicates\n",
        "print(f\"Total rows: {len(df3)}\")\n",
        "print(f\"Exact duplicates: {df3_str.duplicated().sum()}\")\n",
        "print(f\"Unique rows: {df3_str.drop_duplicates().shape[0]}\")\n",
        "\n",
        "# Check key columns that ACTUALLY exist in df3\n",
        "print(\"\\nKey column duplicates (actual df3 columns):\")\n",
        "actual_key_columns = ['id_colonne', 'adresse', 'commune', 'type_dechet', 'type_colonne']\n",
        "for col in actual_key_columns:\n",
        "    if col in df3.columns:\n",
        "        dup_count = df3[col].duplicated().sum()\n",
        "        unique_count = df3[col].nunique()\n",
        "        print(f\"  {col}: {dup_count} duplicates ({unique_count} unique values)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'lon': -1.623654076876046, 'lat': 47.181190184213065}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df3['geo_point_2d'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully extracted coordinates for 2575 rows\n"
          ]
        }
      ],
      "source": [
        "# Extract coordinates from dictionaries\n",
        "\n",
        "df3['lon'] = df3['geo_point_2d'].apply(\n",
        "    lambda x: float(x['lon']) if isinstance(x, dict) and 'lon' in x else None\n",
        ")\n",
        "df3['lat'] = df3['geo_point_2d'].apply(\n",
        "    lambda x: float(x['lat']) if isinstance(x, dict) and 'lat' in x else None\n",
        ")\n",
        "\n",
        "print(f\"Successfully extracted coordinates for {df3['lon'].notna().sum()} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame saved as 'location_dropoff_points_nantes.csv'\n"
          ]
        }
      ],
      "source": [
        "# Save df with name location dropoff points nantes (index = False)\n",
        "\n",
        "df3.to_csv('location_dropoff_points_nantes.csv', index=False)\n",
        "print(\"DataFrame saved as 'location_dropoff_points_nantes.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "UPLOADING TO BIGQUERY\n",
            "============================================================\n",
            "‚úÖ BigQuery client initialized successfully\n",
            "‚úÖ Dataset 'nantes' exists\n",
            "   Location: EU\n",
            "\n",
            "üìä Data to upload:\n",
            "   Rows: 2,575\n",
            "   Columns: 29\n",
            "   Columns: ['id_colonne', 'id_colonne_ancien', 'type_colonne', 'type_dechet', 'adresse', 'mot_directeur', 'volume_colonne', 'volume_fosse', 'matiere', 'modele', 'fournisseur', 'prehension', 'type_avaloir', 'date_mise_en_place', 'date_mise_en_service', 'numero_serie', 'investisseur', 'domanialite', 'changement_colonne', 'nouvelle_convention', 'operateur_collecte', 'commune', 'pole', 'observation', 'gid', 'globalid', 'geo_point_2d', 'lon', 'lat']\n",
            "\n",
            "üßπ Cleaning data for BigQuery\n",
            "   Converted geo_point_2d to string\n",
            "   Found 25440 NaN values\n",
            "   Cleaned shape: (2575, 29)\n",
            "\n",
            "üìÑ Converting DataFrame to CSV in memory...\n",
            "\n",
            "‚¨ÜÔ∏è  Uploading 2,575 rows to table 'location_dropoff_points_nantes'...\n",
            "   Job submitted. Waiting for completion...\n",
            "\n",
            "‚úÖ SUCCESS!\n",
            "   Table: trash-optimizer-479913.nantes.location_dropoff_points_nantes\n",
            "   Rows uploaded: 2,575\n",
            "   Table size: 0.73 MB\n",
            "   Created: 2025-12-03 11:46:17\n",
            "\n",
            "üìê Schema preview (first 5 columns):\n",
            "   1. id_colonne           : STRING\n",
            "   2. id_colonne_ancien    : STRING\n",
            "   3. type_colonne         : STRING\n",
            "   4. type_dechet          : STRING\n",
            "   5. adresse              : STRING\n",
            "   ... and 24 more columns\n",
            "Tables in dataset 'nantes':\n",
            "   ‚Ä¢ alimentary_garbage\n",
            "   ‚Ä¢ alimentary_garbage_clean\n",
            "   ‚Ä¢ all_trash_locations\n",
            "   ‚Ä¢ collection_centres_pdl\n",
            "   ‚Ä¢ ecopoints\n",
            "   ‚úÖ location_dropoff_points_nantes (just uploaded)\n",
            "   ‚Ä¢ trash_collection_points\n"
          ]
        }
      ],
      "source": [
        "# CSV UPLOAD METHOD TO BIGQUERY\n",
        "# Set credentials\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/dariaserbichenko/code/DariaSerb/key-gcp/trash-optimizer-479913-91e59ecc96c9.json\"\n",
        "\n",
        "# BigQuery setup\n",
        "PROJECT = \"trash-optimizer-479913\"\n",
        "DATASET = \"nantes\"\n",
        "TABLE = \"location_dropoff_points_nantes\"\n",
        "table_id = f\"{PROJECT}.{DATASET}.{TABLE}\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"UPLOADING TO BIGQUERY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize client\n",
        "client = bigquery.Client(project=PROJECT)\n",
        "print(f\"‚úÖ BigQuery client initialized successfully\")\n",
        "\n",
        "# Check dataset\n",
        "dataset_ref = f\"{PROJECT}.{DATASET}\"\n",
        "try:\n",
        "    dataset = client.get_dataset(dataset_ref)\n",
        "    print(f\"‚úÖ Dataset '{DATASET}' exists\")\n",
        "    print(f\"   Location: {dataset.location}\")\n",
        "except NotFound:\n",
        "    print(f\"üìÅ Creating dataset '{DATASET}'...\")\n",
        "    dataset = bigquery.Dataset(dataset_ref)\n",
        "    dataset.location = \"EU\"\n",
        "    dataset = client.create_dataset(dataset, timeout=30)\n",
        "    print(f\"‚úÖ Dataset created\")\n",
        "    print(f\"   Location: {dataset.location}\")\n",
        "\n",
        "# Display DataFrame info\n",
        "print(f\"\\nüìä Data to upload:\")\n",
        "print(f\"   Rows: {len(df3):,}\")\n",
        "print(f\"   Columns: {len(df3.columns)}\")\n",
        "print(f\"   Columns: {list(df3.columns)}\")\n",
        "\n",
        "# Check if we need to split the position column\n",
        "if 'position' in df3.columns and 'lat' not in df3.columns and 'lon' not in df3.columns:\n",
        "    print(\"\\nüîç Splitting 'position' column into lat/lon\")\n",
        "    # Split coordinates if format is \"lat,lon\"\n",
        "    df3[['lat', 'lon']] = df3['position'].str.split(',', expand=True)\n",
        "    df3['lat'] = pd.to_numeric(df3['lat'], errors='coerce')\n",
        "    df3['lon'] = pd.to_numeric(df3['lon'], errors='coerce')\n",
        "    print(f\"‚úÖ Coordinates split: {df2['lat'].notna().sum()} valid coordinates\")\n",
        "\n",
        "# Prepare DataFrame - ensure no lists/dicts\n",
        "df3_clean = df3.copy()\n",
        "\n",
        "# Clean column names for BigQuery compatibility\n",
        "df3_clean.columns = df3_clean.columns.str.replace('[^a-zA-Z0-9_]', '_', regex=True)\n",
        "print(f\"\\nüßπ Cleaning data for BigQuery\")\n",
        "\n",
        "conversions = 0\n",
        "for col in df3_clean.columns:\n",
        "    # Convert lists/dicts to strings\n",
        "    if df3_clean[col].apply(lambda x: isinstance(x, (list, dict, tuple))).any():\n",
        "        df3_clean[col] = df3_clean[col].astype(str)\n",
        "        conversions += 1\n",
        "        print(f\"   Converted {col} to string\")\n",
        "\n",
        "# Fill NaN values for string columns\n",
        "nan_count = df3_clean.isna().sum().sum()\n",
        "if nan_count > 0:\n",
        "    print(f\"   Found {nan_count} NaN values\")\n",
        "    for col in df3_clean.columns:\n",
        "        if df3_clean[col].dtype == 'object':\n",
        "            df3_clean[col] = df3_clean[col].fillna('')\n",
        "\n",
        "print(f\"   Cleaned shape: {df3_clean.shape}\")\n",
        "\n",
        "# Convert DataFrame to CSV in memory\n",
        "print(\"\\nüìÑ Converting DataFrame to CSV in memory...\")\n",
        "csv_buffer = io.StringIO()\n",
        "df3_clean.to_csv(csv_buffer, index=False, encoding='utf-8')\n",
        "csv_content = csv_buffer.getvalue().encode('utf-8')\n",
        "\n",
        "# Create job configuration\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_TRUNCATE\",  # Will replace existing table\n",
        "    autodetect=True,                     # Let BigQuery detect schema\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,                 # Skip header row\n",
        "    max_bad_records=100,                 # Allow some bad records\n",
        "    encoding='UTF-8'\n",
        ")\n",
        "\n",
        "print(f\"\\n‚¨ÜÔ∏è  Uploading {len(df3_clean):,} rows to table '{TABLE}'...\")\n",
        "\n",
        "# Upload from CSV\n",
        "try:\n",
        "    # Create file-like object\n",
        "    file_obj = io.BytesIO(csv_content)\n",
        "\n",
        "    # Submit job\n",
        "    job = client.load_table_from_file(\n",
        "        file_obj,\n",
        "        table_id,\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(\"   Job submitted. Waiting for completion...\")\n",
        "    job.result()  # Wait for completion\n",
        "\n",
        "    # Verify upload\n",
        "    table = client.get_table(table_id)\n",
        "    print(f\"\\n‚úÖ SUCCESS!\")\n",
        "    print(f\"   Table: {table_id}\")\n",
        "    print(f\"   Rows uploaded: {table.num_rows:,}\")\n",
        "    print(f\"   Table size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
        "    print(f\"   Created: {table.created.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Show schema preview\n",
        "    print(f\"\\nüìê Schema preview (first 5 columns):\")\n",
        "    for i, field in enumerate(table.schema[:5], 1):\n",
        "        print(f\"   {i}. {field.name:20} : {field.field_type}\")\n",
        "\n",
        "    if len(table.schema) > 5:\n",
        "        print(f\"   ... and {len(table.schema) - 5} more columns\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Upload failed: {e}\")\n",
        "\n",
        "    # Try alternative method\n",
        "    print(\"\\nüîÑ Trying alternative upload method...\")\n",
        "    try:\n",
        "        # Try direct DataFrame upload\n",
        "        direct_job_config = bigquery.LoadJobConfig(\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "            autodetect=True,\n",
        "            max_bad_records=100\n",
        "        )\n",
        "\n",
        "        direct_job = client.load_table_from_dataframe(df3_clean, table_id, job_config=direct_job_config)\n",
        "        direct_job.result()\n",
        "\n",
        "        table = client.get_table(table_id)\n",
        "        print(f\"‚úÖ Direct upload successful!\")\n",
        "        print(f\"   Rows uploaded: {table.num_rows:,}\")\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå Alternative method also failed: {e2}\")\n",
        "        print(\"\\nüí° You can:\")\n",
        "        print(\"1. Check the saved CSV file: 'location_dropoff_points_nantes.csv'\")\n",
        "        print(\"2. Upload it manually via Google Cloud Console\")\n",
        "        print(\"3. Or check for data format issues\")\n",
        "\n",
        "try:\n",
        "    # List all tables in dataset\n",
        "    tables = list(client.list_tables(DATASET))\n",
        "    table_names = [t.table_id for t in tables]\n",
        "\n",
        "    print(f\"Tables in dataset '{DATASET}':\")\n",
        "    for name in sorted(table_names):\n",
        "        if name == TABLE:\n",
        "            print(f\"   ‚úÖ {name} (just uploaded)\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ {name}\")\n",
        "\n",
        "    if TABLE not in table_names:\n",
        "        print(f\"\\n‚ö†Ô∏è  Warning: Table '{TABLE}' not found in dataset!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error listing tables: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating unified table...\n",
            "‚úÖ Unified table created successfully!\n",
            "\n",
            "Table info:\n",
            "   Name: trash-optimizer-479913.nantes.all_trash_locations\n",
            "   Rows: 1987\n",
            "   Size: 0.13 MB\n"
          ]
        }
      ],
      "source": [
        "# Create unified table in BigQuery combining all three datasets\n",
        "# Set credentials\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/dariaserbichenko/code/DariaSerb/key-gcp/trash-optimizer-479913-91e59ecc96c9.json\"\n",
        "\n",
        "PROJECT = \"trash-optimizer-479913\"\n",
        "DATASET = \"nantes\"\n",
        "UNIFIED_TABLE = \"all_trash_locations\"\n",
        "unified_table_id = f\"{PROJECT}.{DATASET}.{UNIFIED_TABLE}\"\n",
        "\n",
        "client = bigquery.Client(project=PROJECT)\n",
        "\n",
        "# Simple SQL query - FIXED\n",
        "create_unified_table_query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{PROJECT}.{DATASET}.{UNIFIED_TABLE}` AS\n",
        "\n",
        "-- From alimentary_garbage_clean\n",
        "SELECT\n",
        "  'alimentary' as trash_type,\n",
        "  identifiant as nom,\n",
        "  adresse,\n",
        "  lat as latitude,\n",
        "  lon as longitude\n",
        "FROM `{PROJECT}.{DATASET}.alimentary_garbage_clean`\n",
        "WHERE lat IS NOT NULL AND lon IS NOT NULL\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "-- From ecopoints\n",
        "SELECT\n",
        "  'ecopoints' as trash_type,\n",
        "  nom,\n",
        "  adresse,\n",
        "  lat as latitude,\n",
        "  lon as longitude\n",
        "FROM `{PROJECT}.{DATASET}.ecopoints`\n",
        "WHERE lat IS NOT NULL AND lon IS NOT NULL\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "-- From collection_centres_pdl\n",
        "SELECT\n",
        "  'collection_centres' as trash_type,\n",
        "  N_SERVICE as nom,\n",
        "  AD1_ACTEUR as adresse,\n",
        "  lat as latitude,\n",
        "  lon as longitude\n",
        "FROM `{PROJECT}.{DATASET}.collection_centres_pdl`\n",
        "WHERE lat IS NOT NULL AND lon IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "print(\"Creating unified table...\")\n",
        "try:\n",
        "    # Execute query\n",
        "    job = client.query(create_unified_table_query)\n",
        "    job.result()\n",
        "    print(\"‚úÖ Unified table created successfully!\")\n",
        "\n",
        "    # Get table info\n",
        "    table = client.get_table(unified_table_id)\n",
        "    print(f\"\\nTable info:\")\n",
        "    print(f\"   Name: {unified_table_id}\")\n",
        "    print(f\"   Rows: {table.num_rows}\")\n",
        "    print(f\"   Size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "trash-optimizer",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
