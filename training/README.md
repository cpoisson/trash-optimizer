# Training Module

This module provides a flexible and extensible system for training image classification models on any classification dataset. It supports multiple architectures, configurable training parameters, and easy experiment management.

## ğŸ“ Project Structure

```
training/
â”œâ”€â”€ train.py                    # Main CLI entry point
â”œâ”€â”€ Makefile                    # Training orchestration
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ base_config.py         # Base training configuration
â”‚   â””â”€â”€ model_configs.py       # Model-specific configs
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model_factory.py       # Model creation for all architectures
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset.py             # TrashDataset class
â”‚   â”œâ”€â”€ transforms.py          # Data augmentation strategies
â”‚   â””â”€â”€ loader.py              # DataLoader creation
â”œâ”€â”€ training_lib/
â”‚   â”œâ”€â”€ trainer.py             # Training loop logic
â”‚   â””â”€â”€ evaluator.py           # Evaluation & metrics
â””â”€â”€ utils/
    â”œâ”€â”€ device.py              # Device detection
    â””â”€â”€ visualization.py       # Plotting functions
```

## ğŸš€ Quick Start

### Using Makefile (Recommended)

```bash
# Show all available commands
make help

# Check environment setup
make check-env

# Train specific models
make train-efficientnet-b0
make train-efficientnet-v2-s
make train-convnext

# Train all EfficientNet variants
make train-efficientnets

# Train all models sequentially
make train-all

# Quick test (5 epochs)
make quick-test

# Custom training
make train-custom MODEL=efficientnet_b2 EPOCHS=100 BATCH_SIZE=64
```

### Using CLI Directly

```bash
# Basic training
python train.py --model efficientnet_b0

# Custom parameters
python train.py --model efficientnet_v2_s --epochs 100 --batch-size 64 --learning-rate 0.0002

# Custom output directory
python train.py --model convnext_tiny --output-dir ./my_experiment

# List available models
make list-models
```

## ğŸ¤– Supported Models

| Model | Description | Default Batch Size |
|-------|-------------|-------------------|
| `efficientnet_b0` | Lightweight baseline | 32 |
| `efficientnet_b2` | More capacity | 32 |
| `efficientnet_v2_s` | Improved efficiency | 32 |
| `efficientnet_v2_m` | Larger capacity | 24 |
| `convnext_tiny` | Modern architecture | 32 |
| `resnet50` | Classic architecture | 32 |

## âš™ï¸ Configuration

### Environment Variables (.env)

```bash
DATASET_ROOT_DIR=/path/to/dataset
RESULTS_ROOT_DIR=/path/to/results
```

### Default Training Parameters

Edit `config/base_config.py` for global defaults:
- Batch size: 32
- Learning rate: 0.0001
- Epochs: 50
- Early stopping patience: 10
- Validation split: 25%

### Per-Model Overrides

Edit `config/model_configs.py` to customize individual models:
```python
MODEL_CONFIGS = {
    'efficientnet_v2_s': {
        'learning_rate': 0.0002,  # Custom LR
        'batch_size': 64,         # Larger batch
        'freeze_layers': -2,      # Different freezing
    }
}
```

## ğŸ“Š Output Structure

Each training run creates:

```
RESULTS_ROOT_DIR/
â””â”€â”€ YYYYMMDD_HHMMSS_modelname/
    â”œâ”€â”€ best_model.pth              # Best checkpoint
    â”œâ”€â”€ final_model.pth             # Final model
    â”œâ”€â”€ class_mapping.txt           # Class index mapping
    â”œâ”€â”€ training_history.txt        # Epoch-by-epoch metrics
    â”œâ”€â”€ training_curves.png         # Loss/accuracy plots
    â”œâ”€â”€ confusion_matrix.png        # Confusion matrix
    â””â”€â”€ classification_report.txt   # Per-class metrics
```

## ğŸ”§ Advanced Usage

### Training with Custom Dataset

```bash
python train.py --model efficientnet_b0 --dataset-dir /path/to/custom/dataset
```

### Hyperparameter Tuning

```bash
# Try different learning rates
for lr in 0.0001 0.0005 0.001; do
    python train.py --model efficientnet_v2_s --learning-rate $lr --output-dir ./lr_$lr
done
```

### Batch Training Script

```bash
#!/bin/bash
# train_all_experiments.sh

models=("efficientnet_b0" "efficientnet_b2" "efficientnet_v2_s" "convnext_tiny")

for model in "${models[@]}"; do
    echo "Training $model..."
    python train.py --model $model
    if [ $? -ne 0 ]; then
        echo "Error training $model"
        exit 1
    fi
done

echo "All models trained successfully!"
```

## ğŸ§© Extending the System

### Adding a New Model

1. Add to `models/model_factory.py`:
```python
@staticmethod
def _create_my_model(num_classes, freeze_layers):
    model = models.my_model(weights='IMAGENET1K_V1')
    # Configure model...
    return model
```

2. Register in `SUPPORTED_MODELS` list

3. Add config to `config/model_configs.py`:
```python
MODEL_CONFIGS = {
    'my_model': {
        'learning_rate': 0.0001,
        'batch_size': 32,
        'description': 'My custom model'
    }
}
```

4. Add Makefile target (optional):
```makefile
train-my-model: ## Train my custom model
	$(PYTHON) $(TRAIN_SCRIPT) --model my_model
```

### Custom Data Augmentation

Edit `data/transforms.py`:
```python
def get_train_transforms():
    return transforms.Compose([
        # Add your custom transforms
        transforms.RandomPerspective(p=0.2),
        # ...
    ])
```

### Custom Loss Functions

Edit `training_lib/trainer.py`:
```python
# Replace CrossEntropyLoss with custom loss
self.criterion = FocalLoss(alpha=1, gamma=2)
```

## ğŸ“ˆ Monitoring Training

View training progress:
```bash
# Watch training history
tail -f <output_dir>/training_history.txt

# Monitor GPU usage (if using CUDA)
watch -n 1 nvidia-smi
```

## ğŸ› Troubleshooting

### Out of Memory
- Reduce batch size: `--batch-size 16`
- Use smaller model: `--model efficientnet_b0`

### Slow Training
- Increase batch size: `--batch-size 64`
- Increase num_workers in `config/base_config.py`

### Poor Performance
- Increase epochs: `--epochs 100`
- Adjust learning rate: `--learning-rate 0.0002`
- Check data augmentation in `data/transforms.py`

## ğŸ§¹ Maintenance

```bash
# Clean Python cache
make clean

# List all training runs
ls -lht $RESULTS_ROOT_DIR

# Remove old results (be careful!)
rm -rf $RESULTS_ROOT_DIR/YYYYMMDD_*
```
